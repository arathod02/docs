<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">The Transformer Architecture | Ashish Rathod</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://arathod02.github.io/docs/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://arathod02.github.io/docs/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://arathod02.github.io/docs/blog/transformer-architecture"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="The Transformer Architecture | Ashish Rathod"><meta data-rh="true" name="description" content="The release of the &quot;Attention is All You Need&quot; paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the Transformer architecture, which powers the Generative AI explosion we see today."><meta data-rh="true" property="og:description" content="The release of the &quot;Attention is All You Need&quot; paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the Transformer architecture, which powers the Generative AI explosion we see today."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-12-11T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://www.linkedin.com/in/ashish-rathod02/"><link data-rh="true" rel="icon" href="/docs/img/favicon.jpg"><link data-rh="true" rel="canonical" href="https://arathod02.github.io/docs/blog/transformer-architecture"><link data-rh="true" rel="alternate" href="https://arathod02.github.io/docs/blog/transformer-architecture" hreflang="en"><link data-rh="true" rel="alternate" href="https://arathod02.github.io/docs/blog/transformer-architecture" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://arathod02.github.io/docs/blog/transformer-architecture","mainEntityOfPage":"https://arathod02.github.io/docs/blog/transformer-architecture","url":"https://arathod02.github.io/docs/blog/transformer-architecture","headline":"The Transformer Architecture","name":"The Transformer Architecture","description":"The release of the \"Attention is All You Need\" paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the Transformer architecture, which powers the Generative AI explosion we see today.","datePublished":"2025-12-11T00:00:00.000Z","author":{"@type":"Person","name":"Ashish Rathod","description":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","image":"https://github.com/arathod02.png"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://arathod02.github.io/docs/blog","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/docs/blog/rss.xml" title="Ashish Rathod RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/docs/blog/atom.xml" title="Ashish Rathod Atom Feed"><link rel="stylesheet" href="/docs/assets/css/styles.aade1aad.css">
<script src="/docs/assets/js/runtime~main.04393b3f.js" defer="defer"></script>
<script src="/docs/assets/js/main.d058adcd.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/docs/img/favicon.jpg"><link rel="preload" as="image" href="https://github.com/arathod02.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/docs/"><div class="navbar__logo"><img src="/docs/img/favicon.jpg" alt="Ashish Rathod Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/docs/img/favicon.jpg" alt="Ashish Rathod Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Ashish Rathod</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/concurrent-replace">Avoid &quot;REPLACE INTO&quot;</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/slack-design">Slack Architecture</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/distributed-key-value-store">Distributed Key-Value Store</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/db-pessimistic-optimistic-locking">Deep Dive into Database Pessimistic &amp; Optimistic Locking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/proxy-server">Database Proxy Servers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/server-sent-events-explained">Real-Time Communication with Server-Sent Events (SSE)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/application-level-sharding-design">Implementing Shard Aware Application</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/scaling-distributed-systems">Scaling Distributed Systems (Focus on Databases)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/avoid-hard-deletes">The Pains of Hard Delete</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/cache-stampede-thundering-herd">The Thundering Herd - Understanding and Solving the Cache Stampede</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/docs/blog/transformer-architecture">The Transformer Architecture</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/offline-online-indicator">Presence Service</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">The Transformer Architecture</h1><div class="container_mt6G margin-vert--md"><time datetime="2025-12-11T00:00:00.000Z">December 11, 2025</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/docs/blog/authors/ashish"><img class="avatar__photo authorImage_XqGP" src="https://github.com/arathod02.png" alt="Ashish Rathod"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/docs/blog/authors/ashish"><span class="authorName_yefp" translate="no">Ashish Rathod</span></a></div><small class="authorTitle_nd0D" title="Ex-Intuit Staff Engineer">Ex-Intuit Staff Engineer</small><div class="authorSocials_rSDt"><a href="https://www.linkedin.com/in/ashish-rathod02/" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" preserveAspectRatio="xMidYMid" viewBox="0 0 256 256" style="--dark:#0a66c2;--light:#ffffffe6" class="authorSocialIcon_XYv3 linkedinSvg_FCgI"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453"></path></svg></a><a href="https://github.com/arathod02" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 256 250" preserveAspectRatio="xMidYMid" style="--dark:#000;--light:#fff" class="authorSocialIcon_XYv3 githubSvg_Uu4N"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"></path></svg></a></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p>The release of the &quot;Attention is All You Need&quot; paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the <strong>Transformer</strong> architecture, which powers the Generative AI explosion we see today.</p>
<p>But how does a Transformer actually &quot;read&quot; and &quot;understand&quot; a sentence?</p>
<p>In this post, we will walk through the architecture step-by-step, transforming raw text into understanding using the logic from the original paper.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-high-level-view-encoder-and-decoder">The High-Level View: Encoder and Decoder<a href="#the-high-level-view-encoder-and-decoder" class="hash-link" aria-label="Direct link to The High-Level View: Encoder and Decoder" title="Direct link to The High-Level View: Encoder and Decoder" translate="no">​</a></h2>
<p>At its highest level, the Transformer is a statistical calculator. It doesn&#x27;t &quot;know&quot; English or Java; it knows math. The architecture is split into two distinct parts:</p>
<ol>
<li class=""><strong>The Encoder:</strong> Processes the input data.</li>
<li class=""><strong>The Decoder:</strong> Generates the output.</li>
</ol>
<p>They work in conjunction, though in modern models (like GPT), we often see decoder-only architectures. For this guide, we will follow the flow from Input (bottom) to Output (top).</p>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-1-tokenization-words-to-numbers">Step 1: Tokenization (Words to Numbers)<a href="#step-1-tokenization-words-to-numbers" class="hash-link" aria-label="Direct link to Step 1: Tokenization (Words to Numbers)" title="Direct link to Step 1: Tokenization (Words to Numbers)" translate="no">​</a></h2>
<p>Machine learning models cannot process raw text. They need numbers. Before passing a sentence like <em>&quot;The teacher has the book&quot;</em> into the model, we must <strong>tokenize</strong> it.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-tokenization">What is Tokenization?<a href="#what-is-tokenization" class="hash-link" aria-label="Direct link to What is Tokenization?" title="Direct link to What is Tokenization?" translate="no">​</a></h3>
<p>Simply put, this converts words into numbers. Imagine a giant dictionary where every word has a unique ID.</p>
<ul>
<li class="">&quot;The&quot; -&gt; 101</li>
<li class="">&quot;Teacher&quot; -&gt; 2045</li>
<li class="">&quot;Book&quot; -&gt; 3011</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>AI Terminology</div><div class="admonitionContent_BuS1"><p><strong>Tokenizer:</strong> A tool that breaks text into smaller chunks (tokens). These can be whole words or parts of words. Ideally, if you train a model with a specific tokenizer, you must use the exact same one when generating text.</p></div></div>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-2-embeddings-numbers-to-meaning">Step 2: Embeddings (Numbers to Meaning)<a href="#step-2-embeddings-numbers-to-meaning" class="hash-link" aria-label="Direct link to Step 2: Embeddings (Numbers to Meaning)" title="Direct link to Step 2: Embeddings (Numbers to Meaning)" translate="no">​</a></h2>
<p>Now we have a list of numbers (IDs), but numbers don&#x27;t have &quot;meaning.&quot; To a computer, the number 100 and 101 are just close in value, but the words they represent might be totally unrelated.</p>
<p>We solve this with the <strong>Embedding Layer</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-concept-high-dimensional-space">The Concept: High-Dimensional Space<a href="#the-concept-high-dimensional-space" class="hash-link" aria-label="Direct link to The Concept: High-Dimensional Space" title="Direct link to The Concept: High-Dimensional Space" translate="no">​</a></h3>
<p>Each Token ID is matched to a <strong>Vector</strong> (a list of numbers). In the original paper, this vector size was 512.</p>
<ul>
<li class="">Imagine a 3D graph (X, Y, Z).</li>
<li class="">Words with similar meanings are plotted physically close to each other in this space.</li>
<li class="">&quot;King&quot; and &quot;Queen&quot; would be close together. &quot;Apple&quot; and &quot;Car&quot; would be far apart.</li>
</ul>
<p>The model calculates the distance (often as an angle) between these words to understand their relationship mathematically.</p>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-3-positional-encoding-adding-order">Step 3: Positional Encoding (Adding Order)<a href="#step-3-positional-encoding-adding-order" class="hash-link" aria-label="Direct link to Step 3: Positional Encoding (Adding Order)" title="Direct link to Step 3: Positional Encoding (Adding Order)" translate="no">​</a></h2>
<p>Transformers process input tokens in <strong>parallel</strong> (all at once), unlike RNNs which read word-by-word. This is great for speed, but it creates a problem: the model loses the concept of word order.</p>
<p>To the model, <em>&quot;The teacher has the book&quot;</em> and <em>&quot;The book has the teacher&quot;</em> look the same because the ingredients (words) are the same.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-fix-positional-encoding">The Fix: Positional Encoding<a href="#the-fix-positional-encoding" class="hash-link" aria-label="Direct link to The Fix: Positional Encoding" title="Direct link to The Fix: Positional Encoding" translate="no">​</a></h3>
<p>We add a &quot;timestamp&quot; or &quot;position signature&quot; to the word vectors.</p>
<ul>
<li class=""><strong>Analogy:</strong> Imagine throwing a stack of unnumbered pages into the air. If you don&#x27;t write page numbers (positional encoding) on them first, you can&#x27;t put the book back together.</li>
</ul>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-4-self-attention-the-core-magic">Step 4: Self-Attention (The Core Magic)<a href="#step-4-self-attention-the-core-magic" class="hash-link" aria-label="Direct link to Step 4: Self-Attention (The Core Magic)" title="Direct link to Step 4: Self-Attention (The Core Magic)" translate="no">​</a></h2>
<p>This is the &quot;Attention&quot; in &quot;Attention is All You Need.&quot;</p>
<p>Once the vectors enter the model, the <strong>Self-Attention Layer</strong> analyzes the relationships between tokens. It allows the model to look at a specific word and understand its context based on <em>every other word</em> in the sentence.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-who-has-the-book">Example: &quot;Who has the book?&quot;<a href="#example-who-has-the-book" class="hash-link" aria-label="Direct link to Example: &quot;Who has the book?&quot;" title="Direct link to Example: &quot;Who has the book?&quot;" translate="no">​</a></h3>
<p>In the sentence: <em>&quot;The teacher gave the book to the student.&quot;</em></p>
<ul>
<li class="">The word <strong>&quot;Book&quot;</strong> needs to understand who has it (Teacher) and who receives it (Student).</li>
<li class="">The attention mechanism creates strong connections (weights) between &quot;Book,&quot; &quot;Teacher,&quot; and &quot;Student,&quot; while ignoring less relevant words like &quot;the.&quot;</li>
</ul>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-5-multi-head-attention-many-perspectives">Step 5: Multi-Head Attention (Many Perspectives)<a href="#step-5-multi-head-attention-many-perspectives" class="hash-link" aria-label="Direct link to Step 5: Multi-Head Attention (Many Perspectives)" title="Direct link to Step 5: Multi-Head Attention (Many Perspectives)" translate="no">​</a></h2>
<p>The model doesn&#x27;t just do self-attention once. It uses <strong>Multi-Head Attention</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-a-head">What is a &quot;Head&quot;?<a href="#what-is-a-head" class="hash-link" aria-label="Direct link to What is a &quot;Head&quot;?" title="Direct link to What is a &quot;Head&quot;?" translate="no">​</a></h3>
<p>Think of a &quot;Head&quot; as a different lens or filter. The model runs 12 to 100 of these heads in parallel. Each head learns a different aspect of language randomly during training.</p>
<ul>
<li class=""><strong>Head 1 (The Grammar Lens):</strong> Might focus on Subject-Verb agreement.</li>
<li class=""><strong>Head 2 (The Rhyme Lens):</strong> Might focus on phonetics or poetry.</li>
<li class=""><strong>Head 3 (The Context Lens):</strong> Might focus on relationships between people (Teacher/Student).</li>
</ul>
<p>The results of all these heads are combined to give the model a complete understanding of the text.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-6-the-output-logits-and-probabilities">Step 6: The Output (Logits and Probabilities)<a href="#step-6-the-output-logits-and-probabilities" class="hash-link" aria-label="Direct link to Step 6: The Output (Logits and Probabilities)" title="Direct link to Step 6: The Output (Logits and Probabilities)" translate="no">​</a></h2>
<p>After passing through the Feed Forward Network, the model produces an output. But it doesn&#x27;t output a word immediately; it outputs a <strong>Vector of Logits</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="breaking-down-the-jargon">Breaking Down the Jargon<a href="#breaking-down-the-jargon" class="hash-link" aria-label="Direct link to Breaking Down the Jargon" title="Direct link to Breaking Down the Jargon" translate="no">​</a></h3>
<ol>
<li class=""><strong>Logits:</strong> These are raw, unnormalized scores. The model scores every single word in its dictionary on how likely it is to be the next word.<!-- -->
<ul>
<li class=""><em>Example:</em> Apple: 5.0, Ball: 1.2, Cat: -3.0</li>
</ul>
</li>
<li class=""><strong>Softmax Layer:</strong> This turns those raw scores (Logits) into probabilities (Percentages).<!-- -->
<ul>
<li class=""><em>Example:</em> Apple: 98%, Ball: 1.9%, Cat: 0.1%</li>
</ul>
</li>
</ol>
<p>The word with the highest probability is selected as the predicted token.</p>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="putting-it-all-together-an-end-to-end-example">Putting It All Together: An End-to-End Example<a href="#putting-it-all-together-an-end-to-end-example" class="hash-link" aria-label="Direct link to Putting It All Together: An End-to-End Example" title="Direct link to Putting It All Together: An End-to-End Example" translate="no">​</a></h2>
<p>We have looked at the components individually. Now, let&#x27;s watch them work together in a real scenario.</p>
<p><strong>The Task:</strong> Translate the French phrase <em>&quot;J&#x27;aime l&#x27;apprentissage automatique&quot;</em> into English.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="phase-1-the-encoder-reading--understanding">Phase 1: The Encoder (Reading &amp; Understanding)<a href="#phase-1-the-encoder-reading--understanding" class="hash-link" aria-label="Direct link to Phase 1: The Encoder (Reading &amp; Understanding)" title="Direct link to Phase 1: The Encoder (Reading &amp; Understanding)" translate="no">​</a></h3>
<p>First, the model needs to understand the input. This happens entirely in the <strong>Encoder</strong>.</p>
<ol>
<li class=""><strong>Tokenization:</strong> The raw text is broken down into numbers.</li>
<li class=""><strong>Embedding &amp; Position:</strong> These numbers are turned into vectors with position information.</li>
<li class=""><strong>Multi-Head Attention:</strong> The model analyzes the relationships. For example, it understands that <em>&quot;apprentissage&quot;</em> (learning) and <em>&quot;automatique&quot;</em> (machine) are strongly related in this context.</li>
<li class=""><strong>Deep Representation:</strong> The output of the Encoder is not text, but a rich matrix of vectors that represents the <em>meaning</em> of the French sentence.</li>
</ol>
<!-- -->
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="phase-2-the-decoder-generating-the-translation">Phase 2: The Decoder (Generating the Translation)<a href="#phase-2-the-decoder-generating-the-translation" class="hash-link" aria-label="Direct link to Phase 2: The Decoder (Generating the Translation)" title="Direct link to Phase 2: The Decoder (Generating the Translation)" translate="no">​</a></h3>
<p>This is where the magic happens. The <strong>Decoder</strong> takes the &quot;Deep Context&quot; from the Encoder and generates the English translation one word at a time. This is a <strong>loop</strong>.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-a-the-trigger">Step A: The Trigger<a href="#step-a-the-trigger" class="hash-link" aria-label="Direct link to Step A: The Trigger" title="Direct link to Step A: The Trigger" translate="no">​</a></h4>
<p>The process starts by feeding a special <strong>Start-of-Sequence (SOS)</strong> token into the Decoder.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-b-the-prediction-loop">Step B: The Prediction Loop<a href="#step-b-the-prediction-loop" class="hash-link" aria-label="Direct link to Step B: The Prediction Loop" title="Direct link to Step B: The Prediction Loop" translate="no">​</a></h4>
<ol>
<li class="">The Decoder looks at the <strong>Context</strong> (from the Encoder) and the <strong>Inputs so far</strong> (initially just <code>&lt;SOS&gt;</code>).</li>
<li class="">It runs Self-Attention to see what English words match the French meaning.</li>
<li class="">It outputs probabilities via Softmax.</li>
<li class="">The word with the highest score is selected: <strong>&quot;I&quot;</strong>.</li>
</ol>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-c-the-feedback-loop">Step C: The Feedback Loop<a href="#step-c-the-feedback-loop" class="hash-link" aria-label="Direct link to Step C: The Feedback Loop" title="Direct link to Step C: The Feedback Loop" translate="no">​</a></h4>
<p>The model doesn&#x27;t stop. It takes the new word (<strong>&quot;I&quot;</strong>) and feeds it back into the input. Now the input is <code>&lt;SOS&gt; + &quot;I&quot;</code>.</p>
<ul>
<li class=""><em>Cycle 2 Output:</em> <strong>&quot;love&quot;</strong></li>
<li class=""><em>Cycle 3 Output:</em> <strong>&quot;machine&quot;</strong></li>
<li class=""><em>Cycle 4 Output:</em> <strong>&quot;learning&quot;</strong></li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-d-the-stop-condition">Step D: The Stop Condition<a href="#step-d-the-stop-condition" class="hash-link" aria-label="Direct link to Step D: The Stop Condition" title="Direct link to Step D: The Stop Condition" translate="no">​</a></h4>
<p>Finally, the model predicts a special <strong>End-of-Sequence (EOS)</strong> token. This tells the system to stop generating.</p>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="transformer-variations-the-family-tree">Transformer Variations: The Family Tree<a href="#transformer-variations-the-family-tree" class="hash-link" aria-label="Direct link to Transformer Variations: The Family Tree" title="Direct link to Transformer Variations: The Family Tree" translate="no">​</a></h2>
<p>While the example above used both the Encoder and Decoder (Sequence-to-Sequence), modern AI has branched into three distinct families based on which parts they keep.</p>
<table><thead><tr><th style="text-align:left">Type</th><th style="text-align:left">Architecture</th><th style="text-align:left">Best For</th><th style="text-align:left">Popular Models</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Encoder-Only</strong></td><td style="text-align:left">Inputs &amp; Outputs are same length.</td><td style="text-align:left">Classification, Sentiment Analysis, Entity Recognition.</td><td style="text-align:left"><strong>BERT</strong></td></tr><tr><td style="text-align:left"><strong>Encoder-Decoder</strong></td><td style="text-align:left">Input &amp; Output lengths vary.</td><td style="text-align:left">Translation, Text Summarization.</td><td style="text-align:left"><strong>BART</strong>, <strong>T5</strong></td></tr><tr><td style="text-align:left"><strong>Decoder-Only</strong></td><td style="text-align:left">Generates new tokens from a prompt.</td><td style="text-align:left">General Text Generation (Chatbots, Code, Creative Writing).</td><td style="text-align:left"><strong>GPT-4</strong>, <strong>LLaMA</strong>, <strong>Bloom</strong></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">​</a></h2>
<p>The Transformer architecture changed AI by allowing models to:</p>
<ol>
<li class="">Process data in parallel (Speed).</li>
<li class="">Understand the context of every word relative to every other word (Attention).</li>
<li class="">Learn multiple nuances of language simultaneously (Multi-Head).</li>
</ol>
<p>This architecture is the foundation upon which tools like ChatGPT and Claude are built.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="references">References:<a href="#references" class="hash-link" aria-label="Direct link to References:" title="Direct link to References:" translate="no">​</a></h2>
<ol>
<li class=""><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/abs/1706.03762</a></li>
<li class=""><a href="https://learn.deeplearning.ai" target="_blank" rel="noopener noreferrer" class="">https://learn.deeplearning.ai</a></li>
</ol></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col noPrint_WFHX"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/transformer-architecture.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/blog/cache-stampede-thundering-herd"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">The Thundering Herd - Understanding and Solving the Cache Stampede</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/blog/offline-online-indicator"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Design an Online/Offline Indicator (Presence Service)</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#the-high-level-view-encoder-and-decoder" class="table-of-contents__link toc-highlight">The High-Level View: Encoder and Decoder</a></li><li><a href="#step-1-tokenization-words-to-numbers" class="table-of-contents__link toc-highlight">Step 1: Tokenization (Words to Numbers)</a><ul><li><a href="#what-is-tokenization" class="table-of-contents__link toc-highlight">What is Tokenization?</a></li></ul></li><li><a href="#step-2-embeddings-numbers-to-meaning" class="table-of-contents__link toc-highlight">Step 2: Embeddings (Numbers to Meaning)</a><ul><li><a href="#the-concept-high-dimensional-space" class="table-of-contents__link toc-highlight">The Concept: High-Dimensional Space</a></li></ul></li><li><a href="#step-3-positional-encoding-adding-order" class="table-of-contents__link toc-highlight">Step 3: Positional Encoding (Adding Order)</a><ul><li><a href="#the-fix-positional-encoding" class="table-of-contents__link toc-highlight">The Fix: Positional Encoding</a></li></ul></li><li><a href="#step-4-self-attention-the-core-magic" class="table-of-contents__link toc-highlight">Step 4: Self-Attention (The Core Magic)</a><ul><li><a href="#example-who-has-the-book" class="table-of-contents__link toc-highlight">Example: &quot;Who has the book?&quot;</a></li></ul></li><li><a href="#step-5-multi-head-attention-many-perspectives" class="table-of-contents__link toc-highlight">Step 5: Multi-Head Attention (Many Perspectives)</a><ul><li><a href="#what-is-a-head" class="table-of-contents__link toc-highlight">What is a &quot;Head&quot;?</a></li></ul></li><li><a href="#step-6-the-output-logits-and-probabilities" class="table-of-contents__link toc-highlight">Step 6: The Output (Logits and Probabilities)</a><ul><li><a href="#breaking-down-the-jargon" class="table-of-contents__link toc-highlight">Breaking Down the Jargon</a></li></ul></li><li><a href="#putting-it-all-together-an-end-to-end-example" class="table-of-contents__link toc-highlight">Putting It All Together: An End-to-End Example</a><ul><li><a href="#phase-1-the-encoder-reading--understanding" class="table-of-contents__link toc-highlight">Phase 1: The Encoder (Reading &amp; Understanding)</a></li><li><a href="#phase-2-the-decoder-generating-the-translation" class="table-of-contents__link toc-highlight">Phase 2: The Decoder (Generating the Translation)</a></li></ul></li><li><a href="#transformer-variations-the-family-tree" class="table-of-contents__link toc-highlight">Transformer Variations: The Family Tree</a></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References:</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"></div></footer></div>
</body>
</html>