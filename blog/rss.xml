<?xml version="1.0" encoding="utf-8"?><?xml-stylesheet type="text/xsl" href="rss.xsl"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Ashish Rathod Blog</title>
        <link>https://arathod02.github.io/docs/blog</link>
        <description>Ashish Rathod Blog</description>
        <lastBuildDate>Wed, 10 Dec 2025 10:55:15 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[Design an Online/Offline Indicator (Presence Service)]]></title>
            <link>https://arathod02.github.io/docs/blog/offline-online-indicator</link>
            <guid>https://arathod02.github.io/docs/blog/offline-online-indicator</guid>
            <pubDate>Wed, 10 Dec 2025 10:55:15 GMT</pubDate>
            <description><![CDATA[Designing a scalable presence system for 1 billion users.]]></description>
            <content:encoded><![CDATA[<p>Designing a system to indicate if a user is <strong>Online</strong> or <strong>Offline</strong> (and their last seen timestamp) sounds simple on the surface, but becomes a massive engineering challenge when scaling to <strong>1 Billion users</strong>.</p>
<p>In this post, we will breakdown the design of a Presence Service suitable for a massive social network or chat application like WhatsApp or Facebook Messenger.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="problem-statement">Problem Statement<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#problem-statement" class="hash-link" aria-label="Direct link to Problem Statement" title="Direct link to Problem Statement" translate="no">​</a></h2>
<p>We need to build a service that indicates the availability status of a user's connections.</p>
<p><strong>The Functional Requirements:</strong></p>
<ol>
<li class="">Indicate if a friend/connection is currently <strong>Online</strong>.</li>
<li class="">If offline, display the <strong>Last Seen</strong> timestamp.</li>
</ol>
<p><strong>The Scale:</strong></p>
<ul>
<li class=""><strong>Total Users:</strong> ~1 Billion.</li>
<li class=""><strong>Concurrent Online Users:</strong> ~500 Million.</li>
<li class=""><strong>Latency:</strong> The status needs to be near real-time.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="high-level-strategy-push-vs-pull">High-Level Strategy: Push vs. Pull<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#high-level-strategy-push-vs-pull" class="hash-link" aria-label="Direct link to High-Level Strategy: Push vs. Pull" title="Direct link to High-Level Strategy: Push vs. Pull" translate="no">​</a></h2>
<p>How do we keep the server updated about the client's status?</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="option-1-pull-polling">Option 1: Pull (Polling)<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#option-1-pull-polling" class="hash-link" aria-label="Direct link to Option 1: Pull (Polling)" title="Direct link to Option 1: Pull (Polling)" translate="no">​</a></h3>
<p>The server periodically connects to the client to ask, "Are you there?"</p>
<ul>
<li class=""><strong>Verdict:</strong> ❌ <strong>Impossible.</strong></li>
<li class=""><strong>Reasoning:</strong> In a mobile/NAT environment, servers cannot initiate connections to clients easily. Furthermore, polling 1B users is computationally wasteful.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="option-2-push-heartbeat">Option 2: Push (Heartbeat)<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#option-2-push-heartbeat" class="hash-link" aria-label="Direct link to Option 2: Push (Heartbeat)" title="Direct link to Option 2: Push (Heartbeat)" translate="no">​</a></h3>
<p>The client sends a signal to the server periodically saying, "I am alive."</p>
<ul>
<li class=""><strong>Verdict:</strong> ✅ <strong>Selected.</strong></li>
<li class=""><strong>Reasoning:</strong> This is the standard pattern for presence. If the server stops receiving heartbeats (after a timeout threshold), the user is marked offline.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-protocol-http-vs-websockets">The Protocol: HTTP vs. WebSockets<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#the-protocol-http-vs-websockets" class="hash-link" aria-label="Direct link to The Protocol: HTTP vs. WebSockets" title="Direct link to The Protocol: HTTP vs. WebSockets" translate="no">​</a></h2>
<p>We established a "Push" model, but how should the client push this data?</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-resthttp-based">1. REST/HTTP Based<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#1-resthttp-based" class="hash-link" aria-label="Direct link to 1. REST/HTTP Based" title="Direct link to 1. REST/HTTP Based" translate="no">​</a></h3>
<p>The client sends a <code>POST /health</code> request every <em>N</em> seconds.</p>
<p><strong>Why this fails at scale:</strong></p>
<ul>
<li class=""><strong>Overhead:</strong> HTTP is stateless. Every heartbeat requires a full 3-way TCP handshake (if not using keep-alive efficiently), SSL handshake overhead, and heavy HTTP headers.</li>
<li class=""><strong>Traffic:</strong> With 500M concurrent users sending a heartbeat every 10 seconds, that is <strong>50 Million requests per second</strong>. Most of the data transferred would be HTTP headers, not the actual status payload.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-persistent-websockets">2. Persistent WebSockets<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#2-persistent-websockets" class="hash-link" aria-label="Direct link to 2. Persistent WebSockets" title="Direct link to 2. Persistent WebSockets" translate="no">​</a></h3>
<p>The client opens a long-lived, bi-directional connection with the server.</p>
<p><strong>Why this is the winner:</strong></p>
<ul>
<li class=""><strong>Reduced Overhead:</strong> Once the connection is established, data frames have minimal overhead (just a few bytes). There are no repeated headers or handshakes.</li>
<li class=""><strong>Real-time:</strong> The server knows <em>immediately</em> if a connection is severed (TCP FIN or RST).</li>
<li class=""><strong>Bi-directional:</strong> It allows the server to push status updates of <em>friends</em> back to the user over the same channel.</li>
</ul>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>The "Disconnect" Fallacy</div><div class="admonitionContent_BuS1"><p>A common misconception is that WebSockets natively handle all disconnects via an <code>onDisconnect</code> event.</p><ul>
<li class=""><strong>Clean Disconnect:</strong> If a user clicks "Logout", the client sends a TCP FIN. The server knows immediately.</li>
<li class=""><strong>Dirty Disconnect:</strong> If the user loses internet connectivity or the connection is broken for any reason, <strong>the server receives nothing.</strong></li>
<li class=""><strong>The Fix:</strong> We must implement an <strong>Application-Level Heartbeat</strong>. If the server doesn't receive a "Ping" frame or message within $N$ seconds, it forcibly closes the socket and marks the user offline.</li>
</ul></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Scaling WebSockets</div><div class="admonitionContent_BuS1"><p>Scaling persistent connections is harder than scaling stateless HTTP.</p><ol>
<li class=""><strong>OS Limits:</strong> You must tune the kernel to allow &gt;65k open file descriptors (ephemeral ports) per server.</li>
<li class=""><strong>Load Balancing:</strong> You need a Layer 7 Load Balancer that supports "Sticky Sessions" effectively, though for a pure presence service, state can be externalized.</li>
<li class=""><strong>Memory:</strong> Holding 500M open connections requires massive RAM across your fleet of connection handlers (Gateway Service).</li>
</ol></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="database-design--estimation">Database Design &amp; Estimation<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#database-design--estimation" class="hash-link" aria-label="Direct link to Database Design &amp; Estimation" title="Direct link to Database Design &amp; Estimation" translate="no">​</a></h2>
<p>The compute layer is pretty. All it has to do is upon receiving the request, perform a key value based insert or lookup and simply return back to the client. The complexity lies in the <strong>storage layer</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="query-patterns">Query Patterns<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#query-patterns" class="hash-link" aria-label="Direct link to Query Patterns" title="Direct link to Query Patterns" translate="no">​</a></h3>
<ol>
<li class=""><strong>Write (Heavy):</strong> Update User <em>A</em>'s timestamp (Heartbeat).</li>
<li class=""><strong>Read (Heavy):</strong> Get status for User <em>A</em>'s friends/connections when User <em>A</em> opens the app.</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="data-schema">Data Schema<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#data-schema" class="hash-link" aria-label="Direct link to Data Schema" title="Direct link to Data Schema" translate="no">​</a></h3>
<p>We need a simple Key-Value pair.</p>
<table><thead><tr><th style="text-align:left">Field</th><th style="text-align:left">Type</th><th>Size</th></tr></thead><tbody><tr><td style="text-align:left">UserID</td><td style="text-align:left">Integer</td><td>4 Bytes</td></tr><tr><td style="text-align:left">LastSeen</td><td style="text-align:left">Epoch (Int)</td><td>4 Bytes</td></tr><tr><td style="text-align:left"><strong>Total</strong></td><td style="text-align:left"></td><td><strong>8 Bytes</strong></td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="capacity-planning">Capacity Planning<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#capacity-planning" class="hash-link" aria-label="Direct link to Capacity Planning" title="Direct link to Capacity Planning" translate="no">​</a></h3>
<p>With 1 Billion users, do we need massive storage?</p>
<p>1,000,000,000 users * 8 bytes = 8 GB</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Insight</div><div class="admonitionContent_BuS1"><p>We only need <strong>~8 GB</strong> of storage to hold the state of every user on the planet. This entire dataset can fit into the RAM of a single modern server instance.</p></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="managing-online-state-lifecycle">Managing "Online" State Lifecycle<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#managing-online-state-lifecycle" class="hash-link" aria-label="Direct link to Managing &quot;Online&quot; State Lifecycle" title="Direct link to Managing &quot;Online&quot; State Lifecycle" translate="no">​</a></h2>
<p>How do we decide when to switch a user from Online to Offline? We have three strategies.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-the-cron-job-reaper">1. The Cron Job Reaper<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#1-the-cron-job-reaper" class="hash-link" aria-label="Direct link to 1. The Cron Job Reaper" title="Direct link to 1. The Cron Job Reaper" translate="no">​</a></h3>
<p>A background process scans the database every few minutes and deletes entries older than $N$ minutes.</p>
<ul>
<li class=""><strong>Pros:</strong> Keeps DB clean eventually.</li>
<li class=""><strong>Cons:</strong> <strong>Terrible at scale.</strong> Scanning a table of 500M rows every minute creates massive read pressure and locking issues. The "Offline" status will always be laggy.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-connection-events-explicit-disconnect">2. Connection Events (Explicit Disconnect)<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#2-connection-events-explicit-disconnect" class="hash-link" aria-label="Direct link to 2. Connection Events (Explicit Disconnect)" title="Direct link to 2. Connection Events (Explicit Disconnect)" translate="no">​</a></h3>
<p>Leverage WebSocket callbacks (<code>onConnect</code>, <code>onDisconnect</code>) to update the DB.</p>
<ul>
<li class=""><strong>Pros:</strong> extremely efficient. Writes only happen on state changes.</li>
<li class=""><strong>Cons:</strong> Unreliable. If a user loses network (enters a tunnel) or the app crashes, the <code>onDisconnect</code> event might never fire sent to the server. The user will appear "Online" forever (a Zombie session).</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-database-ttl-time-to-live">3. Database TTL (Time-To-Live)<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#3-database-ttl-time-to-live" class="hash-link" aria-label="Direct link to 3. Database TTL (Time-To-Live)" title="Direct link to 3. Database TTL (Time-To-Live)" translate="no">​</a></h3>
<p>Use the database's native feature to auto-expire keys. The Heartbeat simply resets the TTL.</p>
<ul>
<li class=""><strong>Pros:</strong> Handles "unclean" disconnects gracefully. If the heartbeat stops, the key vanishes automatically. No manual cleanup required.</li>
<li class=""><strong>Cons:</strong> Moderate write load (every heartbeat is a write to reset the TTL).</li>
</ul>
<p><strong>Verdict:</strong> We will use <strong>Option 3 (TTL)</strong> as the primary mechanism, potentially optimized by Option 2 (explicitly deleting the key on a clean logout to avoid the TTL wait).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="database-selection-redis-vs-dynamodb">Database Selection: Redis vs. DynamoDB<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#database-selection-redis-vs-dynamodb" class="hash-link" aria-label="Direct link to Database Selection: Redis vs. DynamoDB" title="Direct link to Database Selection: Redis vs. DynamoDB" translate="no">​</a></h2>
<p>We need a Key-Value store that handles massive write throughput.</p>
<p><strong>The Math:</strong></p>
<ul>
<li class="">500 Million concurrent users.</li>
<li class="">Heartbeat interval: 30 seconds.</li>
<li class="">Throughput = $500,000,000 / 30 \approx$ <strong>16.6 Million Writes/Second</strong>.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="candidate-1-amazon-dynamodb">Candidate 1: Amazon DynamoDB<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#candidate-1-amazon-dynamodb" class="hash-link" aria-label="Direct link to Candidate 1: Amazon DynamoDB" title="Direct link to Candidate 1: Amazon DynamoDB" translate="no">​</a></h3>
<ul>
<li class=""><strong>Pros:</strong> Serverless, high durability, multi-region replication (Global Tables).</li>
<li class=""><strong>Cons:</strong> <strong>Cost and Hot Partitions.</strong>
<ul>
<li class="">Cost: DynamoDB charges by <strong>Write Capacity Units (WCUs)</strong>.<!-- -->
<ul>
<li class="">16.6 Million writes/sec = <strong>16.6 Million WCUs</strong>.</li>
<li class="">Cost per WCU (Provisioned) $\approx $0.00065$ / hour.</li>
<li class=""><strong>Hourly Cost:</strong> $$10,833$.</li>
<li class=""><strong>Monthly Cost:</strong> <strong>~$7.9 Million / Month</strong>.</li>
</ul>
</li>
<li class="">Hot Partition: In DynamoDB, a single partition is strictly limited to <strong>1,000 WCUs</strong>. If 2,000 users map to the same partition key, requests get throttled.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="candidate-2-redis-the-winner">Candidate 2: Redis (The Winner)<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#candidate-2-redis-the-winner" class="hash-link" aria-label="Direct link to Candidate 2: Redis (The Winner)" title="Direct link to Candidate 2: Redis (The Winner)" translate="no">​</a></h3>
<p>Redis is an in-memory store. We are limited by CPU/Network throughput per node.</p>
<ul>
<li class=""><strong>Pros:</strong>
<ul>
<li class=""><strong>In-Memory Speed:</strong> Sub-millisecond reads/writes.</li>
<li class=""><strong>Native TTL:</strong> Redis handles key expiration natively and efficiently.</li>
<li class=""><strong>Cost Effective:</strong>
<ul>
<li class="">Redis is an in-memory store. We are limited by CPU/Network throughput per node.</li>
<li class="">A single robust Redis node (e.g., AWS <code>r7g.xlarge</code>) can handle <strong>~600,000 writes/sec</strong>. (<strong>Benchmark</strong>: <a href="https://aws.plainenglish.io/aws-elasticache-a-performance-and-cost-analysis-of-redis-7-1-vs-valkey-7-2-bfac4fb5c22a" target="_blank" rel="noopener noreferrer" class="">https://aws.plainenglish.io/aws-elasticache-a-performance-and-cost-analysis-of-redis-7-1-vs-valkey-7-2-bfac4fb5c22a</a>)</li>
<li class="">Nodes required: $16,600,000 / 600,000 \approx$ <strong>28 Shards</strong>.</li>
<li class="">Cost per node $\approx $0.30$ / hour.</li>
<li class=""><strong>Monthly Cost:</strong> $28 * $0.30 * 730 hours = <strong>~$6132 / Month</strong>.</li>
</ul>
</li>
</ul>
</li>
<li class=""><strong>Cons:</strong>
<ul>
<li class=""><strong>Persistence:</strong> If Redis crashes, we lose "Last Seen" data (unless AOF mode is enabled, which slows performance).</li>
</ul>
</li>
<li class=""><strong>Mitigation:</strong> For a Presence system, <em>ephemeral</em> data loss is acceptable. If Redis crashes, users briefly appear offline until their next heartbeat (seconds later).</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Cost Epiphany</div><div class="admonitionContent_BuS1"><p>By choosing Redis over DynamoDB for this high-throughput/ephemeral workload, we save the company roughly <strong>$7.89 Million per month</strong>.</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="final-architecture">Final Architecture<a href="https://arathod02.github.io/docs/blog/offline-online-indicator#final-architecture" class="hash-link" aria-label="Direct link to Final Architecture" title="Direct link to Final Architecture" translate="no">​</a></h2>
]]></content:encoded>
        </item>
    </channel>
</rss>