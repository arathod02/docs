<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-list-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Blog | Ashish Rathod</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://arathod02.github.io/docs/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://arathod02.github.io/docs/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://arathod02.github.io/docs/blog/page/2"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="Blog | Ashish Rathod"><meta data-rh="true" name="description" content="Blog"><meta data-rh="true" property="og:description" content="Blog"><meta data-rh="true" name="docusaurus_tag" content="blog_posts_list"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_posts_list"><link data-rh="true" rel="icon" href="/docs/img/favicon.jpg"><link data-rh="true" rel="canonical" href="https://arathod02.github.io/docs/blog/page/2"><link data-rh="true" rel="alternate" href="https://arathod02.github.io/docs/blog/page/2" hreflang="en"><link data-rh="true" rel="alternate" href="https://arathod02.github.io/docs/blog/page/2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"Blog","@id":"https://arathod02.github.io/docs/blog/page/2","mainEntityOfPage":"https://arathod02.github.io/docs/blog/page/2","headline":"Blog","description":"Blog","blogPost":[{"@type":"BlogPosting","@id":"https://arathod02.github.io/docs/blog/transformer-architecture","mainEntityOfPage":"https://arathod02.github.io/docs/blog/transformer-architecture","url":"https://arathod02.github.io/docs/blog/transformer-architecture","headline":"The Transformer Architecture","name":"The Transformer Architecture","description":"The release of the \"Attention is All You Need\" paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the Transformer architecture, which powers the Generative AI explosion we see today.","datePublished":"2025-12-11T00:00:00.000Z","author":{"@type":"Person","name":"Ashish Rathod","description":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","image":"https://github.com/arathod02.png"},"keywords":[]},{"@type":"BlogPosting","@id":"https://arathod02.github.io/docs/blog/offline-online-indicator","mainEntityOfPage":"https://arathod02.github.io/docs/blog/offline-online-indicator","url":"https://arathod02.github.io/docs/blog/offline-online-indicator","headline":"Design an Online/Offline Indicator (Presence Service)","name":"Design an Online/Offline Indicator (Presence Service)","description":"Designing a scalable presence system for 1 billion users.","datePublished":"2025-12-10T10:55:15.000Z","author":{"@type":"Person","name":"Ashish Rathod","description":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","image":"https://github.com/arathod02.png"},"keywords":[]}]}</script><link rel="alternate" type="application/rss+xml" href="/docs/blog/rss.xml" title="Ashish Rathod RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/docs/blog/atom.xml" title="Ashish Rathod Atom Feed"><link rel="stylesheet" href="/docs/assets/css/styles.aade1aad.css">
<script src="/docs/assets/js/runtime~main.04393b3f.js" defer="defer"></script>
<script src="/docs/assets/js/main.d058adcd.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")),document.documentElement.setAttribute("data-theme-choice",t||"system")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><link rel="preload" as="image" href="/docs/img/favicon.jpg"><link rel="preload" as="image" href="https://github.com/arathod02.png"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/docs/"><div class="navbar__logo"><img src="/docs/img/favicon.jpg" alt="Ashish Rathod Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/docs/img/favicon.jpg" alt="Ashish Rathod Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Ashish Rathod</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/blog">Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2025</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/concurrent-replace">Avoid &quot;REPLACE INTO&quot;</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/slack-design">Slack Architecture</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/distributed-key-value-store">Distributed Key-Value Store</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/db-pessimistic-optimistic-locking">Deep Dive into Database Pessimistic &amp; Optimistic Locking</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/proxy-server">Database Proxy Servers</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/server-sent-events-explained">Real-Time Communication with Server-Sent Events (SSE)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/application-level-sharding-design">Implementing Shard Aware Application</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/scaling-distributed-systems">Scaling Distributed Systems (Focus on Databases)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/avoid-hard-deletes">The Pains of Hard Delete</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/cache-stampede-thundering-herd">The Thundering Herd - Understanding and Solving the Cache Stampede</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/transformer-architecture">The Transformer Architecture</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/docs/blog/offline-online-indicator">Presence Service</a></li></ul></div></nav></aside><main class="col col--7"><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/docs/blog/transformer-architecture">The Transformer Architecture</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-12-11T00:00:00.000Z">December 11, 2025</time> · <!-- -->8 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/docs/blog/authors/ashish"><img class="avatar__photo authorImage_XqGP" src="https://github.com/arathod02.png" alt="Ashish Rathod"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/docs/blog/authors/ashish"><span class="authorName_yefp" translate="no">Ashish Rathod</span></a></div><small class="authorTitle_nd0D" title="Ex-Intuit Staff Engineer">Ex-Intuit Staff Engineer</small><div class="authorSocials_rSDt"><a href="https://www.linkedin.com/in/ashish-rathod02/" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" preserveAspectRatio="xMidYMid" viewBox="0 0 256 256" style="--dark:#0a66c2;--light:#ffffffe6" class="authorSocialIcon_XYv3 linkedinSvg_FCgI"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453"></path></svg></a><a href="https://github.com/arathod02" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 256 250" preserveAspectRatio="xMidYMid" style="--dark:#000;--light:#fff" class="authorSocialIcon_XYv3 githubSvg_Uu4N"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"></path></svg></a></div></div></div></div></div></header><div class="markdown"><p>The release of the &quot;Attention is All You Need&quot; paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the <strong>Transformer</strong> architecture, which powers the Generative AI explosion we see today.</p>
<p>But how does a Transformer actually &quot;read&quot; and &quot;understand&quot; a sentence?</p>
<p>In this post, we will walk through the architecture step-by-step, transforming raw text into understanding using the logic from the original paper.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-high-level-view-encoder-and-decoder">The High-Level View: Encoder and Decoder<a href="#the-high-level-view-encoder-and-decoder" class="hash-link" aria-label="Direct link to The High-Level View: Encoder and Decoder" title="Direct link to The High-Level View: Encoder and Decoder" translate="no">​</a></h2>
<p>At its highest level, the Transformer is a statistical calculator. It doesn&#x27;t &quot;know&quot; English or Java; it knows math. The architecture is split into two distinct parts:</p>
<ol>
<li class=""><strong>The Encoder:</strong> Processes the input data.</li>
<li class=""><strong>The Decoder:</strong> Generates the output.</li>
</ol>
<p>They work in conjunction, though in modern models (like GPT), we often see decoder-only architectures. For this guide, we will follow the flow from Input (bottom) to Output (top).</p>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-1-tokenization-words-to-numbers">Step 1: Tokenization (Words to Numbers)<a href="#step-1-tokenization-words-to-numbers" class="hash-link" aria-label="Direct link to Step 1: Tokenization (Words to Numbers)" title="Direct link to Step 1: Tokenization (Words to Numbers)" translate="no">​</a></h2>
<p>Machine learning models cannot process raw text. They need numbers. Before passing a sentence like <em>&quot;The teacher has the book&quot;</em> into the model, we must <strong>tokenize</strong> it.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-tokenization">What is Tokenization?<a href="#what-is-tokenization" class="hash-link" aria-label="Direct link to What is Tokenization?" title="Direct link to What is Tokenization?" translate="no">​</a></h3>
<p>Simply put, this converts words into numbers. Imagine a giant dictionary where every word has a unique ID.</p>
<ul>
<li class="">&quot;The&quot; -&gt; 101</li>
<li class="">&quot;Teacher&quot; -&gt; 2045</li>
<li class="">&quot;Book&quot; -&gt; 3011</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>AI Terminology</div><div class="admonitionContent_BuS1"><p><strong>Tokenizer:</strong> A tool that breaks text into smaller chunks (tokens). These can be whole words or parts of words. Ideally, if you train a model with a specific tokenizer, you must use the exact same one when generating text.</p></div></div>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-2-embeddings-numbers-to-meaning">Step 2: Embeddings (Numbers to Meaning)<a href="#step-2-embeddings-numbers-to-meaning" class="hash-link" aria-label="Direct link to Step 2: Embeddings (Numbers to Meaning)" title="Direct link to Step 2: Embeddings (Numbers to Meaning)" translate="no">​</a></h2>
<p>Now we have a list of numbers (IDs), but numbers don&#x27;t have &quot;meaning.&quot; To a computer, the number 100 and 101 are just close in value, but the words they represent might be totally unrelated.</p>
<p>We solve this with the <strong>Embedding Layer</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-concept-high-dimensional-space">The Concept: High-Dimensional Space<a href="#the-concept-high-dimensional-space" class="hash-link" aria-label="Direct link to The Concept: High-Dimensional Space" title="Direct link to The Concept: High-Dimensional Space" translate="no">​</a></h3>
<p>Each Token ID is matched to a <strong>Vector</strong> (a list of numbers). In the original paper, this vector size was 512.</p>
<ul>
<li class="">Imagine a 3D graph (X, Y, Z).</li>
<li class="">Words with similar meanings are plotted physically close to each other in this space.</li>
<li class="">&quot;King&quot; and &quot;Queen&quot; would be close together. &quot;Apple&quot; and &quot;Car&quot; would be far apart.</li>
</ul>
<p>The model calculates the distance (often as an angle) between these words to understand their relationship mathematically.</p>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-3-positional-encoding-adding-order">Step 3: Positional Encoding (Adding Order)<a href="#step-3-positional-encoding-adding-order" class="hash-link" aria-label="Direct link to Step 3: Positional Encoding (Adding Order)" title="Direct link to Step 3: Positional Encoding (Adding Order)" translate="no">​</a></h2>
<p>Transformers process input tokens in <strong>parallel</strong> (all at once), unlike RNNs which read word-by-word. This is great for speed, but it creates a problem: the model loses the concept of word order.</p>
<p>To the model, <em>&quot;The teacher has the book&quot;</em> and <em>&quot;The book has the teacher&quot;</em> look the same because the ingredients (words) are the same.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-fix-positional-encoding">The Fix: Positional Encoding<a href="#the-fix-positional-encoding" class="hash-link" aria-label="Direct link to The Fix: Positional Encoding" title="Direct link to The Fix: Positional Encoding" translate="no">​</a></h3>
<p>We add a &quot;timestamp&quot; or &quot;position signature&quot; to the word vectors.</p>
<ul>
<li class=""><strong>Analogy:</strong> Imagine throwing a stack of unnumbered pages into the air. If you don&#x27;t write page numbers (positional encoding) on them first, you can&#x27;t put the book back together.</li>
</ul>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-4-self-attention-the-core-magic">Step 4: Self-Attention (The Core Magic)<a href="#step-4-self-attention-the-core-magic" class="hash-link" aria-label="Direct link to Step 4: Self-Attention (The Core Magic)" title="Direct link to Step 4: Self-Attention (The Core Magic)" translate="no">​</a></h2>
<p>This is the &quot;Attention&quot; in &quot;Attention is All You Need.&quot;</p>
<p>Once the vectors enter the model, the <strong>Self-Attention Layer</strong> analyzes the relationships between tokens. It allows the model to look at a specific word and understand its context based on <em>every other word</em> in the sentence.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="example-who-has-the-book">Example: &quot;Who has the book?&quot;<a href="#example-who-has-the-book" class="hash-link" aria-label="Direct link to Example: &quot;Who has the book?&quot;" title="Direct link to Example: &quot;Who has the book?&quot;" translate="no">​</a></h3>
<p>In the sentence: <em>&quot;The teacher gave the book to the student.&quot;</em></p>
<ul>
<li class="">The word <strong>&quot;Book&quot;</strong> needs to understand who has it (Teacher) and who receives it (Student).</li>
<li class="">The attention mechanism creates strong connections (weights) between &quot;Book,&quot; &quot;Teacher,&quot; and &quot;Student,&quot; while ignoring less relevant words like &quot;the.&quot;</li>
</ul>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-5-multi-head-attention-many-perspectives">Step 5: Multi-Head Attention (Many Perspectives)<a href="#step-5-multi-head-attention-many-perspectives" class="hash-link" aria-label="Direct link to Step 5: Multi-Head Attention (Many Perspectives)" title="Direct link to Step 5: Multi-Head Attention (Many Perspectives)" translate="no">​</a></h2>
<p>The model doesn&#x27;t just do self-attention once. It uses <strong>Multi-Head Attention</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="what-is-a-head">What is a &quot;Head&quot;?<a href="#what-is-a-head" class="hash-link" aria-label="Direct link to What is a &quot;Head&quot;?" title="Direct link to What is a &quot;Head&quot;?" translate="no">​</a></h3>
<p>Think of a &quot;Head&quot; as a different lens or filter. The model runs 12 to 100 of these heads in parallel. Each head learns a different aspect of language randomly during training.</p>
<ul>
<li class=""><strong>Head 1 (The Grammar Lens):</strong> Might focus on Subject-Verb agreement.</li>
<li class=""><strong>Head 2 (The Rhyme Lens):</strong> Might focus on phonetics or poetry.</li>
<li class=""><strong>Head 3 (The Context Lens):</strong> Might focus on relationships between people (Teacher/Student).</li>
</ul>
<p>The results of all these heads are combined to give the model a complete understanding of the text.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-6-the-output-logits-and-probabilities">Step 6: The Output (Logits and Probabilities)<a href="#step-6-the-output-logits-and-probabilities" class="hash-link" aria-label="Direct link to Step 6: The Output (Logits and Probabilities)" title="Direct link to Step 6: The Output (Logits and Probabilities)" translate="no">​</a></h2>
<p>After passing through the Feed Forward Network, the model produces an output. But it doesn&#x27;t output a word immediately; it outputs a <strong>Vector of Logits</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="breaking-down-the-jargon">Breaking Down the Jargon<a href="#breaking-down-the-jargon" class="hash-link" aria-label="Direct link to Breaking Down the Jargon" title="Direct link to Breaking Down the Jargon" translate="no">​</a></h3>
<ol>
<li class=""><strong>Logits:</strong> These are raw, unnormalized scores. The model scores every single word in its dictionary on how likely it is to be the next word.<!-- -->
<ul>
<li class=""><em>Example:</em> Apple: 5.0, Ball: 1.2, Cat: -3.0</li>
</ul>
</li>
<li class=""><strong>Softmax Layer:</strong> This turns those raw scores (Logits) into probabilities (Percentages).<!-- -->
<ul>
<li class=""><em>Example:</em> Apple: 98%, Ball: 1.9%, Cat: 0.1%</li>
</ul>
</li>
</ol>
<p>The word with the highest probability is selected as the predicted token.</p>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="putting-it-all-together-an-end-to-end-example">Putting It All Together: An End-to-End Example<a href="#putting-it-all-together-an-end-to-end-example" class="hash-link" aria-label="Direct link to Putting It All Together: An End-to-End Example" title="Direct link to Putting It All Together: An End-to-End Example" translate="no">​</a></h2>
<p>We have looked at the components individually. Now, let&#x27;s watch them work together in a real scenario.</p>
<p><strong>The Task:</strong> Translate the French phrase <em>&quot;J&#x27;aime l&#x27;apprentissage automatique&quot;</em> into English.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="phase-1-the-encoder-reading--understanding">Phase 1: The Encoder (Reading &amp; Understanding)<a href="#phase-1-the-encoder-reading--understanding" class="hash-link" aria-label="Direct link to Phase 1: The Encoder (Reading &amp; Understanding)" title="Direct link to Phase 1: The Encoder (Reading &amp; Understanding)" translate="no">​</a></h3>
<p>First, the model needs to understand the input. This happens entirely in the <strong>Encoder</strong>.</p>
<ol>
<li class=""><strong>Tokenization:</strong> The raw text is broken down into numbers.</li>
<li class=""><strong>Embedding &amp; Position:</strong> These numbers are turned into vectors with position information.</li>
<li class=""><strong>Multi-Head Attention:</strong> The model analyzes the relationships. For example, it understands that <em>&quot;apprentissage&quot;</em> (learning) and <em>&quot;automatique&quot;</em> (machine) are strongly related in this context.</li>
<li class=""><strong>Deep Representation:</strong> The output of the Encoder is not text, but a rich matrix of vectors that represents the <em>meaning</em> of the French sentence.</li>
</ol>
<!-- -->
<hr>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="phase-2-the-decoder-generating-the-translation">Phase 2: The Decoder (Generating the Translation)<a href="#phase-2-the-decoder-generating-the-translation" class="hash-link" aria-label="Direct link to Phase 2: The Decoder (Generating the Translation)" title="Direct link to Phase 2: The Decoder (Generating the Translation)" translate="no">​</a></h3>
<p>This is where the magic happens. The <strong>Decoder</strong> takes the &quot;Deep Context&quot; from the Encoder and generates the English translation one word at a time. This is a <strong>loop</strong>.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-a-the-trigger">Step A: The Trigger<a href="#step-a-the-trigger" class="hash-link" aria-label="Direct link to Step A: The Trigger" title="Direct link to Step A: The Trigger" translate="no">​</a></h4>
<p>The process starts by feeding a special <strong>Start-of-Sequence (SOS)</strong> token into the Decoder.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-b-the-prediction-loop">Step B: The Prediction Loop<a href="#step-b-the-prediction-loop" class="hash-link" aria-label="Direct link to Step B: The Prediction Loop" title="Direct link to Step B: The Prediction Loop" translate="no">​</a></h4>
<ol>
<li class="">The Decoder looks at the <strong>Context</strong> (from the Encoder) and the <strong>Inputs so far</strong> (initially just <code>&lt;SOS&gt;</code>).</li>
<li class="">It runs Self-Attention to see what English words match the French meaning.</li>
<li class="">It outputs probabilities via Softmax.</li>
<li class="">The word with the highest score is selected: <strong>&quot;I&quot;</strong>.</li>
</ol>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-c-the-feedback-loop">Step C: The Feedback Loop<a href="#step-c-the-feedback-loop" class="hash-link" aria-label="Direct link to Step C: The Feedback Loop" title="Direct link to Step C: The Feedback Loop" translate="no">​</a></h4>
<p>The model doesn&#x27;t stop. It takes the new word (<strong>&quot;I&quot;</strong>) and feeds it back into the input. Now the input is <code>&lt;SOS&gt; + &quot;I&quot;</code>.</p>
<ul>
<li class=""><em>Cycle 2 Output:</em> <strong>&quot;love&quot;</strong></li>
<li class=""><em>Cycle 3 Output:</em> <strong>&quot;machine&quot;</strong></li>
<li class=""><em>Cycle 4 Output:</em> <strong>&quot;learning&quot;</strong></li>
</ul>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="step-d-the-stop-condition">Step D: The Stop Condition<a href="#step-d-the-stop-condition" class="hash-link" aria-label="Direct link to Step D: The Stop Condition" title="Direct link to Step D: The Stop Condition" translate="no">​</a></h4>
<p>Finally, the model predicts a special <strong>End-of-Sequence (EOS)</strong> token. This tells the system to stop generating.</p>
<!-- -->
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="transformer-variations-the-family-tree">Transformer Variations: The Family Tree<a href="#transformer-variations-the-family-tree" class="hash-link" aria-label="Direct link to Transformer Variations: The Family Tree" title="Direct link to Transformer Variations: The Family Tree" translate="no">​</a></h2>
<p>While the example above used both the Encoder and Decoder (Sequence-to-Sequence), modern AI has branched into three distinct families based on which parts they keep.</p>
<table><thead><tr><th style="text-align:left">Type</th><th style="text-align:left">Architecture</th><th style="text-align:left">Best For</th><th style="text-align:left">Popular Models</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Encoder-Only</strong></td><td style="text-align:left">Inputs &amp; Outputs are same length.</td><td style="text-align:left">Classification, Sentiment Analysis, Entity Recognition.</td><td style="text-align:left"><strong>BERT</strong></td></tr><tr><td style="text-align:left"><strong>Encoder-Decoder</strong></td><td style="text-align:left">Input &amp; Output lengths vary.</td><td style="text-align:left">Translation, Text Summarization.</td><td style="text-align:left"><strong>BART</strong>, <strong>T5</strong></td></tr><tr><td style="text-align:left"><strong>Decoder-Only</strong></td><td style="text-align:left">Generates new tokens from a prompt.</td><td style="text-align:left">General Text Generation (Chatbots, Code, Creative Writing).</td><td style="text-align:left"><strong>GPT-4</strong>, <strong>LLaMA</strong>, <strong>Bloom</strong></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary" translate="no">​</a></h2>
<p>The Transformer architecture changed AI by allowing models to:</p>
<ol>
<li class="">Process data in parallel (Speed).</li>
<li class="">Understand the context of every word relative to every other word (Attention).</li>
<li class="">Learn multiple nuances of language simultaneously (Multi-Head).</li>
</ol>
<p>This architecture is the foundation upon which tools like ChatGPT and Claude are built.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="references">References:<a href="#references" class="hash-link" aria-label="Direct link to References:" title="Direct link to References:" translate="no">​</a></h2>
<ol>
<li class=""><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer" class="">https://arxiv.org/abs/1706.03762</a></li>
<li class=""><a href="https://learn.deeplearning.ai" target="_blank" rel="noopener noreferrer" class="">https://learn.deeplearning.ai</a></li>
</ol></div><footer class="row docusaurus-mt-lg"></footer></article><article class="margin-bottom--xl"><header><h2 class="title_f1Hy"><a href="/docs/blog/offline-online-indicator">Design an Online/Offline Indicator (Presence Service)</a></h2><div class="container_mt6G margin-vert--md"><time datetime="2025-12-10T10:55:15.000Z">December 10, 2025</time> · <!-- -->7 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--12 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a class="avatar__photo-link" href="/docs/blog/authors/ashish"><img class="avatar__photo authorImage_XqGP" src="https://github.com/arathod02.png" alt="Ashish Rathod"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="/docs/blog/authors/ashish"><span class="authorName_yefp" translate="no">Ashish Rathod</span></a></div><small class="authorTitle_nd0D" title="Ex-Intuit Staff Engineer">Ex-Intuit Staff Engineer</small><div class="authorSocials_rSDt"><a href="https://www.linkedin.com/in/ashish-rathod02/" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" preserveAspectRatio="xMidYMid" viewBox="0 0 256 256" style="--dark:#0a66c2;--light:#ffffffe6" class="authorSocialIcon_XYv3 linkedinSvg_FCgI"><path d="M218.123 218.127h-37.931v-59.403c0-14.165-.253-32.4-19.728-32.4-19.756 0-22.779 15.434-22.779 31.369v60.43h-37.93V95.967h36.413v16.694h.51a39.907 39.907 0 0 1 35.928-19.733c38.445 0 45.533 25.288 45.533 58.186l-.016 67.013ZM56.955 79.27c-12.157.002-22.014-9.852-22.016-22.009-.002-12.157 9.851-22.014 22.008-22.016 12.157-.003 22.014 9.851 22.016 22.008A22.013 22.013 0 0 1 56.955 79.27m18.966 138.858H37.95V95.967h37.97v122.16ZM237.033.018H18.89C8.58-.098.125 8.161-.001 18.471v219.053c.122 10.315 8.576 18.582 18.89 18.474h218.144c10.336.128 18.823-8.139 18.966-18.474V18.454c-.147-10.33-8.635-18.588-18.966-18.453"></path></svg></a><a href="https://github.com/arathod02" target="_blank" rel="noopener noreferrer" class="authorSocialLink_owbf" title="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="1em" height="1em" viewBox="0 0 256 250" preserveAspectRatio="xMidYMid" style="--dark:#000;--light:#fff" class="authorSocialIcon_XYv3 githubSvg_Uu4N"><path d="M128.001 0C57.317 0 0 57.307 0 128.001c0 56.554 36.676 104.535 87.535 121.46 6.397 1.185 8.746-2.777 8.746-6.158 0-3.052-.12-13.135-.174-23.83-35.61 7.742-43.124-15.103-43.124-15.103-5.823-14.795-14.213-18.73-14.213-18.73-11.613-7.944.876-7.78.876-7.78 12.853.902 19.621 13.19 19.621 13.19 11.417 19.568 29.945 13.911 37.249 10.64 1.149-8.272 4.466-13.92 8.127-17.116-28.431-3.236-58.318-14.212-58.318-63.258 0-13.975 5-25.394 13.188-34.358-1.329-3.224-5.71-16.242 1.24-33.874 0 0 10.749-3.44 35.21 13.121 10.21-2.836 21.16-4.258 32.038-4.307 10.878.049 21.837 1.47 32.066 4.307 24.431-16.56 35.165-13.12 35.165-13.12 6.967 17.63 2.584 30.65 1.255 33.873 8.207 8.964 13.173 20.383 13.173 34.358 0 49.163-29.944 59.988-58.447 63.157 4.591 3.972 8.682 11.762 8.682 23.704 0 17.126-.148 30.91-.148 35.126 0 3.407 2.304 7.398 8.792 6.14C219.37 232.5 256 184.537 256 128.002 256 57.307 198.691 0 128.001 0Zm-80.06 182.34c-.282.636-1.283.827-2.194.39-.929-.417-1.45-1.284-1.15-1.922.276-.655 1.279-.838 2.205-.399.93.418 1.46 1.293 1.139 1.931Zm6.296 5.618c-.61.566-1.804.303-2.614-.591-.837-.892-.994-2.086-.375-2.66.63-.566 1.787-.301 2.626.591.838.903 1 2.088.363 2.66Zm4.32 7.188c-.785.545-2.067.034-2.86-1.104-.784-1.138-.784-2.503.017-3.05.795-.547 2.058-.055 2.861 1.075.782 1.157.782 2.522-.019 3.08Zm7.304 8.325c-.701.774-2.196.566-3.29-.49-1.119-1.032-1.43-2.496-.726-3.27.71-.776 2.213-.558 3.315.49 1.11 1.03 1.45 2.505.701 3.27Zm9.442 2.81c-.31 1.003-1.75 1.459-3.199 1.033-1.448-.439-2.395-1.613-2.103-2.626.301-1.01 1.747-1.484 3.207-1.028 1.446.436 2.396 1.602 2.095 2.622Zm10.744 1.193c.036 1.055-1.193 1.93-2.715 1.95-1.53.034-2.769-.82-2.786-1.86 0-1.065 1.202-1.932 2.733-1.958 1.522-.03 2.768.818 2.768 1.868Zm10.555-.405c.182 1.03-.875 2.088-2.387 2.37-1.485.271-2.861-.365-3.05-1.386-.184-1.056.893-2.114 2.376-2.387 1.514-.263 2.868.356 3.061 1.403Z"></path></svg></a></div></div></div></div></div></header><div class="markdown"><p>Designing a system to indicate if a user is <strong>Online</strong> or <strong>Offline</strong> (and their last seen timestamp) sounds simple on the surface, but becomes a massive engineering challenge when scaling to <strong>1 Billion users</strong>.</p>
<p>In this post, we will breakdown the design of a Presence Service suitable for a massive social network or chat application like WhatsApp or Facebook Messenger.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="problem-statement">Problem Statement<a href="#problem-statement" class="hash-link" aria-label="Direct link to Problem Statement" title="Direct link to Problem Statement" translate="no">​</a></h2>
<p>We need to build a service that indicates the availability status of a user&#x27;s connections.</p>
<p><strong>The Functional Requirements:</strong></p>
<ol>
<li class="">Indicate if a friend/connection is currently <strong>Online</strong>.</li>
<li class="">If offline, display the <strong>Last Seen</strong> timestamp.</li>
</ol>
<p><strong>The Scale:</strong></p>
<ul>
<li class=""><strong>Total Users:</strong> ~1 Billion.</li>
<li class=""><strong>Concurrent Online Users:</strong> ~500 Million.</li>
<li class=""><strong>Latency:</strong> The status needs to be near real-time.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="high-level-strategy-push-vs-pull">High-Level Strategy: Push vs. Pull<a href="#high-level-strategy-push-vs-pull" class="hash-link" aria-label="Direct link to High-Level Strategy: Push vs. Pull" title="Direct link to High-Level Strategy: Push vs. Pull" translate="no">​</a></h2>
<p>How do we keep the server updated about the client&#x27;s status?</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="option-1-pull-polling">Option 1: Pull (Polling)<a href="#option-1-pull-polling" class="hash-link" aria-label="Direct link to Option 1: Pull (Polling)" title="Direct link to Option 1: Pull (Polling)" translate="no">​</a></h3>
<p>The server periodically connects to the client to ask, &quot;Are you there?&quot;</p>
<ul>
<li class=""><strong>Verdict:</strong> ❌ <strong>Impossible.</strong></li>
<li class=""><strong>Reasoning:</strong> In a mobile/NAT environment, servers cannot initiate connections to clients easily. Furthermore, polling 1B users is computationally wasteful.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="option-2-push-heartbeat">Option 2: Push (Heartbeat)<a href="#option-2-push-heartbeat" class="hash-link" aria-label="Direct link to Option 2: Push (Heartbeat)" title="Direct link to Option 2: Push (Heartbeat)" translate="no">​</a></h3>
<p>The client sends a signal to the server periodically saying, &quot;I am alive.&quot;</p>
<ul>
<li class=""><strong>Verdict:</strong> ✅ <strong>Selected.</strong></li>
<li class=""><strong>Reasoning:</strong> This is the standard pattern for presence. If the server stops receiving heartbeats (after a timeout threshold), the user is marked offline.</li>
</ul>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-protocol-http-vs-websockets">The Protocol: HTTP vs. WebSockets<a href="#the-protocol-http-vs-websockets" class="hash-link" aria-label="Direct link to The Protocol: HTTP vs. WebSockets" title="Direct link to The Protocol: HTTP vs. WebSockets" translate="no">​</a></h2>
<p>We established a &quot;Push&quot; model, but how should the client push this data?</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-resthttp-based">1. REST/HTTP Based<a href="#1-resthttp-based" class="hash-link" aria-label="Direct link to 1. REST/HTTP Based" title="Direct link to 1. REST/HTTP Based" translate="no">​</a></h3>
<p>The client sends a <code>POST /health</code> request every <em>N</em> seconds.</p>
<p><strong>Why this fails at scale:</strong></p>
<ul>
<li class=""><strong>Overhead:</strong> HTTP is stateless. Every heartbeat requires a full 3-way TCP handshake (if not using keep-alive efficiently), SSL handshake overhead, and heavy HTTP headers.</li>
<li class=""><strong>Traffic:</strong> With 500M concurrent users sending a heartbeat every 10 seconds, that is <strong>50 Million requests per second</strong>. Most of the data transferred would be HTTP headers, not the actual status payload.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-persistent-websockets">2. Persistent WebSockets<a href="#2-persistent-websockets" class="hash-link" aria-label="Direct link to 2. Persistent WebSockets" title="Direct link to 2. Persistent WebSockets" translate="no">​</a></h3>
<p>The client opens a long-lived, bi-directional connection with the server.</p>
<p><strong>Why this is the winner:</strong></p>
<ul>
<li class=""><strong>Reduced Overhead:</strong> Once the connection is established, data frames have minimal overhead (just a few bytes). There are no repeated headers or handshakes.</li>
<li class=""><strong>Real-time:</strong> The server knows <em>immediately</em> if a connection is severed (TCP FIN or RST).</li>
<li class=""><strong>Bi-directional:</strong> It allows the server to push status updates of <em>friends</em> back to the user over the same channel.</li>
</ul>
<div class="theme-admonition theme-admonition-caution admonition_xJq3 alert alert--warning"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 16 16"><path fill-rule="evenodd" d="M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 0 0 0 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 0 0 .01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z"></path></svg></span>The &quot;Disconnect&quot; Fallacy</div><div class="admonitionContent_BuS1"><p>A common misconception is that WebSockets natively handle all disconnects via an <code>onDisconnect</code> event.</p><ul>
<li class=""><strong>Clean Disconnect:</strong> If a user clicks &quot;Logout&quot;, the client sends a TCP FIN. The server knows immediately.</li>
<li class=""><strong>Dirty Disconnect:</strong> If the user loses internet connectivity or the connection is broken for any reason, <strong>the server receives nothing.</strong></li>
<li class=""><strong>The Fix:</strong> We must implement an <strong>Application-Level Heartbeat</strong>. If the server doesn&#x27;t receive a &quot;Ping&quot; frame or message within $N$ seconds, it forcibly closes the socket and marks the user offline.</li>
</ul></div></div>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Scaling WebSockets</div><div class="admonitionContent_BuS1"><p>Scaling persistent connections is harder than scaling stateless HTTP.</p><ol>
<li class=""><strong>OS Limits:</strong> You must tune the kernel to allow &gt;65k open file descriptors (ephemeral ports) per server.</li>
<li class=""><strong>Load Balancing:</strong> You need a Layer 7 Load Balancer that supports &quot;Sticky Sessions&quot; effectively, though for a pure presence service, state can be externalized.</li>
<li class=""><strong>Memory:</strong> Holding 500M open connections requires massive RAM across your fleet of connection handlers (Gateway Service).</li>
</ol></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="database-design--estimation">Database Design &amp; Estimation<a href="#database-design--estimation" class="hash-link" aria-label="Direct link to Database Design &amp; Estimation" title="Direct link to Database Design &amp; Estimation" translate="no">​</a></h2>
<p>The compute layer is pretty. All it has to do is upon receiving the request, perform a key value based insert or lookup and simply return back to the client. The complexity lies in the <strong>storage layer</strong>.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="query-patterns">Query Patterns<a href="#query-patterns" class="hash-link" aria-label="Direct link to Query Patterns" title="Direct link to Query Patterns" translate="no">​</a></h3>
<ol>
<li class=""><strong>Write (Heavy):</strong> Update User <em>A</em>&#x27;s timestamp (Heartbeat).</li>
<li class=""><strong>Read (Heavy):</strong> Get status for User <em>A</em>&#x27;s friends/connections when User <em>A</em> opens the app.</li>
</ol>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="data-schema">Data Schema<a href="#data-schema" class="hash-link" aria-label="Direct link to Data Schema" title="Direct link to Data Schema" translate="no">​</a></h3>
<p>We need a simple Key-Value pair.</p>
<table><thead><tr><th style="text-align:left">Field</th><th style="text-align:left">Type</th><th>Size</th></tr></thead><tbody><tr><td style="text-align:left">UserID</td><td style="text-align:left">Integer</td><td>4 Bytes</td></tr><tr><td style="text-align:left">LastSeen</td><td style="text-align:left">Epoch (Int)</td><td>4 Bytes</td></tr><tr><td style="text-align:left"><strong>Total</strong></td><td style="text-align:left"></td><td><strong>8 Bytes</strong></td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="capacity-planning">Capacity Planning<a href="#capacity-planning" class="hash-link" aria-label="Direct link to Capacity Planning" title="Direct link to Capacity Planning" translate="no">​</a></h3>
<p>With 1 Billion users, do we need massive storage?</p>
<p>1,000,000,000 users * 8 bytes = 8 GB</p>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Insight</div><div class="admonitionContent_BuS1"><p>We only need <strong>~8 GB</strong> of storage to hold the state of every user on the planet. This entire dataset can fit into the RAM of a single modern server instance.</p></div></div>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="managing-online-state-lifecycle">Managing &quot;Online&quot; State Lifecycle<a href="#managing-online-state-lifecycle" class="hash-link" aria-label="Direct link to Managing &quot;Online&quot; State Lifecycle" title="Direct link to Managing &quot;Online&quot; State Lifecycle" translate="no">​</a></h2>
<p>How do we decide when to switch a user from Online to Offline? We have three strategies.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="1-the-cron-job-reaper">1. The Cron Job Reaper<a href="#1-the-cron-job-reaper" class="hash-link" aria-label="Direct link to 1. The Cron Job Reaper" title="Direct link to 1. The Cron Job Reaper" translate="no">​</a></h3>
<p>A background process scans the database every few minutes and deletes entries older than $N$ minutes.</p>
<ul>
<li class=""><strong>Pros:</strong> Keeps DB clean eventually.</li>
<li class=""><strong>Cons:</strong> <strong>Terrible at scale.</strong> Scanning a table of 500M rows every minute creates massive read pressure and locking issues. The &quot;Offline&quot; status will always be laggy.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="2-connection-events-explicit-disconnect">2. Connection Events (Explicit Disconnect)<a href="#2-connection-events-explicit-disconnect" class="hash-link" aria-label="Direct link to 2. Connection Events (Explicit Disconnect)" title="Direct link to 2. Connection Events (Explicit Disconnect)" translate="no">​</a></h3>
<p>Leverage WebSocket callbacks (<code>onConnect</code>, <code>onDisconnect</code>) to update the DB.</p>
<ul>
<li class=""><strong>Pros:</strong> extremely efficient. Writes only happen on state changes.</li>
<li class=""><strong>Cons:</strong> Unreliable. If a user loses network (enters a tunnel) or the app crashes, the <code>onDisconnect</code> event might never fire sent to the server. The user will appear &quot;Online&quot; forever (a Zombie session).</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="3-database-ttl-time-to-live">3. Database TTL (Time-To-Live)<a href="#3-database-ttl-time-to-live" class="hash-link" aria-label="Direct link to 3. Database TTL (Time-To-Live)" title="Direct link to 3. Database TTL (Time-To-Live)" translate="no">​</a></h3>
<p>Use the database&#x27;s native feature to auto-expire keys. The Heartbeat simply resets the TTL.</p>
<ul>
<li class=""><strong>Pros:</strong> Handles &quot;unclean&quot; disconnects gracefully. If the heartbeat stops, the key vanishes automatically. No manual cleanup required.</li>
<li class=""><strong>Cons:</strong> Moderate write load (every heartbeat is a write to reset the TTL).</li>
</ul>
<p><strong>Verdict:</strong> We will use <strong>Option 3 (TTL)</strong> as the primary mechanism, potentially optimized by Option 2 (explicitly deleting the key on a clean logout to avoid the TTL wait).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="database-selection-redis-vs-dynamodb">Database Selection: Redis vs. DynamoDB<a href="#database-selection-redis-vs-dynamodb" class="hash-link" aria-label="Direct link to Database Selection: Redis vs. DynamoDB" title="Direct link to Database Selection: Redis vs. DynamoDB" translate="no">​</a></h2>
<p>We need a Key-Value store that handles massive write throughput.</p>
<p><strong>The Math:</strong></p>
<ul>
<li class="">500 Million concurrent users.</li>
<li class="">Heartbeat interval: 30 seconds.</li>
<li class="">Throughput = $500,000,000 / 30 \approx$ <strong>16.6 Million Writes/Second</strong>.</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="candidate-1-amazon-dynamodb">Candidate 1: Amazon DynamoDB<a href="#candidate-1-amazon-dynamodb" class="hash-link" aria-label="Direct link to Candidate 1: Amazon DynamoDB" title="Direct link to Candidate 1: Amazon DynamoDB" translate="no">​</a></h3>
<ul>
<li class=""><strong>Pros:</strong> Serverless, high durability, multi-region replication (Global Tables).</li>
<li class=""><strong>Cons:</strong> <strong>Cost and Hot Partitions.</strong>
<ul>
<li class="">Cost: DynamoDB charges by <strong>Write Capacity Units (WCUs)</strong>.<!-- -->
<ul>
<li class="">16.6 Million writes/sec = <strong>16.6 Million WCUs</strong>.</li>
<li class="">Cost per WCU (Provisioned) $\approx $0.00065$ / hour.</li>
<li class=""><strong>Hourly Cost:</strong> $$10,833$.</li>
<li class=""><strong>Monthly Cost:</strong> <strong>~$7.9 Million / Month</strong>.</li>
</ul>
</li>
<li class="">Hot Partition: In DynamoDB, a single partition is strictly limited to <strong>1,000 WCUs</strong>. If 2,000 users map to the same partition key, requests get throttled.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="candidate-2-redis-the-winner">Candidate 2: Redis (The Winner)<a href="#candidate-2-redis-the-winner" class="hash-link" aria-label="Direct link to Candidate 2: Redis (The Winner)" title="Direct link to Candidate 2: Redis (The Winner)" translate="no">​</a></h3>
<p>Redis is an in-memory store. We are limited by CPU/Network throughput per node.</p>
<ul>
<li class=""><strong>Pros:</strong>
<ul>
<li class=""><strong>In-Memory Speed:</strong> Sub-millisecond reads/writes.</li>
<li class=""><strong>Native TTL:</strong> Redis handles key expiration natively and efficiently.</li>
<li class=""><strong>Cost Effective:</strong>
<ul>
<li class="">Redis is an in-memory store. We are limited by CPU/Network throughput per node.</li>
<li class="">A single robust Redis node (e.g., AWS <code>r7g.xlarge</code>) can handle <strong>~600,000 writes/sec</strong>. (<strong>Benchmark</strong>: <a href="https://aws.plainenglish.io/aws-elasticache-a-performance-and-cost-analysis-of-redis-7-1-vs-valkey-7-2-bfac4fb5c22a" target="_blank" rel="noopener noreferrer" class="">https://aws.plainenglish.io/aws-elasticache-a-performance-and-cost-analysis-of-redis-7-1-vs-valkey-7-2-bfac4fb5c22a</a>)</li>
<li class="">Nodes required: $16,600,000 / 600,000 \approx$ <strong>28 Shards</strong>.</li>
<li class="">Cost per node $\approx $0.30$ / hour.</li>
<li class=""><strong>Monthly Cost:</strong> $28 * $0.30 * 730 hours = <strong>~$6132 / Month</strong>.</li>
</ul>
</li>
</ul>
</li>
<li class=""><strong>Cons:</strong>
<ul>
<li class=""><strong>Persistence:</strong> If Redis crashes, we lose &quot;Last Seen&quot; data (unless AOF mode is enabled, which slows performance).</li>
</ul>
</li>
<li class=""><strong>Mitigation:</strong> For a Presence system, <em>ephemeral</em> data loss is acceptable. If Redis crashes, users briefly appear offline until their next heartbeat (seconds later).</li>
</ul>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>Cost Epiphany</div><div class="admonitionContent_BuS1"><p>By choosing Redis over DynamoDB for this high-throughput/ephemeral workload, we save the company roughly <strong>$7.89 Million per month</strong>.</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="final-architecture">Final Architecture<a href="#final-architecture" class="hash-link" aria-label="Direct link to Final Architecture" title="Direct link to Final Architecture" translate="no">​</a></h2>
</div><footer class="row docusaurus-mt-lg"></footer></article><nav class="pagination-nav" aria-label="Blog list page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/blog"><div class="pagination-nav__label">Newer entries</div></a></nav></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"></div></footer></div>
</body>
</html>