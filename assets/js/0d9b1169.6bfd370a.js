"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[567],{3832:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var a=n(8859),s=n(4848),r=n(8453);const i={title:"The Thundering Herd - Understanding and Solving the Cache Stampede",slug:"cache-stampede-thundering-herd",authors:"ashish",date:new Date("2025-12-11T00:00:00.000Z")},o="The Thundering Herd: Understanding and Solving the Cache Stampede Problem",h={authorsImageUrls:[void 0]},c=[{value:"<strong>The Typical Architecture</strong>",id:"the-typical-architecture",level:2},{value:"<strong>The &quot;Happy Path&quot; Flow</strong>",id:"the-happy-path-flow",level:3},{value:"<strong>The Problem: The Stampede Begins</strong>",id:"the-problem-the-stampede-begins",level:2},{value:"<strong>The Solution: Request Hedging (Debouncing)</strong>",id:"the-solution-request-hedging-debouncing",level:2},{value:"<strong>Approach 1: The Busy Wait (Spinlock)</strong>",id:"approach-1-the-busy-wait-spinlock",level:3},{value:"<strong>Pseudo-code (Busy Wait)</strong>",id:"pseudo-code-busy-wait",level:4},{value:"<strong>The Downside</strong>",id:"the-downside",level:4},{value:"<strong>Approach 2: The Wait/Notify Mechanism (JVM Locks/Futures)</strong>",id:"approach-2-the-waitnotify-mechanism-jvm-locksfutures",level:3},{value:"<strong>Pseudo-code (Wait/Notify with Futures)</strong>",id:"pseudo-code-waitnotify-with-futures",level:4},{value:"<strong>The Benefit</strong>",id:"the-benefit",level:4},{value:"<strong>The Pragmatic Scope: JVM vs. Distributed Locks</strong>",id:"the-pragmatic-scope-jvm-vs-distributed-locks",level:3}];function l(e){const t={code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.p,{children:"In the world of high-scale distributed systems, caching is our best friend. It\u2019s the primary defense layer that protects our fragile databases from the onslaught of user traffic, ensuring low latency and high throughput."}),"\n",(0,s.jsx)(t.p,{children:"But what happens when that defense layer momentarily fails exactly when you need it most?"}),"\n",(0,s.jsxs)(t.p,{children:["Welcome to the ",(0,s.jsx)(t.strong,{children:"Cache Stampede"}),' problem\u2014also known as the "Thundering Herd." It\'s a scenario where the very mechanism designed to speed up your system ends up bringing it to its knees.']}),"\n",(0,s.jsx)(t.p,{children:"Let's dive into what it is, why it happens, and the practical techniques used by major systems (like CDNs) to solve it."}),"\n",(0,s.jsx)(t.h2,{id:"the-typical-architecture",children:(0,s.jsx)(t.strong,{children:"The Typical Architecture"})}),"\n",(0,s.jsx)(t.p,{children:"Before things go wrong, let's look at how things go right."}),"\n",(0,s.jsx)(t.p,{children:'Consider a standard three-tier web architecture. We have API servers fronted by a load balancer. Sitting between these API servers and the "source of truth" (the Database) is a caching layer (like Redis or Memcached).'}),"\n",(0,s.jsx)(t.p,{children:"The goal of the cache is simple: serve data fast and reduce load on the database."}),"\n",(0,s.jsx)(t.p,{children:"Here is a high-level view of this architecture:"}),"\n",(0,s.jsx)(t.mermaid,{value:"graph TD\n    Client1[Client] --\x3e LB[Load Balancer]\n    Client2[Client] --\x3e LB\n    Client3[Client] --\x3e LB\n    LB --\x3e API1[API Server 1]\n    LB --\x3e API2[API Server 2]\n    API1 <--\x3e Cache[(Distributed Cache)]\n    API2 <--\x3e Cache\n    API1 --\x3e DB[(Database)]\n    API2 --\x3e DB\n\n    style Cache fill:#f9f,stroke:#333,stroke-width:2px\n    style DB fill:#ccf,stroke:#333,stroke-width:2px"}),"\n",(0,s.jsx)(t.h3,{id:"the-happy-path-flow",children:(0,s.jsx)(t.strong,{children:'The "Happy Path" Flow'})}),"\n",(0,s.jsx)(t.p,{children:'Under normal operation, the flow for retrieving data is a standard "Read-Through" cache pattern:'}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsx)(t.li,{children:"The API Server receives a request for a specific key."}),"\n",(0,s.jsx)(t.li,{children:"It first checks the cache."}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Cache Hit:"})," If the data is there, return it immediately. (Fast!)"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Cache Miss:"})," If the data is ",(0,s.jsx)(t.em,{children:"not"})," there:","\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Query the Database (Slow)."}),"\n",(0,s.jsx)(t.li,{children:"Populate the cache with the result for future requests."}),"\n",(0,s.jsx)(t.li,{children:"Return the result to the client."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Here is what this typically looks like in simple Java pseudo-code:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-java",children:"public class SimpleDataService {\n\n    private Cache cache;  \n    private Database db;\n\n    public String getData(String key) {  \n        // 1. Try fetching from cache  \n        String cachedValue = cache.get(key);  \n        if (cachedValue != null) {  \n            return cachedValue;  \n        }\n\n        // 2. Cache miss - fetch from source of truth  \n        String dbValue = db.query(key);\n\n        // 3. Backfill cache for next time  \n        // Note: Often done with a Time-To-Live (TTL) to ensure freshness  \n        cache.put(key, dbValue, Duration.ofMinutes(5)); \n\n        return dbValue;  \n    }  \n}\n"})}),"\n",(0,s.jsx)(t.p,{children:"This works perfectly fine for normal traffic loads. The database only sees an occasional read when cache items expire."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.h2,{id:"the-problem-the-stampede-begins",children:(0,s.jsx)(t.strong,{children:"The Problem: The Stampede Begins"})}),"\n",(0,s.jsx)(t.p,{children:"Now, imagine a scenario where a piece of content goes viral."}),"\n",(0,s.jsx)(t.p,{children:"Suddenly, you have thousands of concurrent requests hitting your load balancer for the exact same resource key (e.g., product_details_123)."}),"\n",(0,s.jsx)(t.p,{children:"If the cache entry for product_details_123 has just expired, or perhaps was evicted due to memory pressure, you have a problem."}),"\n",(0,s.jsxs)(t.p,{children:["In that brief window of time\u2014perhaps just a few hundred milliseconds before the first request can refill the cache\u2014",(0,s.jsx)(t.strong,{children:"every single concurrent request will result in a Cache Miss."})]}),"\n",(0,s.jsx)(t.p,{children:"If 5,000 requests arrive simultaneously for that missing key, all 5,000 requests will bypass the cache and bombard your database at the exact same moment."}),"\n",(0,s.jsx)(t.p,{children:"Your database, which was happily serving a few dozen requests per second, suddenly receives thousands. CPU spikes, connection pools become exhausted, queries time out, and the database might even crash. This is the Cache Stampede. It defeats the entire purpose of having a cache."}),"\n",(0,s.jsx)(t.h2,{id:"the-solution-request-hedging-debouncing",children:(0,s.jsx)(t.strong,{children:"The Solution: Request Hedging (Debouncing)"})}),"\n",(0,s.jsx)(t.p,{children:'To handle this, we need to ensure that when a "hot" cache key is missing, we don\'t let the entire herd stampede to the database. We need to designate a leader.'}),"\n",(0,s.jsxs)(t.p,{children:["We leverage a technique known as ",(0,s.jsx)(t.strong,{children:"Request Hedging"})," or ",(0,s.jsx)(t.strong,{children:"Debouncing"}),"."]}),"\n",(0,s.jsxs)(t.p,{children:["The concept is straightforward: Out of the thousands of concurrent requests for the missing key, we allow ",(0,s.jsx)(t.strong,{children:"only one"})," to proceed to the database. All other requests for that same key must ",(0,s.jsx)(t.strong,{children:"wait"})," until that first request completes the job and refills the cache. Once the cache is refilled, the waiting requests can read the data from the cache and proceed."]}),"\n",(0,s.jsx)(t.p,{children:"The idea is simple, but as always, the devil is in the implementation details. How do we make requests wait efficiently and cleanly in a highly concurrent environment? Let's look at two approaches in Java."}),"\n",(0,s.jsx)(t.h3,{id:"approach-1-the-busy-wait-spinlock",children:(0,s.jsx)(t.strong,{children:"Approach 1: The Busy Wait (Spinlock)"})}),"\n",(0,s.jsx)(t.p,{children:'In this technique, if a thread finds a cache miss, it enters a loop where it continuously checks the cache again after sleeping for a tiny duration. It "spins" until data appears.'}),"\n",(0,s.jsx)(t.h4,{id:"pseudo-code-busy-wait",children:(0,s.jsx)(t.strong,{children:"Pseudo-code (Busy Wait)"})}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-java",children:'public class BusyWaitDataService {\n\n    private Cache cache;  \n    private Database db;\n\n    public String getData(String key) {  \n        String value = cache.get(key);  \n          \n        // If cache hit, return immediately  \n        if (value != null) return value;\n\n        // Determine if I am the "leader" responsible for fetching.  \n        // Use an atomic operation like \'setIfAbsent\' (NX) in Redis.  \n        // Set a short TTL on this lock to prevent deadlocks if the service crashes.  \n        boolean acquiredLock = cache.setIfAbsent("lock::" + key, "locked", Duration.ofSeconds(5));\n\n        if (acquiredLock) {  \n            try {  \n                // I am the leader. Fetch from DB.  \n                value = db.query(key);  \n                cache.put(key, value, Duration.ofMinutes(5));  \n            } finally {  \n                // Release lock so others know fetching is done  \n                cache.delete("lock::" + key);  \n            }  \n        } else {  \n            // I am a follower. Spin and wait.  \n            while (value == null) {  \n                try {  \n                    // Sleep briefly to avoid hammering CPU  \n                    Thread.sleep(50);   \n                } catch (InterruptedException e) { Thread.currentThread().interrupt(); }  \n                  \n                // Check cache again  \n                value = cache.get(key);  \n                  \n                // Optional: Check if lock still exists, if not, break and retry fetch  \n            }  \n        }  \n        return value;  \n    }  \n}\n'})}),"\n",(0,s.jsx)(t.h4,{id:"the-downside",children:(0,s.jsx)(t.strong,{children:"The Downside"})}),"\n",(0,s.jsx)(t.p,{children:'As the name suggests, this approach makes the CPU "busy." Even though the threads are sleeping, the constant context switching and polling consumes precious CPU cycles. In high-load scenarios, this wasted CPU can become substantial. It works, but it\'s not elegant.'}),"\n",(0,s.jsx)(t.h3,{id:"approach-2-the-waitnotify-mechanism-jvm-locksfutures",children:(0,s.jsx)(t.strong,{children:"Approach 2: The Wait/Notify Mechanism (JVM Locks/Futures)"})}),"\n",(0,s.jsx)(t.p,{children:"A far more efficient approach is to use native concurrency constructs. Instead of polling, threads should block and go to sleep until they are explicitly notified that the data is ready."}),"\n",(0,s.jsx)(t.p,{children:'In modern Java, a CompletableFuture combined with a ConcurrentHashMap is a robust way to implement this "promise" pattern without getting tangled in low-level monitor locks (synchronized/wait/notify).'}),"\n",(0,s.jsx)(t.h4,{id:"pseudo-code-waitnotify-with-futures",children:(0,s.jsx)(t.strong,{children:"Pseudo-code (Wait/Notify with Futures)"})}),"\n",(0,s.jsx)(t.p,{children:'We maintain a local map of "pending database operations." If a request comes in and an operation is already pending for that key, we hook into that existing operation\'s future result.'}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-java",children:'import java.util.concurrent.*;\n\npublic class BlogService {\n   // "use threadsafe implementation here"\n   // sem_map: Tracks which keys are currently being fetched\n   private final ConcurrentHashMap<String, CountDownLatch> semMap = new ConcurrentHashMap<>();\n\n   // res_map: Temporary storage for followers to grab the result immediately\n   // Note: In a real impl, this might need a TTL (e.g., "1 min" per your note) or explicit cleanup\n   private final ConcurrentHashMap<String, String> resMap = new ConcurrentHashMap<>();\n\n   public String getBlog(String k) {\n      // 1. Check main cache first\n      String v = cache.get(k);\n      if (v != null) {\n         return v;\n      }\n\n      // 2. Check Semaphore Map (Thread-safe check)\n      // We attempt to create a "lock" (Latch) for this key.\n      CountDownLatch myLatch = new CountDownLatch(1); // Starts "blocked"\n\n      // "s = sem_map.get(k)" equivalent using atomic putIfAbsent\n      // If returns value: someone else is already fetching (we are follower)\n      // If returns null: we successfully inserted (we are leader)\n      CountDownLatch existingLatch = semMap.putIfAbsent(k, myLatch);\n\n      if (existingLatch != null) {\n         // --- FOLLOWER PATH ("if s: s.wait()") ---\n         try {\n            // Wait for the leader to signal\n            existingLatch.await();\n            return resMap.get(k);\n         } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            return null;\n         }\n      } else {\n         // --- LEADER PATH ("else") ---\n         try {\n            v = db.query(k);\n\n            // "cache.put(k, v)"\n            cache.put(k, v);\n\n            // "res_map[k] = v" (Temporary map)\n            resMap.put(k, v);\n\n            return v;\n         } finally {\n            // Open the gate for followers\n            myLatch.countDown();\n\n            //Cleanup lock\n            semMap.remove(k);\n\n            // Optional: You might schedule resMap cleanup here or rely on TTL\n         }\n      }\n   }\n}\n'})}),"\n",(0,s.jsx)(t.h4,{id:"the-benefit",children:(0,s.jsx)(t.strong,{children:"The Benefit"})}),"\n",(0,s.jsx)(t.p,{children:"This approach is highly efficient. Waiting threads are parked by the OS and consume virtually no CPU until the leader thread completes the future. It handles concurrency cleanly within a single JVM."}),"\n",(0,s.jsx)(t.h3,{id:"the-pragmatic-scope-jvm-vs-distributed-locks",children:(0,s.jsx)(t.strong,{children:"The Pragmatic Scope: JVM vs. Distributed Locks"})}),"\n",(0,s.jsxs)(t.p,{children:["A sharp observer might notice a slight flaw in Approach 2. The guardrails of approach 2 exist within the memory of a ",(0,s.jsx)(t.em,{children:"single"})," API server JVM."]}),"\n",(0,s.jsxs)(t.p,{children:["If you have a fleet of 20 API servers behind your load balancer, and a stampede occurs, ",(0,s.jsx)(t.em,{children:"one"})," request on ",(0,s.jsx)(t.em,{children:"each"})," of the 20 servers will proceed to the database."]}),"\n",(0,s.jsx)(t.p,{children:"Instead of 5,000 database hits, you will have 20 hits."}),"\n",(0,s.jsxs)(t.p,{children:["Is this perfect? No. To reduce it to exactly one global hit, you would need a ",(0,s.jsx)(t.strong,{children:"Distributed Lock"})," system (using Redis, Zookeeper, or etcd) to coordinate locking across all 20 servers."]}),"\n",(0,s.jsx)(t.p,{children:"However, distributed locks introduce significant complexity, latency, and a new point of failure."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"In practice, the JVM-level solution is often the pragmatic choice."})," Reducing 5,000 simultaneous requests down to 20 is usually sufficient to save the database. It's a massive improvement for relatively low implementation complexity."]}),"\n",(0,s.jsx)(t.p,{children:"It's worth noting that large-scale Content Delivery Networks (CDNs) like Cloudflare, Akamai, and Fastly use exactly this hedging technique at their edge locations to protect customer origin servers from getting overwhelmed when content goes viral."})]})}function d(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>i,x:()=>o});var a=n(6540);const s={},r=a.createContext(s);function i(e){const t=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:t},e.children)}},8859:e=>{e.exports=JSON.parse('{"permalink":"/docs/blog/cache-stampede-thundering-herd","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/thundering-herd-problem.md","source":"@site/blog/thundering-herd-problem.md","title":"The Thundering Herd - Understanding and Solving the Cache Stampede","description":"In the world of high-scale distributed systems, caching is our best friend. It\u2019s the primary defense layer that protects our fragile databases from the onslaught of user traffic, ensuring low latency and high throughput.","date":"2025-12-11T00:00:00.000Z","tags":[],"readingTime":7.37,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"The Thundering Herd - Understanding and Solving the Cache Stampede","slug":"cache-stampede-thundering-herd","authors":"ashish","date":"2025-12-11T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","permalink":"/docs/blog/avoid-hard-deletes"},"nextItem":{"title":"The Transformer Architecture","permalink":"/docs/blog/transformer-architecture"}}')}}]);