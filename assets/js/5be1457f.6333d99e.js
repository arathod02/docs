"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[9258],{2253:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"application-level-sharding-design","metadata":{"permalink":"/docs/blog/application-level-sharding-design","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/shard-aware-app-design.md","source":"@site/blog/shard-aware-app-design.md","title":"Implementing Shard Aware Application","description":"In my previous post, we explored the high-level strategies for scaling databases, touching upon vertical scaling, read replicas, and finally, sharding. Today, I want to double-click on Sharding, specifically focusing on the Application-Level Sharding strategy.","date":"2025-12-14T00:00:00.000Z","tags":[],"readingTime":4.47,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Implementing Shard Aware Application","slug":"application-level-sharding-design","authors":"ashish","date":"2025-12-14T00:00:00.000Z"},"unlisted":false,"nextItem":{"title":"Scaling Distributed Systems (Focus on Databases)","permalink":"/docs/blog/scaling-distributed-systems"}},"content":"In my previous post, we explored the high-level strategies for scaling databases, touching upon vertical scaling, read replicas, and finally, sharding. Today, I want to double-click on **Sharding**, specifically focusing on the **Application-Level Sharding** strategy.\\n\\nWe often talk about \\"making the application shard-aware,\\" but what does that look like in code? How do we manage thousands of connections without overwhelming the application or the database? How do we handle re-sharding without downtime?\\n\\nIn this post, I will walk you through a battle-tested architecture we implemented at a previous organization, where we managed multi-tenant data at scale using a custom **Data Access Layer (DAL)**.\\n\\n## The Metadata Control Plane\\n\\nTo build a robust shard-aware application, you cannot hardcode connection strings or shard logic. You need a dynamic **Control Plane**. We solved this by externalizing our topology into two DynamoDB tables.\\n\\n### 1. Database Metadata\\nThis table acts as the source of truth for the physical infrastructure. It decouples the *logical* definition of a database from its *physical* connection details.\\n\\n* **Key:** `ShardName` + `DatabaseName`\\n* **Attributes:**\\n    * `ConnectionDetails` (JSON): Contains endpoints for different use cases (Write, Read, Analytics), credentials, and schema names.\\n    * `CapacityMetrics`: Tracks the current load (e.g., number of Small/Medium/Large tenants) to aid in placement decisions for new tenants.\\n\\n### 2. Tenant Metadata\\nThis table serves as the routing directory. It maps a specific tenant to a logical shard.\\n\\n* **Key:** `TenantID`\\n* **Attributes:**\\n    * `ShardName`: The logical shard where this tenant resides.\\n    * `DatabaseName`: The specific database instance within that shard.\\n\\n### The Logical Relationship\\n\\nBy splitting these concerns, we create a normalized view of our topology. The Tenant Metadata points to a logical location, and the Database Metadata resolves that location to physical credentials.\\n\\n```mermaid\\nerDiagram\\n    TENANT_METADATA {\\n        string TenantID PK\\n        string ShardName\\n        string DatabaseName\\n        string TenantTier\\n    }\\n    DATABASE_METADATA {\\n        string ShardName PK\\n        string DatabaseName PK\\n        json ConnectionDetails \\"Writes, Reads, Analytics\\"\\n        string CurrentCapacity\\n    }\\n    TENANT_METADATA }|..|| DATABASE_METADATA : \\"resolves to\\"\\n```\\n\\n---\\n\\n## The Bootup Sequence: `initializeDB()`\\n\\nEfficiency is paramount. We cannot fetch connection details from DynamoDB for every single query. Instead, we cache and initialize heavy resources during application startup.\\n\\nWhen the application boots, the **Data Access Layer (DAL)** performs the following `initializeDB()` routine:\\n\\n1.  **Fetch Topology:** It scans the `Database Metadata` table to understand the entire available fleet.\\n2.  **Pool Creation:** For every unique combination of `Shard` + `Database` + `UseCase` (Read/Write), it initializes a connection pool (e.g., HikariCP).\\n3.  **In-Memory Map:** These pools are stored in a concurrent map, keyed by the logical topology identifier.\\n\\nThis ensures that all expensive TCP handshakes are done eagerly, and the app is ready to serve traffic immediately upon becoming healthy.\\n\\n```mermaid\\nsequenceDiagram\\n    participant App as Application (DAL)\\n    participant DDB as DynamoDB (DB Metadata)\\n    participant DB as Physical Databases\\n    \\n    Note over App: Application Bootup\\n    App->>DDB: Scan Database Metadata Table\\n    DDB--\x3e>App: Return List<DbConfig>\\n    \\n    loop For each DbConfig\\n        App->>App: Extract Connection JSON\\n        App->>DB: Initialize Connection Pool (HikariCP)\\n        DB--\x3e>App: Connection Established\\n        App->>App: Map.put(Shard-DB-UseCase, Pool)\\n    end\\n    \\n    Note over App: Ready to serve traffic\\n```\\n\\n---\\n\\n## Runtime Request Handling: `getConnection(tenantId)`\\n\\nOnce the application is running, the focus shifts to low-latency routing. When a request comes in for `Tenant-123`, the DAL needs to find the correct connection seamlessly.\\n\\nThe API exposed to the upper layers is simple: `getConnection(Long tenantId)`.\\n\\n1.  **Lookup:** The DAL queries the `Tenant Metadata` (often cached locally with a TTL) to find the `ShardName` and `DatabaseName` for the tenant.\\n2.  **Resolution:** It constructs the lookup key for the in-memory map.\\n3.  **Acquisition:** It borrows a connection from the pre-warmed pool and hands it to the request thread.\\n\\n```mermaid\\nflowchart TD\\n    Request[\\"Incoming Request (Tenant 123)\\"] --\x3e DAL{Data Access Layer}\\n    \\n    subgraph Phase1 [Resolution Phase]\\n    DAL --\x3e|1. Lookup Tenant| Cache[(\\"Tenant Meta Cache\\")]\\n    Cache --\x3e|Returns| Info[\\"Shard: S1, DB: DB_A\\"]\\n    end\\n    \\n    subgraph Phase2 [Connection Acquisition]\\n    Info --\x3e|2. Construct Key| Key[\\"Key: S1-DB_A-Write\\"]\\n    Key --\x3e|3. Get from Map| Map[Connection Pool Map]\\n    Map --\x3e|4. Borrow Connection| Conn[Active Connection]\\n    end\\n    \\n    Conn --\x3e|Returns| DAL\\n    DAL --\x3e|Execute Query| DB[(\\"Physical DB Shard 1\\")]\\n```\\n\\n---\\n\\n## The Strategic Benefits\\n\\nImplementing this logic within a library provides two massive architectural advantages that go beyond simple connectivity.\\n\\n### 1. Logical Representation of Infrastructure\\nThe application code never touches a raw JDBC URL.\\n* **Scenario:** You need to rotate database credentials or migrate a database to a new host URL.\\n* **Action:** You simply update the JSON in the `Database Metadata` table and trigger a refresh event in the application.\\n* **Result:** The DAL re-initializes the specific pool. No code deployment is required. The infrastructure is now configuration-driven.\\n\\n### 2. Logical Mapping & Zero-Downtime Migration\\nThe mapping between a tenant and a database is fluid, not static.\\n* **Scenario:** `Tenant-ABC` has grown too large for `Shard-1` and needs to be moved to `Shard-2`.\\n* **Action:** After replicating the data to `Shard-2`, you simply update the `Tenant Metadata` table for `Tenant-ABC`.\\n* **Result:** The very next request for `Tenant-ABC` will resolve to `Shard-2`. The application layer requires zero changes to support tenant migration, enabling seamless re-balancing of the fleet.\\n\\n## Conclusion\\n\\nBuilding a shard-aware application is an exercise in **separation of concerns**. By isolating the *topology* (Tenant Metadata) from the *infrastructure* (Database Metadata) and wrapping it all in a smart Data Access Layer, you gain the flexibility to scale, migrate, and manage your data tier without constantly refactoring your application code."},{"id":"scaling-distributed-systems","metadata":{"permalink":"/docs/blog/scaling-distributed-systems","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/scaling.md","source":"@site/blog/scaling.md","title":"Scaling Distributed Systems (Focus on Databases)","description":"In distributed systems, the four possible chokepoints that a system can run into are CPU, Memory, Network, and Disk. Whenever the system hits the thresholds on any of these chokepoints, latency increases, throughput degrades, and we typically need to scale our systems.","date":"2025-12-13T00:00:00.000Z","tags":[],"readingTime":6.29,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Scaling Distributed Systems (Focus on Databases)","slug":"scaling-distributed-systems","authors":"ashish","date":"2025-12-13T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Implementing Shard Aware Application","permalink":"/docs/blog/application-level-sharding-design"},"nextItem":{"title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","permalink":"/docs/blog/avoid-hard-deletes"}},"content":"In distributed systems, the four possible chokepoints that a system can run into are **CPU, Memory, Network, and Disk**. Whenever the system hits the thresholds on any of these chokepoints, latency increases, throughput degrades, and we typically need to scale our systems.\\n\\nScaling isn\'t just about \\"adding more\\"; it\'s about *how* and *where* you add capacity. Broadly, we have two strategies:\\n\\n1.  **Vertical Scaling**\\n2.  **Horizontal Scaling**\\n\\n## The Scaling Strategies\\n\\n### Vertical Scaling (Scale Up)\\nIn Vertical scaling, we aim to solve for bottlenecks by throwing more CPU, memory, disk, and network bandwidth to the application. Basically, we make the infrastructure bulky enough to handle more load and try to remain under the threshold of our resources.\\n\\n**Pros:** Simplifies operations (no network overhead between nodes, no distributed consensus issues).\\n**Cons:** Has a hard ceiling (hardware limits) and can get exponentially expensive.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\"Vertical Scaling\\"\\n    A[Small Server] --\x3e|Scale Up| B[Massive Server]\\n    B --\x3e|Contains| C[More CPU]\\n    B --\x3e|Contains| D[More RAM]\\n    B --\x3e|Contains| E[Higher I/O]\\n    end\\n    style A fill:#f9f,stroke:#333,stroke-width:2px\\n    style B fill:#bbf,stroke:#333,stroke-width:4px\\n```\\n\\n### Horizontal Scaling (Scale Out)\\nIn horizontal scaling, we run multiple instances of the application on multiple machines. Basically, we add more machines to the compute layer. This fleet of compute servers is typically fronted by a load balancer.\\n\\nAs the load increases, the compute layer is linearly scaled to cope with the load. Horizontal scaling also aids in fault tolerance; if one node dies, the others pick up the slack.\\n\\n```mermaid\\ngraph TD\\n    User((User)) --\x3e LB[Load Balancer]\\n    subgraph \\"Compute Fleet\\"\\n    LB --\x3e Server1[App Instance 1]\\n    LB --\x3e Server2[App Instance 2]\\n    LB --\x3e Server3[App Instance 3]\\n    end\\n    style LB fill:#f96,stroke:#333,stroke-width:2px\\n```\\n\\n:::info The \\"Unit Tech Economics\\"\\n\\nDouble-clicking a bit on the phrase *\\"As the load typically increases, the compute layer is linearly scaled\\"*: In order for this step to be efficient, it is very important to know the **\\"Unit Tech Economics\\"**.\\n\\nFor a given machine of a certain size (i.e., # of CPUs, RAM, Disk, and N/W bandwidth), what is the throughput that can be successfully and healthily served?\\n\\nThere isn\'t a magic formula for this. Each application needs to be load tested with **one machine** to arrive at the \\"Unit Tech Economics.\\" Once you have these numbers, all you need to do is extrapolate them to the actual load anticipated in production or during peak hours.\\n:::\\n\\n:::caution The \\"Infinite Scale\\" Fallacy\\nDoes this mean that with horizontal scaling of the API servers you can achieve infinite scale? **Not really.**\\n\\nWhat about your downstream systems, caches, and DBs? Will they be able to serve the spiked-up load? **NO.**\\n\\nWhen scaling, one should adopt the **bottoms-up approach**. Scaling API servers should ideally be the last component to be scaled up.\\n:::\\n---\\n\\n## Deep Dive: Scaling the Database\\n\\nWith this context set, let\'s go deep into the scaling aspects of DBs (stateful components).\\n\\n### 1. Vertical Scaling\\nAs explained above, with vertical scaling\u2014without splitting your DB instance\u2014you typically host your DB on a bigger server, i.e., a machine with more capacity in terms of CPU, Memory, Disk, and Network. Basically, you run your DB on a bulkier server.\\n\\n```mermaid\\ngraph TD\\n    App[Application Fleet] --\x3e|Writes & Reads| DB[Massive Primary DB Instance]\\n    subgraph \\"Vertical DB Scaling\\"\\n    DB\\n    end\\n    style DB fill:#bbf,stroke:#333,stroke-width:4px\\n```\\n\\n> **Real World Example: Zerodha**\\n> One of India\'s largest stock brokers, famously scaled their massive volume by avoiding premature distributed complexity. They run their core systems on a **single, massive Postgres Server**.\\n>\\n> **Summary:** Instead of rushing to sharding, Zerodha focused on optimizing queries and utilizing high-end hardware.\\n> * **How:** They use high-performance hardware (huge RAM to fit working sets in memory) and optimized Postgres configurations.\\n> * **Pros:** Simplifies architecture (no distributed transactions), ensures strict ACID compliance, and offers low latency.\\n> * **Cons:** It is a Single Point of Failure (SPOF) if not HA, and restart times can be long due to the sheer size of memory buffers.\\n>\\n> *Read more: [Zerodha Tech Blog: Working with PostgreSQL](https://zerodha.tech/blog/working-with-postgresql/)*\\n\\n### 2. Adding Read Replicas\\nIn this approach, you provision for serving read traffic. Basically, you add one or more DB servers that specifically serve read traffic. This approach helps in reserving the primary node of the DB for writes, effectively isolating read and write traffic.\\n\\n```mermaid\\ngraph TD\\n    App[Application] --\x3e|Writes| Master[Primary Node]\\n    App --\x3e|Reads| Replica1[Read Replica 1]\\n    App --\x3e|Reads| Replica2[Read Replica 2]\\n    Master -.->|Async Replication| Replica1\\n    Master -.->|Async Replication| Replica2\\n```\\n\\nIn terms of reading from the application standpoint, the application need not know about multiple servers. It can leverage a couple of solutions to simplify reader connection management:\\n\\n#### a) Reader Endpoint (AWS Aurora / RDS)\\nIf you are using AWS Aurora or RDS, you can leverage the **Reader Endpoint**. A reader endpoint is a DNS record exposed by AWS. The application simply connects to this endpoint.\\n\\nAt a high level, one can visualize the reader endpoint as a load balancer sitting in front of the DB readers. It distributes these read requests fairly amongst the available readers.\\n\\n```mermaid\\ngraph TD\\n    App[Application] --\x3e|Connects to| RE[AWS Reader Endpoint]\\n    RE --\x3e|Distributes| R1[Replica 1]\\n    RE --\x3e|Distributes| R2[Replica 2]\\n    RE --\x3e|Distributes| R3[Replica 3]\\n```\\n\\n#### b) RDS Proxy\\n**What is RDS Proxy?**\\nAmazon RDS Proxy is a fully managed, highly available database proxy that sits between your application and your relational database.\\n\\n**Use Cases:**\\n1.  **Connection Pooling:** It maintains a pool of established connections to your database, sharing them among application requests. This is crucial for serverless applications (like Lambda) that might open thousands of connections quickly.\\n2.  **Resiliency:** It reduces failover times by bypassing DNS cache propagation delays.\\n\\n```mermaid\\ngraph TD\\n    Lambda[Lambda/App] --\x3e|Open Connections| Proxy[RDS Proxy]\\n    Proxy --\x3e|Pooled Connections| DB[Database Cluster]\\n    style Proxy fill:#ff9,stroke:#333,stroke-width:2px\\n```\\n\\n### 3. Sharding\\nSharding is **not** a Day 0 approach. You typically do this when you know the user and data volume will exceed the capacity of a vertically scaled single node.\\n\\nIn Sharding, you split the data across multiple primary shards.\\n\\n```mermaid\\ngraph TD\\n    App[Application]\\n    App --\x3e|Subset A-J| Shard1[Shard 1: Users A-J]\\n    App --\x3e|Subset K-T| Shard2[Shard 2: Users K-T]\\n    App --\x3e|Subset U-Z| Shard3[Shard 3: Users U-Z]\\n```\\n\\n**How do you decide where data resides?**\\n\\n1.  **Range-Based Mapping:**\\n    * Assume you are a platform serving millions of tenants. You might say: Tenants starting with `a-j` reside in Shard 1, `k-t` in Shard 2, and `u-z` in Shard 3.\\n2.  **Static Mapping:**\\n    * In this approach, you maintain a static mapping (lookup table) between tenants and DB shards. E.g., Tenant 1 maps to Shard 1, Tenant 2 maps to Shard 1, Tenant 3 maps to Shard 2, etc.\\n\\n**Where do you host this mapping logic?**\\n\\n1.  **Application Layer**\\n    * We make the application \\"shard aware.\\" The application **KNOWS** the DB topology and determines which shard a given tenant resides in.\\n    * The application then connects to the respective DB shard to perform reads and writes.\\n\\n2.  **External Layer (Smart Proxy)**\\n    * Instead of the application knowing the topology, a proxy layer **KNOWS** the DB topology.\\n    * You configure server rules and query rules in the proxy layer. Based on the incoming query, the proxy routes the request to the correct DB shard.\\n\\n*I will write another blog to go really deep into BOTH the above techniques (Application vs. Proxy sharding). I will link those two blogs here soon.*"},{"id":"avoid-hard-deletes","metadata":{"permalink":"/docs/blog/avoid-hard-deletes","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/avoid-hard-deletes.md","source":"@site/blog/avoid-hard-deletes.md","title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","description":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","date":"2025-12-11T12:26:27.000Z","tags":[],"readingTime":18.05,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"slug":"avoid-hard-deletes","title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","sidebar_label":"The Pains of Hard Delete","description":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","authors":"ashish"},"unlisted":false,"prevItem":{"title":"Scaling Distributed Systems (Focus on Databases)","permalink":"/docs/blog/scaling-distributed-systems"},"nextItem":{"title":"The Thundering Herd - Understanding and Solving the Cache Stampede","permalink":"/docs/blog/cache-stampede-thundering-herd"}},"content":"## Summary\\n\\nIn the discipline of database architecture, the mechanism of deletion is frequently underestimated. While the creation and retrieval of data receive significant attention regarding optimization and structure, the removal of data\u2014specifically the \\"Hard Delete\\"\u2014is often treated as a trivial housekeeping operation. However, an exhaustive analysis of storage engine internals reveals that hard deletion is one of the most resource-intensive and structurally disruptive operations in a relational database management system (RDBMS).\\n\\nThis report provides a comprehensive technical analysis of the storage engines underpinning major databases, specifically MySQL\'s InnoDB and PostgreSQL\'s Heap architecture. It articulates the high costs associated with physical record removal, including B+ Tree rebalancing, locking contention, input/output (I/O) thrashing, and vacuum-induced bloat. Consequently, this document advocates for a \\"Soft Delete by Design\\" methodology, treating deletion as a logical state transition rather than a physical storage event. It concludes with a robust lifecycle management strategy that couples logical soft deletion with asynchronous, batched hard deletion to maintain long-term storage hygiene without compromising system stability.\\n\\n## 1\\\\. Storage Engine Internals\\n\\nTo comprehend the catastrophic potential of a hard delete, one must first possess a nuanced understanding of how data resides on the disk. The abstraction of a \\"table\\" composed of \\"rows\\" and \\"columns\\" is a logical convenience; the physical reality is a complex arrangement of binary pages, pointers, and trees.\\n\\n### **1.1 MySQL InnoDB: The Index-Organized Table**\\n\\nIn the MySQL ecosystem, the default storage engine is InnoDB. Its defining characteristic is that it does not merely use an index to find data; the table *is* the index. This architecture is known as a Clustered Index, implemented via a B+ Tree data structure.\\n\\n#### **1.1.1 The B+ Tree Hierarchy**The Lifecycle Strategy\\n\\nThe B+ Tree is a self-balancing tree structure designed to maintain sorted data and allow searches, sequential access, insertions, and deletions in logarithmic time. It is distinct from a binary tree in its high fan-out; a single node can have hundreds of children, allowing the tree to remain incredibly shallow (usually 3 to 4 levels deep) even for tables containing billions of rows.\\n\\n* **The Root Page:** The entry point of the tree. It resides at a fixed location and contains pointers to internal nodes.\\n* **Internal Nodes (Non-Leaf):** These nodes act as the navigational roadmap. They contain only the Primary Key values and pointers to child pages. Because they do not store the full row data, they are lightweight. A single 16KB page can store hundreds of keys, maximizing the \\"fan-out\\" capability and ensuring that traversing the tree requires minimal disk I/O.\\n* **Leaf Nodes (The Data):** This is where the physical reality of the Clustered Index becomes apparent. The leaf nodes contain the actual row data\u2014every column of every record.\\n\\n#### **1.1.2 Primary Key Ordering in Leaf Nodes**\\n\\nA critical architectural feature of InnoDB is that the leaf nodes are strictly ordered by the Primary Key. This is not a suggestion; it is a physical enforcement. If a table has a Primary Key of ID, the row with ID=1 is physically stored adjacent to ID=2 within the 16KB page.\\n\\nFurthermore, these leaf nodes are strictly linked in a doubly linked list. Each leaf page contains a pointer to the previous page and the next page. This structure enables the database to perform extremely fast range scans (e.g., SELECT \\\\* FROM orders WHERE id BETWEEN 100 AND 200\\\\) by traversing the linked list rather than returning to the root node.\\n\\n#### **1.1.3 Page Filling and Fragmentation**\\n\\nInnoDB manages storage in units called Pages, typically 16KB in size. When records are inserted sequentially (e.g., using an auto-incrementing integer), InnoDB fills these pages efficiently, leaving only a small fraction (typically 1/16th) free for future modifications. This results in a \\"Fill Factor\\" of nearly 100%, ensuring optimal disk usage and cache efficiency.\\n\\nHowever, when data is modified or deleted, this order is disturbed. A hard delete creates a physical \\"hole\\" in the page. If enough holes are created, the page density drops, leading to fragmentation\u2014a state where the database engine is caching and reading pages that are largely empty air.\\n\\n**Table 1: B+ Tree Node Characteristics**\\n\\n| Node Type | Content | Purpose | Density |\\n| :---- | :---- | :---- | :---- |\\n| **Internal Node** | Primary Keys \\\\+ Child Pointers | Navigation | High (Keys only) |\\n| **Leaf Node** | Full Row Data | Storage | Variable (Depends on Row Size) |\\n| **Leaf Linkage** | Double Pointers (Prev/Next) | Range Scans | N/A |\\n\\n```mermaid\\ngraph TD\\n    subgraph \\"Internal Nodes (Navigation Layer)\\"\\n        Root --\x3e NodeA[\\"<b>Page 10</b><br/>Keys: 10, 50\\"]\\n        Root --\x3e NodeB[\\"<b>Page 20</b><br/>Keys: 500, 700\\"]\\n    end\\n\\n    subgraph \\"Leaf Nodes (Data Layer - Ordered by PK)\\"\\n        NodeA --\x3e Leaf1\\n        NodeA --\x3e Leaf2\\n        NodeB --\x3e Leaf3\\n        NodeB --\x3e Leaf4\\n    end\\n\\n    Leaf1 <==>|Doubly Linked List| Leaf2\\n    Leaf2 <==>|Doubly Linked List| Leaf3\\n    Leaf3 <==>|Doubly Linked List| Leaf4\\n\\n    style Leaf1 fill:#d4f1f4,stroke:#00acc1,stroke-width:2px\\n    style Leaf2 fill:#d4f1f4,stroke:#00acc1,stroke-width:2px\\n    style Leaf3 fill:#d4f1f4,stroke:#00acc1,stroke-width:2px\\n    style Leaf4 fill:#d4f1f4,stroke:#00acc1,stroke-width:2px\\n```\\n\\n\\n### **1.2 PostgreSQL: The Heap and Tuple Architecture**\\n\\nPostgreSQL employs a fundamentally different storage paradigm known as the Heap. Unlike InnoDB, where the table is the index, PostgreSQL separates the table storage (the Heap) from the index storage.\\n\\n#### **1.2.1 Heap Pages and Item Pointers**\\n\\nIn PostgreSQL, data is stored in a file known as the \\"Heap.\\" This file is divided into fixed-length blocks, typically 8KB. Within these blocks, records (referred to as \\"tuples\\") are stored in an unordered fashion. A tuple is inserted into the first page that has sufficient free space, regardless of its Primary Key value.\\n\\nThis lack of inherent order necessitates a mechanism to locate data. PostgreSQL uses the Tuple Identifier (TID), often referred to as ctid. The ctid is a coordinate pair: (Block Number, Offset Number). For example, (42, 7\\\\) means the 7th item on the 42nd page.\\n\\n#### **1.2.2 The Array of Line Pointers**\\n\\nTo manage the internal organization of an 8KB page, PostgreSQL uses an array of \\"Line Pointers\\" (ItemIds) at the beginning of the page header.\\n\\n* **Indirection:** The ctid actually points to a Line Pointer, not the tuple itself. The Line Pointer then points to the byte offset where the tuple begins within the page.\\n* **Why Indirection Matters:** This allows the database to defragment the page internally (move tuples around to close gaps) without changing the ctid or updating external indexes. As long as the Line Pointer at index 7 remains, the tuple can be anywhere in the page.\\n\\n#### **1.2.3 Ordered Indexes vs. Unordered Heap**\\n\\nIt is a common misconception that PostgreSQL tables are ordered by Primary Key. They are not. The Heap is unordered.8 However, the **Primary Key Index** is a B-Tree structure that *is* strictly ordered. This index stores the Primary Key value and the corresponding ctid of the heap tuple. When a query requests a row by ID, the engine searches the ordered B-Tree Index, finds the ctid, and then retrieves the unordered tuple from the Heap.\\n\\n**Table 2: Comparison of Storage Architectures**\\n\\n| Feature | MySQL InnoDB (B+ Tree) | PostgreSQL (Heap) |\\n| :---- | :---- | :---- |\\n| **Organization** | Clustered (Data is in the Index) | Heap (Data is separate from Index) |\\n| **Ordering** | Physically ordered by Primary Key | Unordered (Insert order / Random) |\\n| **Row ID** | Primary Key | CTID (Block \\\\+ Offset) |\\n| **Secondary Index** | Points to Primary Key Value | Points to CTID |\\n| **Page Size** | 16KB (Default) | 8KB (Default) |\\n\\n```mermaid\\ngraph TB\\n    subgraph \\"Postgres Page Layout (8KB)\\"\\n        Header\\n\\n        subgraph \\"Line Pointer Array\\"\\n            LP1[ItemId 1]\\n            LP2[ItemId 2]\\n            LP3[ItemId 3]\\n            LP4\\n        end\\n\\n        subgraph \\"Tuple Storage (Unordered)\\"\\n            Tuple1\\n            Tuple3\\n            Tuple2\\n            FreeSpace\\n        end\\n    end\\n\\n    Header --\x3e LP1\\n    LP1 -.-> Tuple1\\n    LP2 -.-> Tuple2\\n    LP3 -.-> Tuple3\\n    LP4 -.->|Deleted| FreeSpace\\n\\n    style Tuple1 fill:#fff9c4,stroke:#fbc02d\\n    style Tuple2 fill:#fff9c4,stroke:#fbc02d\\n    style Tuple3 fill:#fff9c4,stroke:#fbc02d\\n    style FreeSpace fill:#eceff1,stroke:#cfd8dc\\n```\\n\\n## 2\\\\. The pain caused by Hard Deletion\\n\\nWith the storage context established, we can analyze the mechanics of the DELETE command. Far from a simple erasure, a hard delete triggers a complex sequence of internal operations that can destabilize the database.\\n\\n### **2.1 MySQL InnoDB: The Rebalancing Storm**\\n\\nIn a B+ Tree, structural integrity is paramount. The tree must remain balanced to ensure predictable performance. Deleting a row threatens this balance.\\n\\n#### **2.1.1 The Search and Destroy Mission**\\n\\nWhen a DELETE is issued, InnoDB must first traverse the tree to locate the leaf node containing the record. Once found, the record is not immediately wiped; it is \\"delete-marked.\\" This is a logical flag in the record header indicating the space is technically free but occupied by a \\"ghost\\".\\n\\n#### **2.1.2 The Merge Threshold**\\n\\nThe true cost arises when deletions accumulate. Each page has a MERGE\\\\_THRESHOLD, typically defaulting to 50%.\\n\\n1. **Underflow:** If a deletion causes the page\'s data volume to drop below this threshold, InnoDB determines that the page is inefficient.\\n2. **Locking:** The engine places locks on the page and its neighbors (sibling nodes).\\n3. **Merge Operation:** It attempts to merge the remaining records into a sibling page (left or right).\\n4. **Cascading Reparenting:** If a page is emptied and removed, the pointer in the *parent* node must be deleted. If this deletion causes the *parent* node to drop below its own threshold, the merge operation propagates upward. This \\"rebalancing storm\\" can ripple up to the root, causing massive I/O and locking overhead.\\n\\n#### **2.1.3 Secondary Index Maintenance**\\n\\nEvery secondary index in InnoDB stores the Primary Key as its pointer. If you delete a user with ID=100, Email=bob@test.com, and Status=Active, InnoDB must:\\n\\n1. Delete 100 from the Clustered Index (B+ Tree).\\n2. Delete bob@test.com from the Email Index (B+ Tree).\\n3. Delete Active from the Status Index (B+ Tree).  \\n   Each of these requires random I/O operations. While the Change Buffer helps mitigate this for non-unique indexes by caching changes, unique indexes require immediate, synchronous disk operations to enforce constraints, amplifying the I/O cost significantly.\\n\\n### **2.2 PostgreSQL**\\n\\nPostgreSQL handles deletion via Multi-Version Concurrency Control (MVCC). It does not remove data immediately; it versions it.\\n\\n#### **2.2.1 The Invisible Update**\\n\\nIn PostgreSQL, an UPDATE is effectively a DELETE followed by an INSERT. A DELETE is simply an UPDATE that puts nothing back.\\n\\n* **xmin and xmax:** Every tuple has an xmin (the transaction ID that created it) and an xmax (the transaction ID that deleted it).\\n* **Marking as Dead:** When DELETE is run, Postgres finds the tuple and sets its xmax to the current transaction ID. The data remains physically on the disk.\\n* **Visibility Rules:** Future transactions check the xmax. If xmax is set and committed, the tuple is invisible. To the storage engine, the page looks exactly the same as before, but the tuple is logically dead.\\n\\n#### **2.2.2 The Vacuum Necessity**\\n\\nSince DELETE does not free space, the table size does not decrease. It creates \\"Dead Tuples.\\" If these are not cleaned up, the table becomes \\"bloated\\"\u2014a mixture of live data and digital corpses.\\n\\n* **Autovacuum:** The autovacuum daemon runs in the background. It scans tables, looking for dead tuples that are older than any active transaction.\\n* **Freeing Space:** It marks the line pointers as \\"unused,\\" allowing new inserts to overwrite the dead space. However, this process consumes CPU and I/O bandwidth and can block schema changes.\\n\\n```mermaid\\ngraph LR\\n    subgraph \\"Postgres MVCC Delete Process\\"\\n        T1\\n        Action\\n        T2\\n        Vacuum[Vacuum Process]\\n        Free\\n\\n        T1 --\x3e|Update xmax| T2\\n        T2 --\x3e|Scanned by| Vacuum\\n        Vacuum --\x3e|Reclaim| Free\\n    end\\n```\\n\\n## 3\\\\. The Performance Impact of Hard Deletion\\n\\nThe internal mechanics described above manifest as tangible performance degradation in production environments. The impact of hard deletes is rarely linear; it is exponential relative to the volume of data and concurrency of the system.\\n\\n### **3.1 The Locking Bottleneck**\\n\\nHard deletes are blocking operations.\\n\\n* **Gap Locks (MySQL):** To preserve transaction isolation (specifically to prevent \\"Phantom Reads\\"), InnoDB places \\"Gap Locks\\" on the space *between* records. If you delete ID=10, InnoDB might lock the gap from ID=5 to ID=15. Any other transaction trying to insert ID=12 will be blocked until the delete commits. In high-concurrency systems, this leads to lock contention and \\"Lock Wait Timeout Exceeded\\" errors.\\n* **Exclusive Locks:** Both engines take exclusive locks on the specific rows being deleted. If a reporting query is reading those rows, the delete will block (or vice versa), causing system stutter.\\n\\n### **3.2 I/O Thrashing and Buffer Pool Pollution**\\n\\nDatabase performance relies heavily on caching \\"hot\\" pages in RAM (Buffer Pool).\\n\\n* **Random Access:** Hard deletes are often random (e.g., deleting users who cancelled today). This forces the database to load widely scattered pages from the disk into memory just to mark a single bit.\\n* **Dirty Pages:** Modifying a page marks it as \\"dirty.\\" Dirty pages must be flushed back to disk. A massive delete operation creates a flood of dirty pages, saturating the I/O subsystem and slowing down critical read operations.\\n\\n### **3.3 Index Bloat and Degradation**\\n\\n* **Postgres Index Bloat:** In PostgreSQL, indexes contain pointers to heap tuples. When a tuple is updated or deleted, the index entry remains until Vacuum runs. If a table is heavily churned (high delete/update rate), the indexes can grow larger than the table itself. Larger indexes equate to slower searches, as they are less likely to fit in RAM.\\n* **MySQL Fragmentation:** As described in Section 2.1.2, B+ Tree pages that are 50-60% full are inefficient. This fragmentation means that to read 1GB of actual data, the engine might need to read 2GB of disk pages, effectively halving the I/O throughput.\\n\\n## 4\\\\. The Architectural Solution: Soft Deletion\\n\\nThe \\"Soft Delete\\" pattern solves the physical storage problems by decoupling the **business intent** of deletion from the **database mechanism** of deletion. Instead of instructing the storage engine to perform a destructive structural change, the application performs a non-structural state change.\\n\\n### **4.1 Implementation Patterns**\\n\\n#### **4.1.1 The Boolean Flag**\\n\\nThe simplest implementation is a boolean column.\\n\\n* **Schema:** is\\\\_deleted BOOLEAN DEFAULT FALSE\\n* **Logic:** UPDATE table SET is\\\\_deleted \\\\= TRUE WHERE id \\\\= 1\\n* **Critique:** While lightweight, this pattern lacks context. It tells you *that* a record was deleted, but not *when*.\\n\\n#### **4.1.2 The Timestamp (Recommended)**\\n\\nThe industry standard pattern involves a nullable timestamp.\\n\\n* **Schema:** deleted\\\\_at TIMESTAMP NULL\\n* **Logic:** UPDATE table SET deleted\\\\_at \\\\= NOW() WHERE id \\\\= 1\\n* **Query:** SELECT \\\\* FROM table WHERE deleted\\\\_at IS NULL\\n* **Benefit:** This acts as a boolean flag (Null/NotNull) while simultaneously providing an audit trail of the deletion event.\\n\\n#### **4.1.3 The Status Enum**\\n\\nFor complex state machines, deletion is just one of many states.\\n\\n* **Schema:** status VARCHAR(20) CHECK (status IN (\'active\', \'pending\', \'archived\', \'deleted\'))\\n* **Benefit:** Useful when \\"deletion\\" is part of a workflow, such as a \\"Recycle Bin\\" that transitions to \\"Permanently Deleted.\\"\\n\\n### **4.2 Why Soft Deletes Resolve Storage Issues**\\n\\n1. **Zero Rebalancing:** Updating a deleted\\\\_at timestamp is an in-place update. In MySQL, since the Primary Key is not changing, the row does not move. The B+ Tree structure remains perfectly balanced. No page merges occur.\\n2. **Preserved Sequentiality:** The data remains physically adjacent. Sequential read performance is preserved.\\n3. **Reduced Locking:** An update to a non-indexed column (like deleted\\\\_at) generally requires only a row lock, avoiding the aggressive Gap Locks associated with structural removal.\\n\\n```mermaid\\ngraph TD\\nUser\\n\\nsubgraph \\"Hard Delete Path (Destructive)\\"\\nUser --\x3e|DELETE SQL| HD_Lock\\nHD_Lock --\x3e|B-Tree Rebalance| HD_Struct\\nHD_Struct --\x3e|Page Merge| HD_IO[High I/O Cost]\\nHD_IO --\x3e|Data Gone| HD_End[Irrecoverable]\\nend\\n\\nsubgraph \\"Soft Delete Path (Non-Destructive)\\"\\nUser --\x3e|UPDATE SQL| SD_Lock\\nSD_Lock --\x3e|In-Place Update| SD_Struct\\nSD_Struct --\x3e|Log Write| SD_IO[Low I/O Cost]\\nSD_IO --\x3e|Data Hidden| SD_End\\nend\\n\\nstyle HD_End fill:#ffcdd2,stroke:#c62828\\nstyle SD_End fill:#c8e6c9,stroke:#2e7d32\\n```\\n\\n## 5\\\\. Business Use Cases for Soft Deletion\\n\\nBeyond performance optimization, Soft Deletion enables critical business capabilities that Hard Deletion inherently destroys.\\n\\n### **5.1 Recoverability: The \\"Undo\\" Button**\\n\\nHuman error is an inevitability in software systems.\\n\\n* **Unintended Deletes:** Users frequently delete content accidentally on mobile devices. Soft deletes allow for an immediate \\"Undo\\" feature without complex backup restoration.\\n* **Production Safety:** Engineering history is replete with stories of developers accidentally running DELETE without a WHERE clause. With soft deletes, this catastrophe is reversible via a simple SQL UPDATE statement (UPDATE table SET deleted\\\\_at \\\\= NULL). With hard deletes, this becomes a disaster recovery scenario requiring Point-in-Time Recovery (PITR), potentially costing hours of downtime and data loss.\\n\\n### **5.2 Audit and Compliance**\\n\\nIn regulated industries, data deletion is often a legal construct rather than a physical one.\\n\\n* **Traceability:** A deleted\\\\_at column, often paired with deleted\\\\_by\\\\_user\\\\_id, provides an immutable audit trail. Organizations can answer inquiries such as \\"Who cancelled this order?\\" or \\"When was this account terminated?\\".\\n* **Legal Obligations:** Depending upon the business, some regulations might mandate data retention for 7-10 years. A user\'s request to \\"delete my account\\" must be balanced against the legal requirement to \\"retain transaction history.\\" Soft deletion satisfies the user\'s visibility requirement while satisfying the regulator\'s retention requirement.\\n\\n### **5.3 Referential Integrity and Cascading**\\n\\nRelational databases enforce integrity via Foreign Keys. You cannot delete a Parent if it has Children.\\n\\n* **Hard Delete Complexity:** To hard delete a User, you must first delete their Orders, Invoices, Logs, and Comments. This triggers a massive \\"Cascading Delete\\" transaction that can touch dozens of tables and lock millions of rows.\\n* **Soft Delete Simplicity:** You simply mark the User as deleted. The child records remain untouched (and referentially valid). The application layer is responsible for filtering out \\"Orders belonging to deleted Users\\" during display. This avoids the massive transactional overhead of cascading deletes.\\n\\n## 6\\\\. Disadvantages and Engineering Trade-offs\\n\\nSoft deletion is an architectural compromise. It solves storage and recovery issues but introduces application-layer complexity.\\n\\n### **6.1 The \\"Forgotten WHERE Clause\\"**\\n\\nThe most pervasive risk is query correctness.\\n\\n* **The Leak:** Every query in the application must now include AND deleted\\\\_at IS NULL. If a developer forgets this clause in a \\"Total Revenue\\" report, the report will incorrectly include refunded (deleted) orders.\\n* **Mitigation:**\\n    * **ORM Scopes:** Use framework features (e.g., Hibernate @Where, Eloquent SoftDeletes) to automatically inject this clause.\\n    * **Views:** Create a database view active\\\\_users that filters the data, and restrict application access to the view rather than the raw table.\\n\\n### **6.2 Index Bloat**\\n\\nSoft-deleted rows are still physically present. If a table contains 90% deleted data (e.g., a queue table), the indexes will be 90% \\"junk.\\" This reduces the effectiveness of the RAM cache, as valuable memory is wasted storing pointers to deleted data.\\n\\n## 7\\\\. The Lifecycle Strategy: Batch Hard Deletion\\n\\nWe have established that Hard Deletes are destructive, but Soft Deletes cause unlimited growth (bloat). The optimal architecture is a hybrid lifecycle: **Soft Delete for Operations, Batch Hard Delete for Maintenance.**\\n\\n### **7.1 The \\"Nightly\\" Batch Deletion Job**\\n\\nThe goal is to decouple the high-frequency user action (clicking delete) from the high-cost database operation (physical removal).\\n\\n1. **User Action:** Soft Delete. Instant, safe, recoverable.\\n2. **Retention Policy:** \\"We keep deleted data for 30 days.\\"\\n3. **Background Process:** A nightly job runs during off-peak hours (e.g., 3:00 AM) to physically purge records older than 30 days.\\n\\n### **7.2 Why Batching is Superior**\\n\\n* **Amortized I/O:** Loading a page to delete 100 records is 100x more efficient than loading that page 100 separate times to delete 1 record at a time.\\n* **Sequential Access:** Batch jobs can process deletions in Primary Key order, ensuring the disk head moves linearly.\\n* **Reduced Locking:** The batch job can lock a small range, perform the delete, and release the lock, minimizing impact on active users.\\n\\n### **7.3 Implementation: Chunked Deletion**\\n\\nRunning DELETE FROM logs WHERE created\\\\_at \\\\< \'2023-01-01\' is dangerous. It will attempt to lock millions of rows, potentially crashing the database. The correct approach is **Chunking**.\\n\\n#### **Algorithm**\\n\\n1. Identify the target rows.\\n2. Delete in small batches (e.g., 1000 rows).\\n3. Sleep/Throttle between batches to allow replication to catch up and CPU to cool down.\\n\\n#### **MySQL Example (Stored Procedure)**\\n\\nMySQL allows LIMIT in DELETE statements, enabling simple chunking.\\n\\n```sql\\nCREATE PROCEDURE PurgeOldData()  \\nBEGIN  \\nREPEAT  \\n-- Delete 1000 rows, ordered by ID to maintain B-Tree locality  \\nDELETE FROM users   \\nWHERE deleted\\\\_at \\\\< DATE\\\\_SUB(NOW(), INTERVAL 30 DAY)   \\nORDER BY id   \\nLIMIT 1000;\\n\\n    -- Sleep to prevent lock contention  \\n    DO SLEEP(0.5);  \\nUNTIL ROW\\\\_COUNT() \\\\= 0 END REPEAT;  \\nEND  \\n```\\n\\n\\n\\nNote: The ORDER BY id clause is critical. It ensures the deletion walks the B+ Tree leaves sequentially, preventing random I/O thrashing.\\n\\n#### **PostgreSQL Example (CTE)**\\n\\nPostgreSQL requires a Common Table Expression (CTE) to achieve similar chunking with lock skipping.\\n\\n```sql\\nWITH rows_to_delete AS (  \\n  SELECT ctid  \\n  FROM users  \\n  WHERE deleted_at < NOW() - INTERVAL \'30 days\'  \\n  LIMIT 1000  \\n  FOR UPDATE SKIP LOCKED -- Critical: Skip rows currently in use  \\n)  \\nDELETE FROM users u  \\nUSING rows_to_delete d  \\nWHERE u.ctid = d.ctid;\\n```\\n\\nNote: FOR UPDATE SKIP LOCKED allows the maintenance job to run concurrently with user activity without blocking.\\n\\n### **7.4 Partition Pruning**\\n\\nFor massive datasets (e.g., Audit Logs, Event Streams), even batch deletion is too slow. The architectural solution is **Table Partitioning**.\\n\\n* **Strategy:** Partition the table by date (e.g., audit\\\\_2023\\\\_01, audit\\\\_2023\\\\_02).\\n* **Deletion:** When January\'s data expires, you do not run DELETE. You run DROP TABLE audit\\\\_2023\\\\_01.\\n\\n## 8\\\\. Conclusion\\n\\nThe decision to adopt Soft Deletes is not merely a preference for data retention; it is a fundamental alignment with the physics of database storage engines.\\n\\n* **Hard Deletes** operate *against* the grain of the storage engine, forcing expensive rebalancing, causing fragmentation, and introducing dangerous locking contention in OLTP systems.\\n* **Soft Deletes** operate *with* the grain, converting destructive structural changes into efficient state updates. They provide the safety net required for modern applications, enabling undo capabilities, audit trails, and simplified synchronization.\\n\\nBy implementing Soft Deletes by design, combined with a disciplined Batch Hard Delete lifecycle, architects can build systems that remain performant, recoverable, and stable under scale."},{"id":"cache-stampede-thundering-herd","metadata":{"permalink":"/docs/blog/cache-stampede-thundering-herd","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/thundering-herd-problem.md","source":"@site/blog/thundering-herd-problem.md","title":"The Thundering Herd - Understanding and Solving the Cache Stampede","description":"In the world of high-scale distributed systems, caching is our best friend. It\u2019s the primary defense layer that protects our fragile databases from the onslaught of user traffic, ensuring low latency and high throughput.","date":"2025-12-11T00:00:00.000Z","tags":[],"readingTime":7.37,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"The Thundering Herd - Understanding and Solving the Cache Stampede","slug":"cache-stampede-thundering-herd","authors":"ashish","date":"2025-12-11T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","permalink":"/docs/blog/avoid-hard-deletes"},"nextItem":{"title":"Design an Online/Offline Indicator (Presence Service)","permalink":"/docs/blog/offline-online-indicator"}},"content":"In the world of high-scale distributed systems, caching is our best friend. It\u2019s the primary defense layer that protects our fragile databases from the onslaught of user traffic, ensuring low latency and high throughput.\\n\\nBut what happens when that defense layer momentarily fails exactly when you need it most?\\n\\nWelcome to the **Cache Stampede** problem\u2014also known as the \\"Thundering Herd.\\" It\'s a scenario where the very mechanism designed to speed up your system ends up bringing it to its knees.\\n\\nLet\'s dive into what it is, why it happens, and the practical techniques used by major systems (like CDNs) to solve it.\\n\\n## **The Typical Architecture**\\n\\nBefore things go wrong, let\'s look at how things go right.\\n\\nConsider a standard three-tier web architecture. We have API servers fronted by a load balancer. Sitting between these API servers and the \\"source of truth\\" (the Database) is a caching layer (like Redis or Memcached).\\n\\nThe goal of the cache is simple: serve data fast and reduce load on the database.\\n\\nHere is a high-level view of this architecture:\\n\\n```mermaid\\ngraph TD\\n    Client1[Client] --\x3e LB[Load Balancer]\\n    Client2[Client] --\x3e LB\\n    Client3[Client] --\x3e LB\\n    LB --\x3e API1[API Server 1]\\n    LB --\x3e API2[API Server 2]\\n    API1 <--\x3e Cache[(Distributed Cache)]\\n    API2 <--\x3e Cache\\n    API1 --\x3e DB[(Database)]\\n    API2 --\x3e DB\\n\\n    style Cache fill:#f9f,stroke:#333,stroke-width:2px\\n    style DB fill:#ccf,stroke:#333,stroke-width:2px\\n```\\n\\n\\n### **The \\"Happy Path\\" Flow**\\n\\nUnder normal operation, the flow for retrieving data is a standard \\"Read-Through\\" cache pattern:\\n\\n1. The API Server receives a request for a specific key.\\n2. It first checks the cache.\\n3. **Cache Hit:** If the data is there, return it immediately. (Fast\\\\!)\\n4. **Cache Miss:** If the data is *not* there:\\n    * Query the Database (Slow).\\n    * Populate the cache with the result for future requests.\\n    * Return the result to the client.\\n\\nHere is what this typically looks like in simple Java pseudo-code:\\n\\n```java\\npublic class SimpleDataService {\\n\\n    private Cache cache;  \\n    private Database db;\\n\\n    public String getData(String key) {  \\n        // 1. Try fetching from cache  \\n        String cachedValue = cache.get(key);  \\n        if (cachedValue != null) {  \\n            return cachedValue;  \\n        }\\n\\n        // 2. Cache miss - fetch from source of truth  \\n        String dbValue = db.query(key);\\n\\n        // 3. Backfill cache for next time  \\n        // Note: Often done with a Time-To-Live (TTL) to ensure freshness  \\n        cache.put(key, dbValue, Duration.ofMinutes(5)); \\n\\n        return dbValue;  \\n    }  \\n}\\n```\\n\\n\\n\\nThis works perfectly fine for normal traffic loads. The database only sees an occasional read when cache items expire.\\n\\n---\\n\\n## **The Problem: The Stampede Begins**\\n\\nNow, imagine a scenario where a piece of content goes viral.\\n\\nSuddenly, you have thousands of concurrent requests hitting your load balancer for the exact same resource key (e.g., product\\\\_details\\\\_123).\\n\\nIf the cache entry for product\\\\_details\\\\_123 has just expired, or perhaps was evicted due to memory pressure, you have a problem.\\n\\nIn that brief window of time\u2014perhaps just a few hundred milliseconds before the first request can refill the cache\u2014**every single concurrent request will result in a Cache Miss.**\\n\\nIf 5,000 requests arrive simultaneously for that missing key, all 5,000 requests will bypass the cache and bombard your database at the exact same moment.\\n\\nYour database, which was happily serving a few dozen requests per second, suddenly receives thousands. CPU spikes, connection pools become exhausted, queries time out, and the database might even crash. This is the Cache Stampede. It defeats the entire purpose of having a cache.\\n\\n## **The Solution: Request Hedging (Debouncing)**\\n\\nTo handle this, we need to ensure that when a \\"hot\\" cache key is missing, we don\'t let the entire herd stampede to the database. We need to designate a leader.\\n\\nWe leverage a technique known as **Request Hedging** or **Debouncing**.\\n\\nThe concept is straightforward: Out of the thousands of concurrent requests for the missing key, we allow **only one** to proceed to the database. All other requests for that same key must **wait** until that first request completes the job and refills the cache. Once the cache is refilled, the waiting requests can read the data from the cache and proceed.\\n\\nThe idea is simple, but as always, the devil is in the implementation details. How do we make requests wait efficiently and cleanly in a highly concurrent environment? Let\'s look at two approaches in Java.\\n\\n### **Approach 1: The Busy Wait (Spinlock)**\\n\\nIn this technique, if a thread finds a cache miss, it enters a loop where it continuously checks the cache again after sleeping for a tiny duration. It \\"spins\\" until data appears.\\n\\n#### **Pseudo-code (Busy Wait)**\\n\\n```java\\npublic class BusyWaitDataService {\\n\\n    private Cache cache;  \\n    private Database db;\\n\\n    public String getData(String key) {  \\n        String value = cache.get(key);  \\n          \\n        // If cache hit, return immediately  \\n        if (value != null) return value;\\n\\n        // Determine if I am the \\"leader\\" responsible for fetching.  \\n        // Use an atomic operation like \'setIfAbsent\' (NX) in Redis.  \\n        // Set a short TTL on this lock to prevent deadlocks if the service crashes.  \\n        boolean acquiredLock = cache.setIfAbsent(\\"lock::\\" + key, \\"locked\\", Duration.ofSeconds(5));\\n\\n        if (acquiredLock) {  \\n            try {  \\n                // I am the leader. Fetch from DB.  \\n                value = db.query(key);  \\n                cache.put(key, value, Duration.ofMinutes(5));  \\n            } finally {  \\n                // Release lock so others know fetching is done  \\n                cache.delete(\\"lock::\\" + key);  \\n            }  \\n        } else {  \\n            // I am a follower. Spin and wait.  \\n            while (value == null) {  \\n                try {  \\n                    // Sleep briefly to avoid hammering CPU  \\n                    Thread.sleep(50);   \\n                } catch (InterruptedException e) { Thread.currentThread().interrupt(); }  \\n                  \\n                // Check cache again  \\n                value = cache.get(key);  \\n                  \\n                // Optional: Check if lock still exists, if not, break and retry fetch  \\n            }  \\n        }  \\n        return value;  \\n    }  \\n}\\n```\\n\\n\\n#### **The Downside**\\n\\nAs the name suggests, this approach makes the CPU \\"busy.\\" Even though the threads are sleeping, the constant context switching and polling consumes precious CPU cycles. In high-load scenarios, this wasted CPU can become substantial. It works, but it\'s not elegant.\\n\\n### **Approach 2: The Wait/Notify Mechanism (JVM Locks/Futures)**\\n\\nA far more efficient approach is to use native concurrency constructs. Instead of polling, threads should block and go to sleep until they are explicitly notified that the data is ready.\\n\\nIn modern Java, a CompletableFuture combined with a ConcurrentHashMap is a robust way to implement this \\"promise\\" pattern without getting tangled in low-level monitor locks (synchronized/wait/notify).\\n\\n#### **Pseudo-code (Wait/Notify with Futures)**\\n\\nWe maintain a local map of \\"pending database operations.\\" If a request comes in and an operation is already pending for that key, we hook into that existing operation\'s future result.\\n\\n```java\\nimport java.util.concurrent.*;\\n\\npublic class BlogService {\\n   // \\"use threadsafe implementation here\\"\\n   // sem_map: Tracks which keys are currently being fetched\\n   private final ConcurrentHashMap<String, CountDownLatch> semMap = new ConcurrentHashMap<>();\\n\\n   // res_map: Temporary storage for followers to grab the result immediately\\n   // Note: In a real impl, this might need a TTL (e.g., \\"1 min\\" per your note) or explicit cleanup\\n   private final ConcurrentHashMap<String, String> resMap = new ConcurrentHashMap<>();\\n\\n   public String getBlog(String k) {\\n      // 1. Check main cache first\\n      String v = cache.get(k);\\n      if (v != null) {\\n         return v;\\n      }\\n\\n      // 2. Check Semaphore Map (Thread-safe check)\\n      // We attempt to create a \\"lock\\" (Latch) for this key.\\n      CountDownLatch myLatch = new CountDownLatch(1); // Starts \\"blocked\\"\\n\\n      // \\"s = sem_map.get(k)\\" equivalent using atomic putIfAbsent\\n      // If returns value: someone else is already fetching (we are follower)\\n      // If returns null: we successfully inserted (we are leader)\\n      CountDownLatch existingLatch = semMap.putIfAbsent(k, myLatch);\\n\\n      if (existingLatch != null) {\\n         // --- FOLLOWER PATH (\\"if s: s.wait()\\") ---\\n         try {\\n            // Wait for the leader to signal\\n            existingLatch.await();\\n            return resMap.get(k);\\n         } catch (InterruptedException e) {\\n            Thread.currentThread().interrupt();\\n            return null;\\n         }\\n      } else {\\n         // --- LEADER PATH (\\"else\\") ---\\n         try {\\n            v = db.query(k);\\n\\n            // \\"cache.put(k, v)\\"\\n            cache.put(k, v);\\n\\n            // \\"res_map[k] = v\\" (Temporary map)\\n            resMap.put(k, v);\\n\\n            return v;\\n         } finally {\\n            // Open the gate for followers\\n            myLatch.countDown();\\n\\n            //Cleanup lock\\n            semMap.remove(k);\\n\\n            // Optional: You might schedule resMap cleanup here or rely on TTL\\n         }\\n      }\\n   }\\n}\\n```\\n\\n\\n\\n#### **The Benefit**\\n\\nThis approach is highly efficient. Waiting threads are parked by the OS and consume virtually no CPU until the leader thread completes the future. It handles concurrency cleanly within a single JVM.\\n\\n### **The Pragmatic Scope: JVM vs. Distributed Locks**\\n\\nA sharp observer might notice a slight flaw in Approach 2. The guardrails of approach 2 exist within the memory of a *single* API server JVM.\\n\\nIf you have a fleet of 20 API servers behind your load balancer, and a stampede occurs, *one* request on *each* of the 20 servers will proceed to the database.\\n\\nInstead of 5,000 database hits, you will have 20 hits.\\n\\nIs this perfect? No. To reduce it to exactly one global hit, you would need a **Distributed Lock** system (using Redis, Zookeeper, or etcd) to coordinate locking across all 20 servers.\\n\\nHowever, distributed locks introduce significant complexity, latency, and a new point of failure.\\n\\n**In practice, the JVM-level solution is often the pragmatic choice.** Reducing 5,000 simultaneous requests down to 20 is usually sufficient to save the database. It\'s a massive improvement for relatively low implementation complexity.\\n\\nIt\'s worth noting that large-scale Content Delivery Networks (CDNs) like Cloudflare, Akamai, and Fastly use exactly this hedging technique at their edge locations to protect customer origin servers from getting overwhelmed when content goes viral."},{"id":"offline-online-indicator","metadata":{"permalink":"/docs/blog/offline-online-indicator","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/presence-design.md","source":"@site/blog/presence-design.md","title":"Design an Online/Offline Indicator (Presence Service)","description":"Designing a scalable presence system for 1 billion users.","date":"2025-12-10T10:55:15.000Z","tags":[],"readingTime":6.08,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"slug":"offline-online-indicator","title":"Design an Online/Offline Indicator (Presence Service)","sidebar_label":"Presence Service","description":"Designing a scalable presence system for 1 billion users.","authors":"ashish"},"unlisted":false,"prevItem":{"title":"The Thundering Herd - Understanding and Solving the Cache Stampede","permalink":"/docs/blog/cache-stampede-thundering-herd"}},"content":"Designing a system to indicate if a user is **Online** or **Offline** (and their last seen timestamp) sounds simple on the surface, but becomes a massive engineering challenge when scaling to **1 Billion users**.\\n\\nIn this post, we will breakdown the design of a Presence Service suitable for a massive social network or chat application like WhatsApp or Facebook Messenger.\\n\\n## Problem Statement\\n\\nWe need to build a service that indicates the availability status of a user\'s connections.\\n\\n**The Functional Requirements:**\\n1.  Indicate if a friend/connection is currently **Online**.\\n2.  If offline, display the **Last Seen** timestamp.\\n\\n**The Scale:**\\n* **Total Users:** ~1 Billion.\\n* **Concurrent Online Users:** ~500 Million.\\n* **Latency:** The status needs to be near real-time.\\n\\n## High-Level Strategy: Push vs. Pull\\n\\nHow do we keep the server updated about the client\'s status?\\n\\n### Option 1: Pull (Polling)\\nThe server periodically connects to the client to ask, \\"Are you there?\\"\\n* **Verdict:** \u274c **Impossible.**\\n* **Reasoning:** In a mobile/NAT environment, servers cannot initiate connections to clients easily. Furthermore, polling 1B users is computationally wasteful.\\n\\n### Option 2: Push (Heartbeat)\\nThe client sends a signal to the server periodically saying, \\"I am alive.\\"\\n* **Verdict:** \u2705 **Selected.**\\n* **Reasoning:** This is the standard pattern for presence. If the server stops receiving heartbeats (after a timeout threshold), the user is marked offline.\\n\\n---\\n\\n## The Protocol: HTTP vs. WebSockets\\n\\nWe established a \\"Push\\" model, but how should the client push this data?\\n\\n### 1. REST/HTTP Based\\nThe client sends a `POST /health` request every *N* seconds.\\n\\n**Why this fails at scale:**\\n* **Overhead:** HTTP is stateless. Every heartbeat requires a full 3-way TCP handshake (if not using keep-alive efficiently), SSL handshake overhead, and heavy HTTP headers.\\n* **Traffic:** With 500M concurrent users sending a heartbeat every 10 seconds, that is **50 Million requests per second**. Most of the data transferred would be HTTP headers, not the actual status payload.\\n\\n### 2. Persistent WebSockets\\nThe client opens a long-lived, bi-directional connection with the server.\\n\\n**Why this is the winner:**\\n* **Reduced Overhead:** Once the connection is established, data frames have minimal overhead (just a few bytes). There are no repeated headers or handshakes.\\n* **Real-time:** The server knows *immediately* if a connection is severed (TCP FIN or RST).\\n* **Bi-directional:** It allows the server to push status updates of *friends* back to the user over the same channel.\\n\\n:::caution The \\"Disconnect\\" Fallacy\\nA common misconception is that WebSockets natively handle all disconnects via an `onDisconnect` event.\\n* **Clean Disconnect:** If a user clicks \\"Logout\\", the client sends a TCP FIN. The server knows immediately.\\n* **Dirty Disconnect:** If the user loses internet connectivity or the connection is broken for any reason, **the server receives nothing.**\\n* **The Fix:** We must implement an **Application-Level Heartbeat**. If the server doesn\'t receive a \\"Ping\\" frame or message within $N$ seconds, it forcibly closes the socket and marks the user offline.\\n  :::\\n\\n:::info Scaling WebSockets\\nScaling persistent connections is harder than scaling stateless HTTP.\\n1.  **OS Limits:** You must tune the kernel to allow >65k open file descriptors (ephemeral ports) per server.\\n2.  **Load Balancing:** You need a Layer 7 Load Balancer that supports \\"Sticky Sessions\\" effectively, though for a pure presence service, state can be externalized.\\n3.  **Memory:** Holding 500M open connections requires massive RAM across your fleet of connection handlers (Gateway Service).\\n    :::\\n\\n---\\n\\n## Database Design & Estimation\\n\\nThe compute layer is pretty. All it has to do is upon receiving the request, perform a key value based insert or lookup and simply return back to the client. The complexity lies in the **storage layer**. \\n\\n### Query Patterns\\n1.  **Write (Heavy):** Update User *A*\'s timestamp (Heartbeat).\\n2.  **Read (Heavy):** Get status for User *A*\'s friends/connections when User *A* opens the app.\\n\\n### Data Schema\\nWe need a simple Key-Value pair.\\n\\n| Field  | Type    | Size |\\n|:-------|:--------|-----|\\n| UserID | Integer |  4 Bytes |\\n| LastSeen | Epoch (Int) | 4 Bytes |\\n| **Total** | | **8 Bytes** |\\n\\n### Capacity Planning\\nWith 1 Billion users, do we need massive storage?\\n\\n1,000,000,000 users * 8 bytes = 8 GB\\n\\n:::tip Insight\\nWe only need **~8 GB** of storage to hold the state of every user on the planet. This entire dataset can fit into the RAM of a single modern server instance.\\n:::\\n\\n---\\n\\n## Managing \\"Online\\" State Lifecycle\\n\\nHow do we decide when to switch a user from Online to Offline? We have three strategies.\\n\\n### 1. The Cron Job Reaper\\nA background process scans the database every few minutes and deletes entries older than $N$ minutes.\\n* **Pros:** Keeps DB clean eventually.\\n* **Cons:** **Terrible at scale.** Scanning a table of 500M rows every minute creates massive read pressure and locking issues. The \\"Offline\\" status will always be laggy.\\n\\n### 2. Connection Events (Explicit Disconnect)\\nLeverage WebSocket callbacks (`onConnect`, `onDisconnect`) to update the DB.\\n* **Pros:** extremely efficient. Writes only happen on state changes.\\n* **Cons:** Unreliable. If a user loses network (enters a tunnel) or the app crashes, the `onDisconnect` event might never fire sent to the server. The user will appear \\"Online\\" forever (a Zombie session).\\n\\n### 3. Database TTL (Time-To-Live)\\nUse the database\'s native feature to auto-expire keys. The Heartbeat simply resets the TTL.\\n* **Pros:** Handles \\"unclean\\" disconnects gracefully. If the heartbeat stops, the key vanishes automatically. No manual cleanup required.\\n* **Cons:** Moderate write load (every heartbeat is a write to reset the TTL).\\n\\n**Verdict:** We will use **Option 3 (TTL)** as the primary mechanism, potentially optimized by Option 2 (explicitly deleting the key on a clean logout to avoid the TTL wait).\\n\\n## Database Selection: Redis vs. DynamoDB\\n\\nWe need a Key-Value store that handles massive write throughput.\\n\\n**The Math:**\\n* 500 Million concurrent users.\\n* Heartbeat interval: 30 seconds.\\n* Throughput = $500,000,000 / 30 \\\\approx$ **16.6 Million Writes/Second**.\\n\\n### Candidate 1: Amazon DynamoDB\\n* **Pros:** Serverless, high durability, multi-region replication (Global Tables).\\n* **Cons:** **Cost and Hot Partitions.**\\n    * Cost: DynamoDB charges by **Write Capacity Units (WCUs)**.\\n        * 16.6 Million writes/sec = **16.6 Million WCUs**.\\n        * Cost per WCU (Provisioned) $\\\\approx \\\\$0.00065$ / hour.\\n        * **Hourly Cost:** $\\\\$10,833$.\\n        * **Monthly Cost:** **~$7.9 Million / Month**.\\n    * Hot Partition: In DynamoDB, a single partition is strictly limited to **1,000 WCUs**. If 2,000 users map to the same partition key, requests get throttled.\\n\\n### Candidate 2: Redis (The Winner)\\nRedis is an in-memory store. We are limited by CPU/Network throughput per node.\\n* **Pros:**\\n    * **In-Memory Speed:** Sub-millisecond reads/writes.\\n    * **Native TTL:** Redis handles key expiration natively and efficiently.\\n    * **Cost Effective:**\\n        * Redis is an in-memory store. We are limited by CPU/Network throughput per node.\\n        * A single robust Redis node (e.g., AWS `r7g.xlarge`) can handle **~600,000 writes/sec**. (**Benchmark**: https://aws.plainenglish.io/aws-elasticache-a-performance-and-cost-analysis-of-redis-7-1-vs-valkey-7-2-bfac4fb5c22a)\\n        * Nodes required: $16,600,000 / 600,000 \\\\approx$ **28 Shards**.\\n        * Cost per node $\\\\approx \\\\$0.30$ / hour.\\n        * **Monthly Cost:** $28 * $0.30 * 730 hours = **~$6132 / Month**.\\n* **Cons:**\\n    * **Persistence:** If Redis crashes, we lose \\"Last Seen\\" data (unless AOF mode is enabled, which slows performance).\\n* **Mitigation:** For a Presence system, *ephemeral* data loss is acceptable. If Redis crashes, users briefly appear offline until their next heartbeat (seconds later).\\n\\n:::tip Cost Epiphany\\nBy choosing Redis over DynamoDB for this high-throughput/ephemeral workload, we save the company roughly **$7.89 Million per month**.\\n:::\\n\\n## Final Architecture\\n\\n```mermaid\\nflowchart TD\\n    Client[User Client] --\x3e|WebSocket Connection| LB[Load Balancer]\\n    LB --\x3e|Sticky Session| WS[WebSocket Server]\\n    \\n    subgraph Data Layer\\n    WS --\x3e|HEARTBEAT every 10s| Redis[(Redis Cluster)]\\n    end\\n    \\n    note[Redis Key: UserID <br/> Value: Timestamp <br/> TTL: 30s] -.-> Redis\\n```"}]}}')}}]);