"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[4959],{2333:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});var s=r(4396),t=r(4848),i=r(8453);const a={title:"Designing L7 Load Balancer",slug:"load-balancer-design",tags:["system-design","load-balancer","distributed-systems","aws","zookeeper"],authors:"ashish",date:new Date("2025-12-30T00:00:00.000Z")},o=void 0,l={authorsImageUrls:[void 0]},c=[{value:"1) Define the Requirements",id:"1-define-the-requirements",level:2},{value:"2) Need of Persistence for the LB Configurations",id:"2-need-of-persistence-for-the-lb-configurations",level:2},{value:"2.a) Need of cache for the LB configurations",id:"2a-need-of-cache-for-the-lb-configurations",level:3},{value:"2.b) How to keep the cache in sync with the Configurations DB",id:"2b-how-to-keep-the-cache-in-sync-with-the-configurations-db",level:3},{value:"Option 1: The PULL Approach (Polling)",id:"option-1-the-pull-approach-polling",level:4},{value:"Option 2: The PUSH Based Approach",id:"option-2-the-push-based-approach",level:4},{value:"2.c) Choice of Configuration DB",id:"2c-choice-of-configuration-db",level:3},{value:"Final Architecture for Persistence Layer",id:"final-architecture-for-persistence-layer",level:3},{value:"3) Need of Orchestrator layer to monitor the health of backend servers",id:"3-need-of-orchestrator-layer-to-monitor-the-health-of-backend-servers",level:2},{value:"3.a) Making the Orchestrator layer Highly Available",id:"3a-making-the-orchestrator-layer-highly-available",level:3},{value:"4) Understanding Scalability of the Load Balancer Itself",id:"4-understanding-scalability-of-the-load-balancer-itself",level:2},{value:"Routing Requests amongst different LB servers via CoreDNS",id:"routing-requests-amongst-different-lb-servers-via-coredns",level:3},{value:"1. Round Robin &amp; Weighted Load Balancing",id:"1-round-robin--weighted-load-balancing",level:4},{value:"2. Geolocation &amp; Latency Routing",id:"2-geolocation--latency-routing",level:4},{value:"5) Making the Discovery Layer Highly Available",id:"5-making-the-discovery-layer-highly-available",level:2},{value:"The Concept: Virtual IP (VIP)",id:"the-concept-virtual-ip-vip",level:3},{value:"The Solution: Keepalived",id:"the-solution-keepalived",level:3},{value:"How VRRP Works",id:"how-vrrp-works",level:4},{value:"The Configuration: <code>keepalived.conf</code>",id:"the-configuration-keepalivedconf",level:4},{value:"Final Architecture: The Complete Picture",id:"final-architecture-the-complete-picture",level:2}];function h(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsxs)(n.p,{children:["In this series, we will evolve the design of a Layer 7 Load Balancer (LB) from scratch. We won't just look at ",(0,t.jsx)(n.em,{children:"how"})," to build it, but ",(0,t.jsx)(n.em,{children:"why"})," we make specific architectural choices at every fork in the road."]}),"\n",(0,t.jsx)(n.h2,{id:"1-define-the-requirements",children:"1) Define the Requirements"}),"\n",(0,t.jsx)(n.p,{children:'Before writing code, we must define the "must-haves" of our system.'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Load the Balance:"})," The core responsibility is distributing incoming network traffic across multiple backend servers to ensure no single server becomes a bottleneck. This maximizes throughput and minimizes response time."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tunable Algorithm:"})," We cannot rely on a single distribution strategy. Different workloads require different approaches (e.g., Round Robin for uniform services, Weighted Round Robin for heterogenous capacity, or Least Connections for long-lived sessions)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scaling beyond single Load Balancer:"})," The LB itself shouldn't become the single point of failure (SPOF) or the bottleneck. We must design for horizontal scalability where we can add more LB nodes dynamically as traffic increases."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"2-need-of-persistence-for-the-lb-configurations",children:"2) Need of Persistence for the LB Configurations"}),"\n",(0,t.jsx)(n.p,{children:"Why do we need a database for a load balancer? Can't we just hardcode the backend IPs in a config file?"}),"\n",(0,t.jsx)(n.p,{children:"In a modern distributed system, the environment is dynamic. Backend servers auto-scale (scale-out/in), health checks fail, and routing rules change. If we hardcode configurations, every change requires a redeployment of the Load Balancer fleet. This is unacceptable."}),"\n",(0,t.jsxs)(n.p,{children:["We need to decouple the ",(0,t.jsx)(n.strong,{children:"Control Plane"})," (Configuration Management) from the ",(0,t.jsx)(n.strong,{children:"Data Plane"})," (Traffic Forwarding). Administrators should be able to tune parameters (add backends, change algorithms, update health check paths) via a console, and these changes must persist in a reliable store that the LB instances can read from."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Architecture V1:"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph LR\n    Client[Client] --\x3e LB[Load Balancer]\n    LB --\x3e Server1[Backend Server 1]\n    LB --\x3e Server2[Backend Server 2]\n    LB --\x3e Server3[Backend Server 3]\n    \n    LB -.->| | DB[(Configuration DB)]\n    \n    style LB fill:#f9f,stroke:#333,stroke-width:2px\n    style DB fill:#ccf,stroke:#333,stroke-width:2px"}),"\n",(0,t.jsx)(n.h3,{id:"2a-need-of-cache-for-the-lb-configurations",children:"2.a) Need of cache for the LB configurations"}),"\n",(0,t.jsxs)(n.p,{children:["While the architecture above works functionally, it fails non-functionally. If the Load Balancer hits the Configuration DB for ",(0,t.jsx)(n.strong,{children:"every single request"}),", we introduce massive latency."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Load Balancer Overheads:"})," The time spent by the LB processing the request before forwarding it."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"The Latency Cost:"})," If a network round-trip to the DB takes 5ms, we are adding 5ms to ",(0,t.jsx)(n.em,{children:"every"}),' user request. In high-throughput systems (100k+ RPS), this also creates a "Thundering Herd" problem that will crush the database.']}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Constraint:"})," LB overhead must be extremely minimal (sub-millisecond)."]}),"\n",(0,t.jsxs)(n.p,{children:["To solve this, we introduce an ",(0,t.jsx)(n.strong,{children:"In-Memory Cache"})," within the Load Balancer. The LB reads from its local RAM, which is nanosecond-latency fast."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The Data Model (Key-Value):"}),"\nWe need a simple Key-Value store structure where the key is the LB Group Name and the value is the configuration blob."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'"lb-group-1" : {\n    "backend_servers" : ["10.0.0.1:80", "10.0.0.2:80"], \n    "health_endpoint" : "/health", \n    "health_endpoint_expected_code": 200,\n    "health_check_frequency_seconds" : 10,\n    "algorithm" : "WEIGHTED_ROUND_ROBIN"\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["However, caching introduces a classic distributed system problem: ",(0,t.jsx)(n.strong,{children:"Cache Staleness"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"2b-how-to-keep-the-cache-in-sync-with-the-configurations-db",children:"2.b) How to keep the cache in sync with the Configurations DB"}),"\n",(0,t.jsx)(n.p,{children:"If an admin adds a new backend server to the DB, the LB's in-memory cache still holds the old list. We need a sync mechanism."}),"\n",(0,t.jsx)(n.h4,{id:"option-1-the-pull-approach-polling",children:"Option 1: The PULL Approach (Polling)"}),"\n",(0,t.jsx)(n.p,{children:"We run a background cron job on the LB every 10 seconds to fetch the latest config from the DB."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"The Problem:"}),' There is a "Staleness Window" of up to 10 seconds. If a backend server is removed from the DB because it was corrupted, the LB might still send traffic to it for 10 seconds, causing 5xx errors for users.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verdict:"}),' Acceptable for low-criticality systems, but "bad product experience" for high-availability requirements.']}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"option-2-the-push-based-approach",children:"Option 2: The PUSH Based Approach"}),"\n",(0,t.jsx)(n.p,{children:'In this model, whenever the Configuration DB is updated, it actively "pushes" a notification to all Load Balancers to invalidate or update their cache immediately.'}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"The Advantage:"})," This minimizes the staleness window to mere milliseconds. The system reacts in near real-time."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Verdict:"})," Preferred for production-grade Load Balancers."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2c-choice-of-configuration-db",children:"2.c) Choice of Configuration DB"}),"\n",(0,t.jsxs)(n.p,{children:["We need a technology that offers ",(0,t.jsx)(n.strong,{children:"Persistence"})," (durability) + ",(0,t.jsx)(n.strong,{children:"Key-Value Lookup"})," + ",(0,t.jsx)(n.strong,{children:"Push/Watch Capabilities"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Let's evaluate the options:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Kafka:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Analysis:"})," Kafka is an event streaming platform, not a configuration store. It is fundamentally PULL-based (consumers poll for messages). Using it as a source of truth for current state is an anti-pattern (requires log compaction gymnastics)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Verdict:"})," ",(0,t.jsx)(n.strong,{children:"Reject."})]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"DynamoDB + DynamoDB Streams:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Analysis:"}),' Excellent for persistence and HA. However, DynamoDB Streams require a Lambda or consumer to poll the stream and then fan-out updates. It doesn\'t provide a native "direct-to-client" push channel.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Verdict:"})," ",(0,t.jsx)(n.strong,{children:"Reject"})," (Too much glue code required)."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Redis Pub/Sub:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Analysis:"}),' Redis is a fantastic KV store. Pub/Sub allows the "Control Plane" to publish an update to a channel that all LBs subscribe to.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"The Risk:"}),' Redis Pub/Sub operates on "At-Most-Once" delivery. If an LB node restarts or has a network blip during the publish event, it ',(0,t.jsx)(n.strong,{children:"misses the message forever"}),". It has no persistence for the channel messages."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"High Availability:"}),' Redis Sentinel can provide HA, but it is historically tricky to configure correctly to avoid split-brain scenarios. Relying on it for the "source of truth" configuration can be risky without a backup polling mechanism.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Verdict:"})," ",(0,t.jsx)(n.strong,{children:"Good"}),', but requires a "safety net" (occasional polling).']}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"ZooKeeper (The Winner):"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Analysis:"})," ZooKeeper is explicitly designed for distributed coordination and configuration management."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"ZNodes:"})," It stores data in file-system-like paths (Key-Value style)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Watches (The Push Mechanism):"}),' Clients can place a "Watch" on a ZNode. When the data changes, ZooKeeper sends a notification to the client. This is a robust Push mechanism.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Consistency:"})," It uses the ZAB (ZooKeeper Atomic Broadcast) protocol, guaranteeing strong consistency."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"High Availability:"})," Running a ZK Ensemble (3 or 5 nodes) tolerates node failures without data loss."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.em,{children:"Verdict:"})," ",(0,t.jsx)(n.strong,{children:"Excellent Fit."})," It solves Persistence + Push + HA natively."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"final-architecture-for-persistence-layer",children:"Final Architecture for Persistence Layer"}),"\n",(0,t.jsxs)(n.p,{children:["We will use ",(0,t.jsx)(n.strong,{children:"ZooKeeper"})," as our configuration source of truth."]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control Plane"})," writes config to ZooKeeper ZNode (",(0,t.jsx)(n.code,{children:"/lb-configs/lb-1"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Load Balancers"}),' keep a "Watch" on that ZNode.']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Event:"})," When config changes, ZK notifies all LBs."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action:"})," LBs fetch the new data, update their local In-Memory Cache, and re-set the Watch."]}),"\n"]}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TD\n    subgraph Control_Plane [Control Plane]\n        Admin[Admin Console] --\x3e|Writes Config| ZK[(ZooKeeper Cluster)]\n    end\n\n    subgraph Data_Plane [Data Plane / Load Balancer Fleet]\n        LB1[LB Instance 1]\n    end\n\n    ZK -- Push / Watch Notification --\x3e LB1\n\n    LB1 -- Reads Data --\x3e ZK\n\n    LB1 --\x3e|Traffic| Backend[Backend Servers]"}),"\n",(0,t.jsx)(n.h2,{id:"3-need-of-orchestrator-layer-to-monitor-the-health-of-backend-servers",children:"3) Need of Orchestrator layer to monitor the health of backend servers"}),"\n",(0,t.jsx)(n.p,{children:'A Load Balancer is only as good as the servers it points to. If a backend server crashes, gets overloaded, or becomes unresponsive, the Load Balancer must stop sending traffic to it immediately. Without this "circuit breaking" capability, a single failing server can cause a cascade of errors for the end users (502 Bad Gateway), turning a partial outage into a total system failure. The LB needs an intelligent subsystem to actively verify that its destinations are actually capable of handling work.'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Enter the Orchestrator."})}),"\n",(0,t.jsx)(n.p,{children:'The Orchestrator acts as the "Health Monitor" of our system. It operates in a continuous loop:'}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fetch Configuration:"})," It continually watches the Configuration DB (ZooKeeper) to know which backend servers are supposed to be active and what their health check parameters are (e.g., endpoint ",(0,t.jsx)(n.code,{children:"/health"}),", interval ",(0,t.jsx)(n.code,{children:"10s"}),")."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Probe:"})," It actively sends HTTP requests (or TCP pings) to these endpoints on the backend servers."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Decide & Update:"})," If a server fails to respond with a ",(0,t.jsx)(n.code,{children:"200 OK"}),' for a configured threshold (e.g., 3 consecutive failures), the Orchestrator marks it as "Unhealthy." It then updates the Configuration DB (ZooKeeper), effectively removing that server from the active rotation. Since our LBs are watching ZooKeeper, they immediately stop routing traffic to the dead node.']}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3a-making-the-orchestrator-layer-highly-available",children:"3.a) Making the Orchestrator layer Highly Available"}),"\n",(0,t.jsx)(n.p,{children:"We cannot rely on a single Orchestrator node. If that single node dies, health checks stop. If health checks stop, the Load Balancers are flying blind\u2014they might route traffic to dead servers (false negatives) or fail to route to recovered servers (false positives)."}),"\n",(0,t.jsxs)(n.p,{children:["To solve this, we implement a ",(0,t.jsx)(n.strong,{children:"Master-Worker Architecture with Leader Election"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Master Node:"})," Responsible for coordination. It divides the list of backend servers into chunks and assigns them to different Worker nodes to balance the health-checking load."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Worker Nodes:"})," These are the workhorses. They perform the actual HTTP pings to the backend servers and report status back to the Master or directly to the DB."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Self-Healing & Leader Election:"})," We leverage ",(0,t.jsx)(n.strong,{children:"ZooKeeper's Ephemeral Nodes"})," for this. All Orchestrator nodes attempt to create a lock file (ZNode) in ZooKeeper.","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The node that succeeds becomes the ",(0,t.jsx)(n.strong,{children:"Master"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["The others become ",(0,t.jsx)(n.strong,{children:"Workers"})," (standby)."]}),"\n",(0,t.jsx)(n.li,{children:"If the Master process dies, its ephemeral node in ZooKeeper disappears. The Worker nodes detect this event immediately and trigger a new election. One of them promotes itself to Master, ensuring zero downtime for the monitoring subsystem."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"System State Diagram (So Far):"})}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TD\n    subgraph Data_Plane [Data Plane]\n        Client --\x3e LB[Load Balancer]\n        LB --\x3e BE1[Backend 1]\n        LB --\x3e BE2[Backend 2]\n        LB --\x3e BE3[Backend 3]\n    end\n\n    subgraph Control_Plane [Control Plane]\n        ZK[(ZooKeeper Config & State)]\n        \n        subgraph Orchestrator_Cluster\n            OrchM[Orchestrator Master]\n            OrchW1[Orchestrator Worker 1]\n            OrchW2[Orchestrator Worker 2]\n        end\n        \n        OrchM -- Assigns Tasks --\x3e OrchW1\n        OrchM -- Assigns Tasks --\x3e OrchW2\n        OrchM -.->|Leader Election| ZK\n        \n        OrchW1 -- Health Checks --\x3e BE1\n        OrchW1 -- Health Checks --\x3e BE2\n        OrchW2 -- Health Checks --\x3e BE3\n        \n        OrchW1 -- Updates Health Status --\x3e ZK\n        OrchW2 -- Updates Health Status --\x3e ZK\n    end\n\n    ZK -- Push Updates --\x3e LB\n    \n    style OrchM fill:#ff9,stroke:#333\n    style ZK fill:#ccf,stroke:#333"}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"4-understanding-scalability-of-the-load-balancer-itself",children:"4) Understanding Scalability of the Load Balancer Itself"}),"\n",(0,t.jsxs)(n.p,{children:["We have scaled the backends, but what happens when the Load Balancer ",(0,t.jsx)(n.em,{children:"itself"})," is overwhelmed?"]}),"\n",(0,t.jsx)(n.p,{children:"A single LB instance has limits\u2014CPU, Memory, and Network Bandwidth (throughput). To detect this, we inject metrics (active connections, request latency, CPU usage) into a Time-Series Database (like Prometheus or InfluxDB). When thresholds are breached, our Auto-Scaling Group triggers the provision of a new Load Balancer instance."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The Discovery Problem:"}),"\nIf we add a new Load Balancer (IP: ",(0,t.jsx)(n.code,{children:"10.0.0.50"}),") to help the existing one (IP: ",(0,t.jsx)(n.code,{children:"10.0.0.10"}),"), how do clients know? Clients typically have the domain ",(0,t.jsx)(n.code,{children:"api.myblog.com"})," cached to point to ",(0,t.jsx)(n.code,{children:"10.0.0.10"}),". We cannot put another Load Balancer in front of our Load Balancers, or we just move the bottleneck one layer up (turtles all the way down)."]}),"\n",(0,t.jsx)(n.h3,{id:"routing-requests-amongst-different-lb-servers-via-coredns",children:"Routing Requests amongst different LB servers via CoreDNS"}),"\n",(0,t.jsxs)(n.p,{children:["We solve this using ",(0,t.jsx)(n.strong,{children:"DNS Load Balancing"})," with ",(0,t.jsx)(n.strong,{children:"CoreDNS"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"CoreDNS is a flexible, extensible DNS server that can serve as the entry point for our traffic. Instead of a static mapping, CoreDNS can dynamically return different IP addresses for the same domain name."}),"\n",(0,t.jsx)(n.h4,{id:"1-round-robin--weighted-load-balancing",children:"1. Round Robin & Weighted Load Balancing"}),"\n",(0,t.jsx)(n.p,{children:"CoreDNS can be configured to return multiple A records (IP addresses) for a single domain. Modern clients (browsers, mobile apps) will try these IPs. We can update these records dynamically as we scale our LB fleet."}),"\n",(0,t.jsx)(n.h4,{id:"2-geolocation--latency-routing",children:"2. Geolocation & Latency Routing"}),"\n",(0,t.jsxs)(n.p,{children:["CoreDNS can utilize plugins (like the ",(0,t.jsx)(n.code,{children:"geoip"})," plugin) to inspect the source IP of the incoming DNS query."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"User from India"})," -> CoreDNS resolves ",(0,t.jsx)(n.code,{children:"blog.com"})," to the VIP (Virtual IP) of the LB cluster in Mumbai."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"User from US"})," -> CoreDNS resolves ",(0,t.jsx)(n.code,{children:"blog.com"})," to the VIP of the LB cluster in Virginia."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Technical Implementation:"}),"\nCoreDNS often uses an ",(0,t.jsx)(n.code,{children:"etcd"})," backend to store records, allowing for real-time updates without restarting the DNS server. Here is how a ",(0,t.jsx)(n.strong,{children:"JSON"})," record for an LB service might look inside the store that CoreDNS reads:"]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"The CoreDNS Configuration (Corefile):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:". {\n    etcd myblog.com {\n        path /skydns\n        endpoint http://etcd-cluster:2379\n    }\n    loadbalance round_robin\n    cache 30\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"The Data (stored in etcd as JSON):"}),'\nThis is the "Service Discovery" record. The key path represents the domain, and the value is the target LB.']}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'/* Key: /skydns/com/myblog/api/lb1 */\n{\n    "host": "10.0.0.10",\n    "port": 80,\n    "priority": 10,\n    "weight": 100,\n    "text": "Location: US-East"\n}\n\n/* Key: /skydns/com/myblog/api/lb2 */\n{\n    "host": "10.0.0.50",\n    "port": 80,\n    "priority": 10,\n    "weight": 100,\n    "text": "Location: US-East"\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.em,{children:["Note: By adjusting the ",(0,t.jsx)(n.code,{children:"weight"})," or removing the entry, we control traffic flow to the LBs instantly."]})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"5-making-the-discovery-layer-highly-available",children:"5) Making the Discovery Layer Highly Available"}),"\n",(0,t.jsxs)(n.p,{children:["We have introduced CoreDNS to handle the discovery of our Load Balancers. But we have introduced a new problem: ",(0,t.jsx)(n.strong,{children:"What if the CoreDNS server itself goes down?"})]}),"\n",(0,t.jsx)(n.p,{children:"If our DNS layer fails, new clients cannot resolve the domain name to an IP address. Even if our Load Balancers and Backend Servers are healthy, the door to our system is effectively locked."}),"\n",(0,t.jsx)(n.p,{children:'To solve this, we cannot simply spin up two CoreDNS servers with different IP addresses and hope for the best. Clients typically configure a specific IP (or a limited list) for their DNS resolver. We need a way to have multiple DNS servers "share" a single, highly available IP address.'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Enter Keepalived and the Virtual IP (VIP)."})}),"\n",(0,t.jsx)(n.h3,{id:"the-concept-virtual-ip-vip",children:"The Concept: Virtual IP (VIP)"}),"\n",(0,t.jsx)(n.p,{children:"In standard networking, two machines cannot share the same IP address on the same network segment. If they do, it causes an IP conflict, and switches/routers get confused about where to send packets."}),"\n",(0,t.jsxs)(n.p,{children:["A ",(0,t.jsx)(n.strong,{children:"Virtual IP (VIP)"}),' is a "floating" IP address that is not permanently bound to a specific physical machine\'s network interface. Instead, it is a resource that can be dynamically claimed by a machine.']}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["We assign a VIP (e.g., ",(0,t.jsx)(n.code,{children:"192.168.1.100"}),") to our DNS cluster."]}),"\n",(0,t.jsxs)(n.li,{children:["Clients are configured to send DNS queries to ",(0,t.jsx)(n.code,{children:"192.168.1.100"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Only the ",(0,t.jsx)(n.strong,{children:"Master"}),' node currently "holds" this IP.']}),"\n",(0,t.jsxs)(n.li,{children:["If the Master dies, a ",(0,t.jsx)(n.strong,{children:"Backup"}),' node detects the failure and immediately "claims" the IP, handling traffic seamlessly.']}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"the-solution-keepalived",children:"The Solution: Keepalived"}),"\n",(0,t.jsxs)(n.p,{children:["We use ",(0,t.jsx)(n.strong,{children:"Keepalived"}),", a routing software that implements the ",(0,t.jsx)(n.strong,{children:"VRRP (Virtual Router Redundancy Protocol)"}),"."]}),"\n",(0,t.jsx)(n.h4,{id:"how-vrrp-works",children:"How VRRP Works"}),"\n",(0,t.jsx)(n.p,{children:"VRRP is designed to eliminate single points of failure."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The nodes in the cluster communicate using ",(0,t.jsx)(n.strong,{children:"Multicast"})," packets."]}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.strong,{children:"Master"}),' node periodically sends "advertisements" (heartbeats) to the multicast group.']}),"\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.strong,{children:"Backup"})," nodes listen for these advertisements."]}),"\n",(0,t.jsx)(n.li,{children:"If the Backup nodes stop receiving advertisements (meaning the Master is dead or network partitioned), the Backup with the highest priority promotes itself to Master and claims the VIP."}),"\n"]}),"\n",(0,t.jsxs)(n.h4,{id:"the-configuration-keepalivedconf",children:["The Configuration: ",(0,t.jsx)(n.code,{children:"keepalived.conf"})]}),"\n",(0,t.jsx)(n.p,{children:"Let's look at the implementation. We install Keepalived on both CoreDNS servers."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsxs)(n.strong,{children:["Master Node Configuration (",(0,t.jsx)(n.code,{children:"/etc/keepalived/keepalived.conf"}),"):"]})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"vrrp_instance VI_1 {\n    state MASTER           # 1. Initial State\n    interface eth0         # 2. Network Interface to bind to\n    virtual_router_id 51   # 3. Unique ID for this cluster\n    priority 100           # 4. Election Priority (Higher wins)\n    advert_int 1           # 5. Advertisement Interval (1 second)\n    \n    authentication {\n        auth_type PASS\n        auth_pass my_secret_password\n    }\n\n    virtual_ipaddress {\n        192.168.1.100/24   # 6. The Virtual IP (VIP)\n    }\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Backup Node Configuration:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"vrrp_instance VI_1 {\n    state BACKUP           # Starts as backup\n    interface eth0\n    virtual_router_id 51   # Must match the Master\n    priority 90            # Lower priority than Master\n    advert_int 1\n    \n    authentication {\n        auth_type PASS\n        auth_pass my_secret_password\n    }\n\n    virtual_ipaddress {\n        192.168.1.100/24   # Must match the Master\n    }\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:"We have now eliminated the final Single Point of Failure in our control path."}),"\n",(0,t.jsx)(n.h2,{id:"final-architecture-the-complete-picture",children:"Final Architecture: The Complete Picture"}),"\n",(0,t.jsx)(n.p,{children:"We have now evolved a robust, scalable, and self-healing L7 Load Balancer system."}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Clients"})," query ",(0,t.jsx)(n.strong,{children:"CoreDNS"})," to find an available Load Balancer."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CoreDNS"})," returns the IP of a healthy LB (based on Geo/Load)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"L7 Load Balancers"})," receive traffic. They read their routing rules and backend lists from ",(0,t.jsx)(n.strong,{children:"ZooKeeper"})," (cached in memory)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Backend Servers"})," process the requests."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Orchestrator Cluster"})," (Master/Worker) continuously probes Backends and updates ",(0,t.jsx)(n.strong,{children:"ZooKeeper"})," if health status changes."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ZooKeeper"})," pushes config updates to LBs in real-time."]}),"\n"]}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TD\n    User[User / Client]\n    \n    subgraph Service_Discovery [Discovery Layer]\n        DNS[CoreDNS Server]\n        Etcd[(etcd / DNS Store)]\n        DNS -.->|Reads| Etcd\n    end\n\n    subgraph LB_Layer [Load Balancer Layer]\n        LB1[L7 LB Instance 1]\n        LB2[L7 LB Instance 2]\n    end\n\n    subgraph Coordination [Control Plane]\n        ZK[(ZooKeeper Cluster)]\n        \n        subgraph Orchestrator_System\n            OrchM[Orchestrator Master]\n            OrchW[Orchestrator Workers]\n            OrchM --- OrchW\n        end\n    end\n\n    subgraph Backend_Layer [Application Layer]\n        Server1[App Server 1]\n        Server2[App Server 2]\n        Server3[App Server 3]\n        Server4[App Server 4]\n    end\n\n    %% Flows\n    User -- 1. DNS Query --\x3e DNS\n    DNS -- 2. Returns LB IP (10.0.0.X) --\x3e User\n    User -- 3. HTTP Request --\x3e LB1\n    User -- 3. HTTP Request --\x3e LB2\n\n    LB1 -- Traffic --\x3e Server1\n    LB1 -- Traffic --\x3e Server2\n    LB2 -- Traffic --\x3e Server3\n    LB2 -- Traffic --\x3e Server4\n\n    %% Control Flows\n    OrchW -- Health Check --\x3e Server1\n    OrchW -- Health Check --\x3e Server2\n    OrchW -- Health Check --\x3e Server3\n    OrchW -- Health Check --\x3e Server4\n\n    OrchW -- Update Bad Server --\x3e ZK\n    OrchM -- Leader Election --\x3e ZK\n    \n    ZK -- Push Config Change --\x3e LB1\n    LB1 -- Watch --\x3e ZK\n    ZK -- Push Config Change --\x3e LB2\n    LB2 -- Watch --\x3e ZK\n\n    style DNS fill:#f96,stroke:#333\n    style ZK fill:#ccf,stroke:#333\n    style LB1 fill:#f9f,stroke:#333\n    style LB2 fill:#f9f,stroke:#333"})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},4396:e=>{e.exports=JSON.parse('{"permalink":"/docs/blog/load-balancer-design","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/load-balancer.md","source":"@site/blog/load-balancer.md","title":"Designing L7 Load Balancer","description":"In this series, we will evolve the design of a Layer 7 Load Balancer (LB) from scratch. We won\'t just look at how to build it, but why we make specific architectural choices at every fork in the road.","date":"2025-12-30T00:00:00.000Z","tags":[{"inline":true,"label":"system-design","permalink":"/docs/blog/tags/system-design"},{"inline":true,"label":"load-balancer","permalink":"/docs/blog/tags/load-balancer"},{"inline":true,"label":"distributed-systems","permalink":"/docs/blog/tags/distributed-systems"},{"inline":true,"label":"aws","permalink":"/docs/blog/tags/aws"},{"inline":true,"label":"zookeeper","permalink":"/docs/blog/tags/zookeeper"}],"readingTime":13.76,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Designing L7 Load Balancer","slug":"load-balancer-design","tags":["system-design","load-balancer","distributed-systems","aws","zookeeper"],"authors":"ashish","date":"2025-12-30T00:00:00.000Z"},"unlisted":false,"nextItem":{"title":"Avoid \\"REPLACE INTO\\"","permalink":"/docs/blog/concurrent-replace"}}')},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var s=r(6540);const t={},i=s.createContext(t);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);