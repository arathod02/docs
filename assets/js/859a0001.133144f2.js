"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[4565],{5524:e=>{e.exports=JSON.parse('{"permalink":"/docs/blog/distributed-key-value-store","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/distributed-key-value-store.md","source":"@site/blog/distributed-key-value-store.md","title":"Distributed Key-Value Store","description":"In this post, we will walk through the journey of designing and implementing a horizontally scalable Key-Value store. We will start from a basic implementation, iterate through schema designs, optimize for performance, and finally scale it to handle massive loads.","date":"2025-12-28T00:00:00.000Z","tags":[],"readingTime":7.42,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"slug":"distributed-key-value-store","title":"Distributed Key-Value Store","authors":"ashish","date":"2025-12-28T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Designing Slack\'s Realtime Communication System","permalink":"/docs/blog/slack-design"},"nextItem":{"title":"Deep Dive into Database Pessimistic & Optimistic Locking","permalink":"/docs/blog/db-pessimistic-optimistic-locking"}}')},5927:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>d});var t=i(5524),s=i(4848),a=i(8453);const r={slug:"distributed-key-value-store",title:"Distributed Key-Value Store",authors:"ashish",date:new Date("2025-12-28T00:00:00.000Z")},l=void 0,o={authorsImageUrls:[void 0]},d=[{value:"Requirements",id:"requirements",level:3},{value:"1. The Initial Architecture",id:"1-the-initial-architecture",level:2},{value:"2. Schema Design: The Performance Trap",id:"2-schema-design-the-performance-trap",level:2},{value:"Option 1: Calculated Expiry",id:"option-1-calculated-expiry",level:3},{value:"Option 2: Pre-calculated Expiry",id:"option-2-pre-calculated-expiry",level:3},{value:"Why Option 1 is a disaster for scale",id:"why-option-1-is-a-disaster-for-scale",level:3},{value:"Why Option 2 wins",id:"why-option-2-wins",level:3},{value:"3. Implementing Operations",id:"3-implementing-operations",level:2},{value:"The INSERT / PUT Operation",id:"the-insert--put-operation",level:3},{value:"The GET Operation",id:"the-get-operation",level:3},{value:"The DEL Operation",id:"the-del-operation",level:3},{value:"4. The TTL Cleanup (Garbage Collection)",id:"4-the-ttl-cleanup-garbage-collection",level:2},{value:"5. Scaling the System",id:"5-scaling-the-system",level:2},{value:"Scenario 1: High Traffic (Read/Write)",id:"scenario-1-high-traffic-readwrite",level:3},{value:"Scenario 2: Read Heavy Load",id:"scenario-2-read-heavy-load",level:3},{value:"Scenario 3: Write Heavy Load",id:"scenario-3-write-heavy-load",level:3},{value:"Final Architecture",id:"final-architecture",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"In this post, we will walk through the journey of designing and implementing a horizontally scalable Key-Value store. We will start from a basic implementation, iterate through schema designs, optimize for performance, and finally scale it to handle massive loads."}),"\n",(0,s.jsx)(n.h3,{id:"requirements",children:"Requirements"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Horizontally Scalable:"})," The system must handle growth in data and traffic."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Core Operations:"})," Support ",(0,s.jsx)(n.code,{children:"GET"}),", ",(0,s.jsx)(n.code,{children:"PUT"}),", ",(0,s.jsx)(n.code,{children:"DEL"}),", and ",(0,s.jsx)(n.code,{children:"TTL"})," (Time To Live)."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"1-the-initial-architecture",children:"1. The Initial Architecture"}),"\n",(0,s.jsx)(n.p,{children:"To begin, let's look at a typical 3-tier application architecture. This serves as our baseline."}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR\n    Client[Client] --\x3e LB[Load Balancer]\n    LB --\x3e API[API Server Cluster]\n    API --\x3e DB[(MySQL DB)]"}),"\n",(0,s.jsxs)(n.admonition,{title:"Concept: Storage and Compute Separation",type:"tip",children:[(0,s.jsxs)(n.p,{children:["One of the most powerful architectural patterns in modern cloud computing is the ",(0,s.jsx)(n.strong,{children:"Separation of Storage and Compute"}),"."]}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Independent Scaling:"})," You can scale your processing power (API/Compute) independently from your data storage size. If your logic becomes complex but data volume is low, you scale compute. If you store petabytes but access it rarely, you scale storage."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cost Optimization:"})," This significantly reduces TCO (Total Cost of Ownership). You don't pay for CPU cycles you don't use just because you need more disk space."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cloud Native Examples:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Snowflake:"}),' Pioneered this by allowing data to sit in S3 (cheap object storage) while spinning up "Warehouses" (Compute) only when queries run.']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Aurora:"})," The compute node processes queries, but the storage is a distributed, self-healing volume that grows automatically."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS DocumentDB:"})," This is a classic example. Clients see a MongoDB-compatible API (NoSQL), but it is actually built on top of the Aurora storage engine (Relational/Cloud Native). ",(0,s.jsx)(n.strong,{children:"Clients care about guarantees and APIs, not the underlying engine."})]}),"\n"]}),"\n"]}),"\n"]})]}),"\n",(0,s.jsxs)(n.p,{children:["For this blog, we will use ",(0,s.jsx)(n.strong,{children:"MySQL"})," as our persistence layer."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"2-schema-design-the-performance-trap",children:"2. Schema Design: The Performance Trap"}),"\n",(0,s.jsx)(n.p,{children:"We need to store a Key, a Value, and manage the lifecycle (TTL) of the data. Let's evaluate two schema options."}),"\n",(0,s.jsx)(n.h3,{id:"option-1-calculated-expiry",children:"Option 1: Calculated Expiry"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE store (\n    `key` VARCHAR(255) PRIMARY KEY,\n    `value` TEXT,\n    `created_at` DATETIME,\n    `ttl` INT\n);\n"})}),"\n",(0,s.jsxs)(n.p,{children:["To delete expired items, the query would be:\n",(0,s.jsx)(n.code,{children:"DELETE FROM store WHERE created_at + ttl < NOW();"})]}),"\n",(0,s.jsx)(n.h3,{id:"option-2-pre-calculated-expiry",children:"Option 2: Pre-calculated Expiry"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"CREATE TABLE store (\n    `key` VARCHAR(255) PRIMARY KEY,\n    `value` TEXT,\n    `expired_at` BIGINT,\n    INDEX idx_expiry (expired_at)\n);\n"})}),"\n",(0,s.jsx)(n.h3,{id:"why-option-1-is-a-disaster-for-scale",children:"Why Option 1 is a disaster for scale"}),"\n",(0,s.jsxs)(n.p,{children:["In Option 1, the condition ",(0,s.jsx)(n.code,{children:"WHERE created_at + ttl < NOW()"})," creates a massive performance bottleneck."]}),"\n",(0,s.jsxs)(n.p,{children:["When you perform an operation on a column (like addition) on the left side of a comparison, the database ",(0,s.jsx)(n.strong,{children:"cannot"})," efficiently use a B-Tree index. The database engine must perform the calculation ",(0,s.jsx)(n.code,{children:"created_at + ttl"})," for ",(0,s.jsx)(n.strong,{children:"every single row"})," in the table to determine if it meets the criteria."]}),"\n",(0,s.jsxs)(n.p,{children:["This results in a ",(0,s.jsx)(n.strong,{children:"Full Table Scan"})," (or a full index scan), which operates at ",(0,s.jsx)(n.code,{children:"O(N)"})," complexity. If you have 100 million rows, the DB reads 100 million rows."]}),"\n",(0,s.jsx)(n.h3,{id:"why-option-2-wins",children:"Why Option 2 wins"}),"\n",(0,s.jsxs)(n.p,{children:["In Option 2, we store ",(0,s.jsx)(n.code,{children:"expired_at"}),". This is a static value."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Sorted Storage:"})," The Secondary Index on ",(0,s.jsx)(n.code,{children:"expired_at"})," keeps pointers sorted by time."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficient Range Scan:"})," The query becomes ",(0,s.jsx)(n.code,{children:"WHERE expired_at < NOW()"}),". The database goes to the index, finds the first entry, and reads sequentially until it hits the timestamp for ",(0,s.jsx)(n.code,{children:"NOW()"}),". This is an efficient range seek."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Decision:"})," We will proceed with ",(0,s.jsx)(n.strong,{children:"Option 2"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"3-implementing-operations",children:"3. Implementing Operations"}),"\n",(0,s.jsx)(n.p,{children:"Now let's implement the core logic, keeping network efficiency and database load in mind."}),"\n",(0,s.jsx)(n.h3,{id:"the-insert--put-operation",children:"The INSERT / PUT Operation"}),"\n",(0,s.jsx)(n.p,{children:"A naive implementation might look like this:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"def put(key, value, ttl):\n    start_transaction()     # 1. Network Call\n    v = get(key)            # 2. Network Call\n    if v:\n        update(key, value)  # 3. Network Call\n    else:\n        insert(key, value)  # 3. Network Call\n    commit()                # 4. Network Call\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Problem:"})," This requires up to 4 distinct network round trips to the database for a single application request. At scale, this latency kills performance."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Solution:"})," Use an ",(0,s.jsx)(n.strong,{children:"UPSERT"}),". In MySQL, we can use ",(0,s.jsx)(n.code,{children:"REPLACE INTO"})," or ",(0,s.jsx)(n.code,{children:"INSERT ON DUPLICATE KEY UPDATE"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"INSERT INTO store (`key`, `value`, `expired_at`) \n                VALUES (%s, %s, NOW())\n                ON DUPLICATE KEY UPDATE \n                `value` = VALUES(`value`), \n                `expired_at` = NOW()\n"})}),"\n",(0,s.jsx)(n.admonition,{title:'Avoid "REPLACE INTO"',type:"tip",children:(0,s.jsxs)(n.p,{children:["More details can be found ",(0,s.jsx)(n.a,{href:"/docs/blog/concurrent-replace",children:"here"})]})}),"\n",(0,s.jsx)(n.admonition,{title:"Transaction Autocommit",type:"info",children:(0,s.jsxs)(n.p,{children:["In modern databases (like MySQL with InnoDB), if you fire a single statement like ",(0,s.jsx)(n.code,{children:"REPLACE INTO"})," or ",(0,s.jsx)(n.code,{children:"UPDATE"}),", the engine implicitly starts a transaction, executes the statement, and commits it. You do not need to manually handle transaction boundaries for single atomic operations."]})}),"\n",(0,s.jsxs)(n.p,{children:["We have optimized 4 operations down to ",(0,s.jsx)(n.strong,{children:"1"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"the-get-operation",children:"The GET Operation"}),"\n",(0,s.jsx)(n.p,{children:"We must ensure we don't return expired data, even if the cleanup job hasn't run yet."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"SELECT value \nFROM store \nWHERE key = 'my_key' AND expired_at > NOW();\n"})}),"\n",(0,s.jsx)(n.h3,{id:"the-del-operation",children:"The DEL Operation"}),"\n",(0,s.jsx)(n.admonition,{title:"Concept: Hard vs. Soft Deletes",type:"note",children:(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Hard Delete:"})," Physically removes the row from the storage. This causes immediate I/O overhead as the B-Tree must rebalance and pages might need to be merged."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Soft Delete:"})," Updates a flag (e.g., ",(0,s.jsx)(n.code,{children:"is_deleted=true"}),") or a timestamp. The data remains but is logically invisible. This minimizes immediate I/O fluctuation."]}),"\n"]})}),"\n",(0,s.jsxs)(n.p,{children:["To implement ",(0,s.jsx)(n.code,{children:"DEL"})," efficiently, we will perform a ",(0,s.jsx)(n.strong,{children:"Soft Delete"})," by expiring the key immediately. We update the ",(0,s.jsx)(n.code,{children:"expired_at"})," to a past timestamp (or a sentinel value like -1) to indicate it was deleted by the user, not naturally expired.\n",(0,s.jsxs)(n.em,{children:["More details on hard deletes can be found ",(0,s.jsx)(n.a,{href:"/docs/blog/avoid-hard-deletes",children:"here"}),"."]})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"UPDATE store \nSET expired_at = -1 \nWHERE key = 'my_key' AND expired_at > NOW();\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"4-the-ttl-cleanup-garbage-collection",children:"4. The TTL Cleanup (Garbage Collection)"}),"\n",(0,s.jsx)(n.p,{children:"We need a background cron job to physically remove expired rows."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Naive Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"DELETE FROM store WHERE expired_at < NOW();\n"})}),"\n",(0,s.jsxs)(n.admonition,{title:"Performance Risk: Unbounded Deletes",type:"warning",children:[(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Never perform unbounded deletes in production."}),"\nIf your table has 100 million rows and 10 million are expired:"]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"The DB tries to lock all 10 million rows."}),"\n",(0,s.jsx)(n.li,{children:"It fills up the Undo/Redo logs to support rollback."}),"\n",(0,s.jsxs)(n.li,{children:["It spikes CPU and IO, potentially blocking live traffic.\n",(0,s.jsx)(n.strong,{children:"Bounded Deletes are always superior."})," Deleting 100 rows in a loop is infinitely better than crashing the DB trying to delete 10k at once."]}),"\n"]})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Optimized Approach (Batching):"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sql",children:"DELETE FROM store \nWHERE expired_at < UNIX_TIMESTAMP() \nLIMIT 1000;\n"})}),"\n",(0,s.jsx)(n.p,{children:"The cron job should run this query in a loop until the affected row count is 0."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"5-scaling-the-system",children:"5. Scaling the System"}),"\n",(0,s.jsx)(n.p,{children:"We have the logic. Now, let's handle the growth."}),"\n",(0,s.jsx)(n.h3,{id:"scenario-1-high-traffic-readwrite",children:"Scenario 1: High Traffic (Read/Write)"}),"\n",(0,s.jsx)(n.p,{children:"If the number of requests spikes, the first bottleneck is usually the CPU/Memory on the API servers."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution:"})," Horizontally scale the API layer."]}),"\n",(0,s.jsx)(n.admonition,{title:"Recap: Storage/Compute Separation",type:"tip",children:(0,s.jsx)(n.p,{children:"Because our API (Compute) is decoupled from the DB (Storage), we can spin up 50 new API instances without touching the Database configuration."})}),"\n",(0,s.jsx)(n.mermaid,{value:"graph LR\n    LB[Load Balancer] --\x3e API1[API Server 1]\n    LB --\x3e API2[API Server 2]\n    LB --\x3e API3[API Server 3]\n    API1 --\x3e DB[(Master DB)]\n    API2 --\x3e DB\n    API3 --\x3e DB"}),"\n",(0,s.jsx)(n.h3,{id:"scenario-2-read-heavy-load",children:"Scenario 2: Read Heavy Load"}),"\n",(0,s.jsxs)(n.p,{children:["As you grow, the single Master DB will struggle to handle all the ",(0,s.jsx)(n.code,{children:"SELECT"})," queries."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solution:"})," Add Read Replicas."]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    API[API Cluster] --\x3e Master[(DB Master)]\n    API --\x3e ReadReplica1[(Read Replica 1)]\n    API --\x3e ReadReplica2[(Read Replica 2)]\n    \n    Master -- Replication --\x3e ReadReplica1\n    Master -- Replication --\x3e ReadReplica2\n    \n    style Master fill:#f96,stroke:#333"}),"\n",(0,s.jsxs)(n.p,{children:["We route ",(0,s.jsx)(n.code,{children:"PUT/DEL"})," to Master, and ",(0,s.jsx)(n.code,{children:"GET"})," to Replicas."]}),"\n",(0,s.jsxs)(n.admonition,{title:"The Cost of Scale: Eventual Consistency",type:"danger",children:[(0,s.jsxs)(n.p,{children:["When you introduce Read Replicas, you introduce ",(0,s.jsx)(n.strong,{children:"Replication Lag"}),"."]}),(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Client A writes ",(0,s.jsx)(n.code,{children:"Key=X"})," to Master."]}),"\n",(0,s.jsxs)(n.li,{children:["Client B reads ",(0,s.jsx)(n.code,{children:"Key=X"})," from Replica 1 immediately."]}),"\n",(0,s.jsxs)(n.li,{children:["If the data hasn't copied over yet (lag), Client B gets a ",(0,s.jsx)(n.strong,{children:"Cache Miss"})," or stale data."]}),"\n"]}),(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Real World Solution: DynamoDB"}),"\nAmazon DynamoDB solves this by giving the client a choice. You can perform a standard read (eventually consistent, cheaper) or a ",(0,s.jsx)(n.strong,{children:"Strongly Consistent Read"}),".\nIf you request ",(0,s.jsx)(n.code,{children:"ConsistentRead=True"}),", DynamoDB routes the request to the Leader node to guarantee the latest data, but it consumes 2x capacity units (costs more)."]})]}),"\n",(0,s.jsx)(n.h3,{id:"scenario-3-write-heavy-load",children:"Scenario 3: Write Heavy Load"}),"\n",(0,s.jsx)(n.p,{children:"Eventually, your Master DB won't handle the incoming write volume."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Step 1: Vertical Scaling."})," Upgrade the Master to a bigger machine (more RAM, better CPU).\n",(0,s.jsx)(n.strong,{children:"Step 2: Sharding."})," When vertical scaling hits the limit (physics or cost), we shard."]}),"\n",(0,s.jsxs)(n.p,{children:["More details on sharding can be found in my previous ",(0,s.jsx)(n.a,{href:"/docs/blog/scaling-distributed-systems#3-sharding",children:"post on sharding"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"final-architecture",children:"Final Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Here is the final system diagram, fully scaled vertically and horizontally, featuring sharded masters and read replicas."}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart TD\n    Client --\x3e LB["Load Balancer"]\n    \n    subgraph API_Layer ["Horizontally Scaled API"]\n        API1["API Server 1"]\n        API2["API Server .."]\n        API3["API Server N"]\n    end\n\n    LB --\x3e API1 & API2 & API3\n    \n    subgraph Shard_1 ["Shard 1: Keys A-M"]\n        M1[("Master 1")] -- Async Repl --\x3e R1a[("Replica 1A")]\n        M1 -- Async Repl --\x3e R1b[("Replica 1B")]\n    end\n    \n    subgraph Shard_2 ["Shard 2: Keys N-Z"]\n        M2[("Master 2")] -- Async Repl --\x3e R2a[("Replica 2A")]\n        M2 -- Async Repl --\x3e R2b[("Replica 2B")]\n    end\n    \n    %% Routing connections\n    API1 & API2 -.->|"Writes A-M"| M1\n    API1 & API2 -.->|"Writes N-Z"| M2\n    \n    %% Read connections (Simplified)\n    API2 -.->|"Reads A-M"| R1a & R1b\n    API2 -.->|"Reads N-Z"| R2a & R2b\n\n    style M1 fill:#f96,stroke:#333\n    style M2 fill:#f96,stroke:#333'})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var t=i(6540);const s={},a=t.createContext(s);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);