"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[9258],{2253:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/concurrent-replace","metadata":{"permalink":"/docs/blog/concurrent-replace","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/concurrent-replace.md","source":"@site/blog/concurrent-replace.md","title":"Avoid \\"REPLACE INTO\\"","description":"In distributed systems, Upsert (Update or Insert) is a fundamental operation. You want to store a record: if it exists, update it; if not, create it.","date":"2025-12-29T00:00:00.000Z","tags":[],"readingTime":5.17,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Avoid \\"REPLACE INTO\\"","authors":"ashish","date":"2025-12-29T00:00:00.000Z"},"unlisted":false,"nextItem":{"title":"Designing Slack\'s Realtime Communication System","permalink":"/docs/blog/slack-design"}},"content":"In distributed systems, **Upsert** (Update or Insert) is a fundamental operation. You want to store a record: if it exists, update it; if not, create it.\\n\\nMySQL offers a deceptively simple command to handle this: `REPLACE INTO`. It looks like a standard insert, works like a charm in development, and satisfies the requirement of \\"single statement atomicity.\\"\\n\\nHowever, under high concurrency, `REPLACE INTO` can become a source of **deadlocks**, **latency spikes**, and **unexpected behavior**.\\n\\nIn this post, we will tear apart the `REPLACE` statement to understand its internal mechanics, how InnoDB handles locking, and why your client application **must** be prepared to handle concurrency explicitly.\\n\\n## 1. The \\"Atomic\\" Myth\\n\\nDevelopers often assume that because `REPLACE INTO` is a single SQL statement, it is a single atomic operation that effectively \\"locks\\" the row, does the job, and leaves.\\n\\n**The Reality:**\\nInternally, `REPLACE INTO` is **not** a simple update. It is a destructive two-step process disguised as one.\\n\\n:::danger How REPLACE works internally\\n1.  **Check:** It attempts to insert the new row.\\n2.  **Conflict:** If a Unique Key or Primary Key collision occurs:\\n    * **Step A:** It performs a **DELETE** of the conflicting row.\\n    * **Step B:** It performs a fresh **INSERT** of the new row.\\n      :::\\n\\nThis \\"Delete + Insert\\" behavior has massive implications for locking and referential integrity.\\n\\n---\\n\\n## 2. The Locking Mechanism: Why Deadlocks Happen\\n\\nTo understand why concurrent `REPLACE` operations cause deadlocks, we need to look at **InnoDB Locking**.\\n\\n### The Gap Lock & Next-Key Lock\\nWhen you write to a table with a Unique Index, MySQL must ensure that no other transaction inserts a duplicate value while your transaction is running.\\n\\n1.  **Shared Lock (S-Lock):** When `REPLACE` detects a duplicate, it first acquires a *Shared Lock* on the existing record to ensure it stays there while it decides what to do.\\n2.  **Exclusive Lock (X-Lock):** To delete the old row and insert the new one, it must upgrade that lock to an *Exclusive Lock*.\\n\\n### The Deadlock Scenario\\n\\nImagine two clients (Transaction A and Transaction B) trying to `REPLACE` the **same key** at the exact same time.\\n\\n```mermaid\\nsequenceDiagram\\n    participant TxA as Transaction A\\n    participant DB as MySQL (InnoDB)\\n    participant TxB as Transaction B\\n\\n    Note over TxA, TxB: Both try to REPLACE \'Key=100\'\\n    \\n    TxA->>DB: REPLACE INTO ... VALUES (100)\\n    DB--\x3e>TxA: Conflict Detected. Acquires S-Lock (Shared) on Key=100.\\n    \\n    TxB->>DB: REPLACE INTO ... VALUES (100)\\n    DB--\x3e>TxB: Conflict Detected. Acquires S-Lock (Shared) on Key=100.\\n    \\n    Note over DB: Both hold S-Locks. Reading is fine.\\n    \\n    TxA->>DB: Attempts to DELETE Key=100 (Needs X-Lock)\\n    Note over DB: TxA waits for TxB to release S-Lock.\\n    \\n    TxB->>DB: Attempts to DELETE Key=100 (Needs X-Lock)\\n    Note over DB: TxB waits for TxA to release S-Lock.\\n    \\n    Note over TxA, TxB: \ud83d\udca5 DEADLOCK \ud83d\udca5\\n    DB--\x3e>TxA: Error 1213: Deadlock found when trying to get lock\\n```\\n\\n:::warning The Deadlock Trap\\n1.  **TxA** holds **S-Lock**. Wants **X-Lock**. Waits for **TxB**.\\n2.  **TxB** holds **S-Lock**. Wants **X-Lock**. Waits for **TxA**.\\n3.  InnoDB detects the cycle and kills one transaction (usually the smaller one) to break the deadlock.\\n    :::\\n\\n**Does MySQL inherently handle it?**\\nTechnically, yes\u2014by **killing** one of your requests. MySQL resolves the deadlock by forcing an error on one client. It does **not** queue them up nicely; it breaks the logjam with force.\\n\\n**Does the client need to handle it?**\\n**YES.** Your code **must** catch Error `1213` and implement a retry logic.\\n\\n---\\n\\n## 3. REPLACE vs. INSERT ON DUPLICATE KEY UPDATE\\n\\nIf `REPLACE` is so dangerous, what should we use?\\nThe industry standard alternative is `INSERT ... ON DUPLICATE KEY UPDATE` (IODKU).\\n\\nLet\'s compare the internal semantics.\\n\\n| Feature | `REPLACE INTO` | `INSERT ON DUPLICATE ...` |\\n| :--- | :--- | :--- |\\n| **Mechanic** | Delete + Insert | Update |\\n| **Primary Key** | **Changes** (New ID generated) | **Preserved** (ID stays same) |\\n| **Foreign Keys** | **Breaks** (Cascading Deletes triggered) | **Safe** (References maintained) |\\n| **Locking** | Aggressive (Gap/Next-Key Locks) | Moderate (Row Locks) |\\n| **Deadlocks** | High Probability | Lower Probability (but non-zero) |\\n\\n### Visualizing the Mechanics\\n\\n```mermaid\\nflowchart TD\\n    subgraph REPLACE [\\"REPLACE INTO\\"]\\n        R1[Try Insert] --\x3e R2{Conflict?}\\n        R2 -- No --\x3e R3[Insert New Row]\\n        R2 -- Yes --\x3e R4[DELETE Old Row]\\n        R4 --\x3e R5[INSERT New Row]\\n        style R4 fill:#ff9999,stroke:#333\\n        style R5 fill:#99ff99,stroke:#333\\n    end\\n\\n    subgraph IODKU [\\"INSERT ON DUPLICATE\\"]\\n        I1[Try Insert] --\x3e I2{Conflict?}\\n        I2 -- No --\x3e I3[Insert New Row]\\n        I2 -- Yes --\x3e I4[UPDATE Existing Row]\\n        style I4 fill:#99ccff,stroke:#333\\n    end\\n```\\n\\n:::tip Pro Tip: Why IODKU is usually better\\n`INSERT ON DUPLICATE KEY UPDATE` is non-destructive.\\n1.  It performs a \\"logical\\" update.\\n2.  It does not churn your auto-increment IDs. `REPLACE` creates \\"holes\\" in your ID sequence because every update burns an ID.\\n3.  It avoids `DELETE` triggers firing unexpectedly.\\n    :::\\n\\n---\\n\\n## 4. Does MySQL inherently handle concurrent upserts?\\n\\nMySQL handles **Data Integrity** (ACID) perfectly. It will never let two transactions corrupt the data. However, it handles concurrency by using **Locks**. When locks conflict cyclically, it handles it via **Deadlock Detection** (rolling back one transaction).\\n\\nIt does **not** inherently \\"serialize\\" them without side effects. The side effect is the Deadlock Error.\\n\\n### What are the internal semantics?\\n1.  **Gap Locks:** Even with `READ COMMITTED` isolation, unique constraint checks trigger Gap Locks or Next-Key Locks. This locks the \\"space\\" before and after the record.\\n2.  **Lock Upgrades:** The transition from Shared (Read) to Exclusive (Write) is the danger zone.\\n\\n---\\n\\n## 5. Implementation Strategy\\n\\nIf you are building a high-scale system, you should prefer `INSERT ON DUPLICATE KEY UPDATE`. However, regardless of which statement you choose, you must implement a retry loop in your application code.\\n\\n### Python Pseudo-code for Robust Upsert\\n\\n```python\\nMAX_RETRIES = 3\\n\\ndef upsert_data(key, value):\\n    retries = 0\\n    while retries < MAX_RETRIES:\\n        try:\\n            # Prefer IODKU over REPLACE\\n            cursor.execute(\\"\\"\\"\\n                INSERT INTO store (`key`, `value`, `updated_at`) \\n                VALUES (%s, %s, NOW())\\n                ON DUPLICATE KEY UPDATE \\n                `value` = VALUES(`value`), \\n                `updated_at` = NOW()\\n            \\"\\"\\", (key, value))\\n            \\n            db.commit()\\n            return # Success!\\n            \\n        except MySQLdb.OperationalError as e:\\n            if e.args[0] == 1213: # Error Code: Deadlock found\\n                retries += 1\\n                time.sleep(0.1 * retries) # Exponential Backoff\\n                continue\\n            else:\\n                raise e # Real error, re-raise\\n```\\n\\n:::note Key Takeaway\\nAt scale, **Database Deadlocks are not bugs; they are a fact of life.** They are the database\'s way of protecting your data\'s consistency. Your application logic must be resilient enough to retry when they happen.\\n:::"},{"id":"/slack-design","metadata":{"permalink":"/docs/blog/slack-design","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/slack-design.md","source":"@site/blog/slack-design.md","title":"Designing Slack\'s Realtime Communication System","description":"In this post, we will deep dive into the architecture of an enterprise-grade realtime communication system like Slack. We will categorize different communication patterns, design the database schema for performance, and evolve the system architecture from a simple REST service to a scalable, distributed websocket cluster.","date":"2025-12-29T00:00:00.000Z","tags":[{"inline":true,"label":"system-design","permalink":"/docs/blog/tags/system-design"},{"inline":true,"label":"websocket","permalink":"/docs/blog/tags/websocket"},{"inline":true,"label":"redis","permalink":"/docs/blog/tags/redis"},{"inline":true,"label":"architecture","permalink":"/docs/blog/tags/architecture"}],"readingTime":9.18,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"id":"slack-realtime-architecture","title":"Designing Slack\'s Realtime Communication System","sidebar_label":"Slack Architecture","tags":["system-design","websocket","redis","architecture"],"authors":"ashish","date":"2025-12-29T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Avoid \\"REPLACE INTO\\"","permalink":"/docs/blog/concurrent-replace"},"nextItem":{"title":"Distributed Key-Value Store","permalink":"/docs/blog/distributed-key-value-store"}},"content":"In this post, we will deep dive into the architecture of an enterprise-grade realtime communication system like Slack. We will categorize different communication patterns, design the database schema for performance, and evolve the system architecture from a simple REST service to a scalable, distributed websocket cluster.\\n\\n## 1. Categorization of Communication Systems\\n\\nNot all chat applications are built the same. The architectural choices depend heavily on the business requirements regarding **Persistence** (saving the message) versus **Delivery** (getting the message to the recipient).\\n\\n### The Three Scenarios\\n\\nWe can visualize the three main architectural patterns in the diagram below:\\n\\n```mermaid\\nflowchart TD\\n    subgraph Slack [\\"Persistence Over Delivery (Slack)\\"]\\n        direction TB\\n        A1[Client A] --\x3e|1. REST POST| LB1[Load Balancer]\\n        LB1 --\x3e Svc[Message Service]\\n        Svc --\x3e|2. Write Blocking| DB[(Database)]\\n        Svc -.->|3. Async Fanout| WSS1[WSS]\\n    end\\n\\n    subgraph WhatsApp [\\"Delivery Over Persistence (WhatsApp)\\"]\\n        direction TB\\n        A2[Client A] <--\x3e|WebSocket| WSS2[WSS Cluster]\\n        WSS2 <--\x3e|WebSocket| B2[Client B]\\n        WSS2 -.->|Async Backup| Broker[Message Broker]\\n        Broker -.-> DB2[(Archival DB)]\\n    end\\n\\n    subgraph Zoom [\\"No Persistence (Zoom)\\"]\\n        direction TB\\n        A3[Client A] <--\x3e|WebSocket| WSS3[WSS Cluster]\\n        WSS3 <--\x3e|WebSocket| B3[Client B]\\n        WSS3 -.->|No Storage| Void((Void))\\n    end\\n```\\n\\n### Deep Dive: Persistence vs. Delivery\\n\\n#### 1. Persistence Over Delivery (Slack/Teams)\\nThis is the standard for **Enterprise Grade** systems.\\n* **Why?** In an enterprise setting, the \\"Source of Truth\\" is the server. If a user logs in from a new device, they must see the entire history. Regulatory compliance often mandates that data is never lost.\\n* **The Mechanic:** We must guarantee the message is saved to the database *before* we attempt to deliver it. Delivery is a secondary side-effect of persistence. If the DB write fails, the message is not sent.\\n\\n#### 2. Delivery Over Persistence (WhatsApp)\\nThis leverages **Local Storage**.\\n* **Why?** Speed and Privacy. By prioritizing delivery, the app feels \\"snappy.\\" The server often acts as a router.\\n* **The Mechanic:** The delivery to the recipient is the primary goal. Persistence might happen asynchronously or only on the user\'s local device. If the server crashes, the message might still exist on the phones, but not in the cloud.\\n\\n#### 3. Realtime No Persistence (Zoom Chat)\\nThis is **Ephemeral**.\\n* **Why?** Contextual relevance. The chat is only relevant while the video session is active.\\n* **The Mechanic:** Data flows through the WebSocket server and is immediately discarded. If you join a meeting late, the history is gone because it was never saved.\\n\\n---\\n\\n## 2. Designing the Core Schema\\n\\nData modeling is the foundation of scale. Let\'s look at how a naive approach fails and how to fix it.\\n\\n### The Naive Schema\\n\\n* **Users:** `id`, `name`, `email`\\n* **Channels:** `id`, `name`\\n* **Messages:** `id`, `sender_id`, `message`, `created_at`, `receiver_id` (Nullable: set if DM)\\n\\n**The Problematic Query:**\\nWhen User A opens a DM with User B, we need to fetch their history:\\n\\n```sql\\nSELECT * FROM messages \\nWHERE (receiver_id = \'B\' AND sender_id = \'A\') \\n   OR (receiver_id = \'A\' AND sender_id = \'B\') \\nORDER BY created_at DESC;\\n```\\n\\n:::danger Why this query fails at scale\\n**1. Complex Filtering:** The database engine has to evaluate `OR` conditions involving two different columns (`sender_id` and `receiver_id`). This often prevents the effective use of single-column indexes, potentially leading to a **Table Scan**.\\n**2. Time Complexity:** If you have billions of messages, a scan is **$O(N)$**. Even with separate indexes, merging results for the `OR` clause is expensive.\\n**3. Sorting:** Sorting by `created_at` on a massive dataset without a covering index adds significant overhead.\\n:::\\n\\n### The Optimized Schema\\n\\nTo fix this, we need to change how we view Direct Messages (DMs).\\n\\n:::tip Insight: DM\'s are just Channels\\n**We portray a DM as a single channel with exactly two participants.**\\n:::\\n\\nBy normalizing DMs into the \\"Channels\\" concept, we simplify our access patterns significantly.\\n\\n**New Schema:**\\n* **Channels:** `id`, `name`, `type` (ENUM: \'DM\', \'GROUP_CHANNEL\')\\n* **Messages:** `id`, `sender_id`, `message`, `channel_id`, `created_at`\\n    * *Index:* `CREATE INDEX idx_channel_created ON messages (channel_id, created_at);`\\n\\n**The Optimized Query:**\\nNow, whether it is a Group Channel or a DM, the query is identical:\\n\\n```sql\\nSELECT * FROM messages \\nWHERE channel_id = ? \\nORDER BY created_at DESC \\nLIMIT 20;\\n```\\n\\n:::success Why this is faster\\n**1. Index Utilization:** We are querying on a single column `channel_id`. This allows the DB to use a B-Tree index to jump directly to the relevant block of data.\\n**2. Time Complexity:** The lookup becomes **$O(\\\\log N)$** (to find the channel node in the B-Tree) + **$O(K)$** (to read the K limit messages).\\n**3. Pre-sorted:** By including `created_at` in the composite index, the data is already physically stored or indexed in the correct order. The DB doesn\'t need to perform a separate sort operation.\\n:::\\n\\n---\\n\\n## 3. Defining APIs and Protocols\\n\\nWe will build the system iteratively, starting with the Persistence layer.\\n\\n### 1. API to Get Messages (Scroll)\\n* **Endpoint:** `GET /scroll?channel_id={id}&offset={id}`\\n* **Usage:** Called when a user clicks a channel or scrolls up.\\n* **Architecture:** Simple REST call.\\n\\n### 2. API to Send Messages\\n* **Endpoint:** `POST /messages`\\n* **Payload:** `{ \\"channel_id\\": \\"123\\", \\"message\\": \\"Hello World\\" }`\\n\\n:::info Design Decision: REST vs WebSocket for Sending\\nWe intentionally use **REST** for sending messages to enforce **Persistence Over Delivery**.\\n1. The client sends a POST request.\\n2. The server writes to the DB.\\n3. Only on a successful DB write does the server acknowledge (200 OK) to the sender.\\n   This guarantees that if the sender thinks the message is sent, it is definitely saved.\\n   :::\\n\\n### 3. Delivering the Message (WebSockets)\\n\\nOnce the message is saved via REST, we need to push it to other users in real-time. We introduce a **WebSocket Service (WSS)**.\\n\\n* **WSS Role:** Maintains long-lived TCP connections with clients.\\n* **Membership Service:** The WSS needs to know *who* to send the message to. It queries the Membership Service to find all users in `channel_id`.\\n\\n**Current Architecture State:**\\n\\n```mermaid\\nflowchart TD\\n    ClientA[Client A] --\x3e|REST POST| API[Message Service]\\n    API --\x3e|1. Persist| DB[(Database)]\\n    API -.->|2. Push Payload| WSS[WebSocket Service]\\n    WSS -.->|3. Query Members| Member[Membership Service]\\n    WSS --\x3e|4. Push| ClientB[Client B]\\n    WSS --\x3e|4. Push| ClientC[Client C]\\n```\\n\\n---\\n\\n## 4. Scaling Websockets (Redis Pub/Sub)\\n\\nA single WebSocket server cannot hold millions of connections. We must scale out horizontally.\\n* **Problem:** If Client A is connected to `WSS-1` and Client B is connected to `WSS-2`, how does `WSS-1` send a message to Client B?\\n* **Solution:** Redis Pub/Sub.\\n\\n**The Strategy: Channel per Workspace**\\n1.  We create a Redis Channel for every generic Workspace (e.g., \\"Intuit Workspace\\").\\n2.  All WSS instances subscribe to the Redis channels for the workspaces they are currently serving users for.\\n3.  The Message Service publishes the message to the specific Redis Workspace Channel.\\n4.  Redis fans the message out to all WSS instances subscribed to that workspace.\\n5.  Each WSS instance checks its local connection list: \\"Do I have User B?\\" If yes, it sends the message.\\n\\n```mermaid\\nflowchart TD\\n    subgraph Services\\n    API[Message Service]\\n    Redis((Redis Pub/Sub))\\n    end\\n\\n    subgraph WSS_Layer [WebSocket Layer]\\n    WSS1[WSS Node 1]\\n    WSS2[WSS Node 2]\\n    WSS3[WSS Node 3]\\n    end\\n\\n    API --\x3e|1. Publish WorkspaceID| Redis\\n    Redis --\x3e|2. Fanout| WSS1\\n    Redis --\x3e|2. Fanout| WSS2\\n    Redis --\x3e|2. Fanout| WSS3\\n    \\n    WSS1 --x|3. User B not here| Void1[Ignore]\\n    WSS2 --\x3e|3. Found User B!| ClientB[Client B]\\n```\\n\\n---\\n\\n## 5. Naked Servers & Connection Balancing\\n\\nHow does a client know *which* WSS node to connect to? We cannot expose WSS nodes directly behind a standard Load Balancer because we need sticky, long-lived connections and intelligent routing.\\n\\n**The Solution: Connection Balancer**\\n1.  **Discovery:** Client calls `GET /connect`.\\n2.  **Logic:** The Connection Balancer checks the health and load of all WSS nodes (via a Heartbeat mechanism).\\n3.  **Assignment:** It returns the IP/URL of the best available WSS node (e.g., `wss://node-5.slack.com`). This is known as the \\"Naked Server\\" pattern because the client connects directly to the node, bypassing a reverse proxy for the websocket pipe.\\n4.  **Failover:** If `node-5` stops sending heartbeats, the Connection Balancer marks it as unhealthy. Clients disconnected from `node-5` will poll the Balancer again and be assigned to a new node.\\n\\n---\\n\\n## Final Architecture & Flow Walkthrough\\n\\nBelow is the complete system design. We have organized the architecture into logical layers to illustrate how the **REST (Sending)** path and **WebSocket (Receiving)** path operate in parallel yet converge at the data and Pub/Sub layers.\\n\\n```mermaid\\nflowchart TD\\n    %% Styles for visual distinction\\n    classDef client fill:#e1f5fe,stroke:#01579b,stroke-width:2px;\\n    classDef service fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px;\\n    classDef infra fill:#fff3e0,stroke:#ef6c00,stroke-width:2px;\\n    classDef storage fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px;\\n\\n    %% 1. CLIENT LAYER\\n    subgraph Clients [Client Layer]\\n        direction LR\\n        Sender(User A: Sender):::client\\n        Receiver(User B: Receiver):::client\\n    end\\n\\n    %% 2. ENTRY LAYER\\n    subgraph Entry [Entry Points]\\n        LB[Load Balancer]:::service\\n        CB[Connection Balancer]:::service\\n    end\\n\\n    %% 3. SERVICE LAYER\\n    subgraph Services [Service Layer]\\n        direction TB\\n        MsgSvc[Message Service]:::service\\n        MemberSvc[Membership Service]:::service\\n    end\\n\\n    %% 4. REALTIME LAYER\\n    subgraph Realtime [Realtime Infrastructure]\\n        Redis((Redis Pub/Sub)):::infra\\n        \\n        subgraph WSS_Cluster [Naked WebSocket Servers]\\n            direction LR\\n            WSS1[WSS Node 1]:::service\\n            WSS2[WSS Node 2]:::service\\n        end\\n    end\\n\\n    %% 5. STORAGE LAYER\\n    subgraph Data [Storage Layer]\\n        DB[(Primary DB)]:::storage\\n    end\\n\\n    %% --- CONNECTIONS & FLOWS ---\\n\\n    %% Flow A: Initial Connection (Blue Dotted)\\n    Receiver -.->|0. Request Node IP| CB\\n    CB -.->|Assign WSS-2| Receiver\\n    Receiver ===|Long-lived WS Connection| WSS2\\n\\n    %% Flow B: Sending Message (Solid Black)\\n    Sender --\x3e|1. POST /messages| LB\\n    LB --\x3e MsgSvc\\n    MsgSvc --\x3e|2. Write Blocking| DB\\n\\n    %% Flow C: Internal Fanout (Red)\\n    MsgSvc --\x3e|3. Publish Event| Redis\\n    Redis --\x3e|4. Fanout| WSS1\\n    Redis --\x3e|4. Fanout| WSS2\\n\\n    %% Flow D: Delivery (Green)\\n    WSS2 -.->|5. Get Members| MemberSvc\\n    WSS2 --\x3e|6. Push Message| Receiver\\n    \\n    %% Formatting Hints\\n    linkStyle 0,1 stroke:#29b6f6,stroke-width:2px,stroke-dasharray: 5 5;\\n    linkStyle 2 stroke:#01579b,stroke-width:3px;\\n    linkStyle 3,4,5 stroke:#333,stroke-width:2px;\\n    linkStyle 6,7,8 stroke:#ef6c00,stroke-width:2px;\\n    linkStyle 9 stroke:#2e7d32,stroke-width:2px,stroke-dasharray: 5 5;\\n    linkStyle 10 stroke:#2e7d32,stroke-width:3px;\\n```\\n\\n### Detailed Flow Walkthrough\\n\\n1.  **Connection Handshake (Blue Dotted Line):**\\n    * **User B (Receiver)** opens the app.\\n    * The client polls the **Connection Balancer** (Step 0) to find a healthy WebSocket server.\\n    * The Balancer assigns **WSS Node 2**.\\n    * User B establishes a direct, long-lived \\"naked\\" TCP/WebSocket connection with WSS Node 2.\\n\\n2.  **Sending (Black Path):**\\n    * **User A (Sender)** sends a message via **REST POST** to the Load Balancer (Step 1).\\n    * The **Message Service** writes the message to the **Primary DB** (Step 2).\\n    * *Constraint:* The server waits for the DB acknowledgment (`ACK`) before responding `200 OK` to User A. This ensures **Persistence**.\\n\\n3.  **Internal Routing (Orange Path):**\\n    * Once persisted, the Message Service publishes an event to **Redis Pub/Sub** (Step 3).\\n    * Redis fans this message out to *all* WebSocket Servers (Step 4) that are subscribed to the user\'s workspace channel.\\n\\n4.  **Delivery (Green Path):**\\n    * **WSS Node 1** receives the event, checks its internal map, sees it has no connection for User B, and ignores it.\\n    * **WSS Node 2** receives the event. It queries the **Membership Service** (Step 5) to resolve channel members.\\n    * It identifies that it holds the active socket for User B.\\n    * It pushes the message payload down the open WebSocket connection to **User B** (Step 6)."},{"id":"distributed-key-value-store","metadata":{"permalink":"/docs/blog/distributed-key-value-store","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/distributed-key-value-store.md","source":"@site/blog/distributed-key-value-store.md","title":"Distributed Key-Value Store","description":"In this post, we will walk through the journey of designing and implementing a horizontally scalable Key-Value store. We will start from a basic implementation, iterate through schema designs, optimize for performance, and finally scale it to handle massive loads.","date":"2025-12-28T00:00:00.000Z","tags":[],"readingTime":7.42,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"slug":"distributed-key-value-store","title":"Distributed Key-Value Store","authors":"ashish","date":"2025-12-28T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Designing Slack\'s Realtime Communication System","permalink":"/docs/blog/slack-design"},"nextItem":{"title":"Deep Dive into Database Pessimistic & Optimistic Locking","permalink":"/docs/blog/db-pessimistic-optimistic-locking"}},"content":"In this post, we will walk through the journey of designing and implementing a horizontally scalable Key-Value store. We will start from a basic implementation, iterate through schema designs, optimize for performance, and finally scale it to handle massive loads.\\n\\n### Requirements\\n1.  **Horizontally Scalable:** The system must handle growth in data and traffic.\\n2.  **Core Operations:** Support `GET`, `PUT`, `DEL`, and `TTL` (Time To Live).\\n\\n---\\n\\n## 1. The Initial Architecture\\n\\nTo begin, let\'s look at a typical 3-tier application architecture. This serves as our baseline.\\n\\n```mermaid\\ngraph LR\\n    Client[Client] --\x3e LB[Load Balancer]\\n    LB --\x3e API[API Server Cluster]\\n    API --\x3e DB[(MySQL DB)]\\n```\\n\\n:::tip Concept: Storage and Compute Separation\\nOne of the most powerful architectural patterns in modern cloud computing is the **Separation of Storage and Compute**.\\n\\n* **Independent Scaling:** You can scale your processing power (API/Compute) independently from your data storage size. If your logic becomes complex but data volume is low, you scale compute. If you store petabytes but access it rarely, you scale storage.\\n* **Cost Optimization:** This significantly reduces TCO (Total Cost of Ownership). You don\'t pay for CPU cycles you don\'t use just because you need more disk space.\\n* **Cloud Native Examples:**\\n    * **Snowflake:** Pioneered this by allowing data to sit in S3 (cheap object storage) while spinning up \\"Warehouses\\" (Compute) only when queries run.\\n    * **AWS Aurora:** The compute node processes queries, but the storage is a distributed, self-healing volume that grows automatically.\\n    * **AWS DocumentDB:** This is a classic example. Clients see a MongoDB-compatible API (NoSQL), but it is actually built on top of the Aurora storage engine (Relational/Cloud Native). **Clients care about guarantees and APIs, not the underlying engine.**\\n      :::\\n\\nFor this blog, we will use **MySQL** as our persistence layer.\\n\\n---\\n\\n## 2. Schema Design: The Performance Trap\\n\\nWe need to store a Key, a Value, and manage the lifecycle (TTL) of the data. Let\'s evaluate two schema options.\\n\\n### Option 1: Calculated Expiry\\n```sql\\nCREATE TABLE store (\\n    `key` VARCHAR(255) PRIMARY KEY,\\n    `value` TEXT,\\n    `created_at` DATETIME,\\n    `ttl` INT\\n);\\n```\\nTo delete expired items, the query would be:\\n`DELETE FROM store WHERE created_at + ttl < NOW();`\\n\\n### Option 2: Pre-calculated Expiry\\n```sql\\nCREATE TABLE store (\\n    `key` VARCHAR(255) PRIMARY KEY,\\n    `value` TEXT,\\n    `expired_at` BIGINT,\\n    INDEX idx_expiry (expired_at)\\n);\\n```\\n\\n### Why Option 1 is a disaster for scale\\n\\nIn Option 1, the condition `WHERE created_at + ttl < NOW()` creates a massive performance bottleneck.\\n\\nWhen you perform an operation on a column (like addition) on the left side of a comparison, the database **cannot** efficiently use a B-Tree index. The database engine must perform the calculation `created_at + ttl` for **every single row** in the table to determine if it meets the criteria.\\n\\nThis results in a **Full Table Scan** (or a full index scan), which operates at `O(N)` complexity. If you have 100 million rows, the DB reads 100 million rows.\\n\\n### Why Option 2 wins\\nIn Option 2, we store `expired_at`. This is a static value.\\n1.  **Sorted Storage:** The Secondary Index on `expired_at` keeps pointers sorted by time.\\n2.  **Efficient Range Scan:** The query becomes `WHERE expired_at < NOW()`. The database goes to the index, finds the first entry, and reads sequentially until it hits the timestamp for `NOW()`. This is an efficient range seek.\\n\\n**Decision:** We will proceed with **Option 2**.\\n\\n---\\n\\n## 3. Implementing Operations\\n\\nNow let\'s implement the core logic, keeping network efficiency and database load in mind.\\n\\n### The INSERT / PUT Operation\\n\\nA naive implementation might look like this:\\n\\n```python\\ndef put(key, value, ttl):\\n    start_transaction()     # 1. Network Call\\n    v = get(key)            # 2. Network Call\\n    if v:\\n        update(key, value)  # 3. Network Call\\n    else:\\n        insert(key, value)  # 3. Network Call\\n    commit()                # 4. Network Call\\n```\\n\\n**The Problem:** This requires up to 4 distinct network round trips to the database for a single application request. At scale, this latency kills performance.\\n\\n**The Solution:** Use an **UPSERT**. In MySQL, we can use `REPLACE INTO` or `INSERT ON DUPLICATE KEY UPDATE`.\\n\\n```sql\\nINSERT INTO store (`key`, `value`, `expired_at`) \\n                VALUES (%s, %s, NOW())\\n                ON DUPLICATE KEY UPDATE \\n                `value` = VALUES(`value`), \\n                `expired_at` = NOW()\\n```\\n:::tip Avoid \\"REPLACE INTO\\" \\nMore details can be found [here](/concurrent-replace.md)\\n:::\\n\\n:::info Transaction Autocommit\\nIn modern databases (like MySQL with InnoDB), if you fire a single statement like `REPLACE INTO` or `UPDATE`, the engine implicitly starts a transaction, executes the statement, and commits it. You do not need to manually handle transaction boundaries for single atomic operations.\\n:::\\n\\nWe have optimized 4 operations down to **1**.\\n\\n### The GET Operation\\n\\nWe must ensure we don\'t return expired data, even if the cleanup job hasn\'t run yet.\\n\\n```sql\\nSELECT value \\nFROM store \\nWHERE key = \'my_key\' AND expired_at > NOW();\\n```\\n\\n### The DEL Operation\\n\\n:::note Concept: Hard vs. Soft Deletes\\n* **Hard Delete:** Physically removes the row from the storage. This causes immediate I/O overhead as the B-Tree must rebalance and pages might need to be merged.\\n* **Soft Delete:** Updates a flag (e.g., `is_deleted=true`) or a timestamp. The data remains but is logically invisible. This minimizes immediate I/O fluctuation.\\n  :::\\n\\nTo implement `DEL` efficiently, we will perform a **Soft Delete** by expiring the key immediately. We update the `expired_at` to a past timestamp (or a sentinel value like -1) to indicate it was deleted by the user, not naturally expired.\\n*More details on hard deletes can be found [here](/avoid-hard-deletes.md).*\\n```sql\\nUPDATE store \\nSET expired_at = -1 \\nWHERE key = \'my_key\' AND expired_at > NOW();\\n```\\n\\n---\\n\\n## 4. The TTL Cleanup (Garbage Collection)\\n\\nWe need a background cron job to physically remove expired rows.\\n\\n**Naive Approach:**\\n```sql\\nDELETE FROM store WHERE expired_at < NOW();\\n```\\n\\n:::warning Performance Risk: Unbounded Deletes\\n**Never perform unbounded deletes in production.**\\nIf your table has 100 million rows and 10 million are expired:\\n1.  The DB tries to lock all 10 million rows.\\n2.  It fills up the Undo/Redo logs to support rollback.\\n3.  It spikes CPU and IO, potentially blocking live traffic.\\n    **Bounded Deletes are always superior.** Deleting 100 rows in a loop is infinitely better than crashing the DB trying to delete 10k at once.\\n    :::\\n\\n**Optimized Approach (Batching):**\\n\\n```sql\\nDELETE FROM store \\nWHERE expired_at < UNIX_TIMESTAMP() \\nLIMIT 1000;\\n```\\nThe cron job should run this query in a loop until the affected row count is 0.\\n\\n---\\n\\n## 5. Scaling the System\\n\\nWe have the logic. Now, let\'s handle the growth.\\n\\n### Scenario 1: High Traffic (Read/Write)\\nIf the number of requests spikes, the first bottleneck is usually the CPU/Memory on the API servers.\\n\\n**Solution:** Horizontally scale the API layer.\\n\\n:::tip Recap: Storage/Compute Separation\\nBecause our API (Compute) is decoupled from the DB (Storage), we can spin up 50 new API instances without touching the Database configuration.\\n:::\\n\\n```mermaid\\ngraph LR\\n    LB[Load Balancer] --\x3e API1[API Server 1]\\n    LB --\x3e API2[API Server 2]\\n    LB --\x3e API3[API Server 3]\\n    API1 --\x3e DB[(Master DB)]\\n    API2 --\x3e DB\\n    API3 --\x3e DB\\n```\\n\\n### Scenario 2: Read Heavy Load\\nAs you grow, the single Master DB will struggle to handle all the `SELECT` queries.\\n\\n**Solution:** Add Read Replicas.\\n\\n```mermaid\\ngraph TD\\n    API[API Cluster] --\x3e Master[(DB Master)]\\n    API --\x3e ReadReplica1[(Read Replica 1)]\\n    API --\x3e ReadReplica2[(Read Replica 2)]\\n    \\n    Master -- Replication --\x3e ReadReplica1\\n    Master -- Replication --\x3e ReadReplica2\\n    \\n    style Master fill:#f96,stroke:#333\\n```\\n\\nWe route `PUT/DEL` to Master, and `GET` to Replicas.\\n\\n:::danger The Cost of Scale: Eventual Consistency\\nWhen you introduce Read Replicas, you introduce **Replication Lag**.\\n1.  Client A writes `Key=X` to Master.\\n2.  Client B reads `Key=X` from Replica 1 immediately.\\n3.  If the data hasn\'t copied over yet (lag), Client B gets a **Cache Miss** or stale data.\\n\\n**Real World Solution: DynamoDB**\\nAmazon DynamoDB solves this by giving the client a choice. You can perform a standard read (eventually consistent, cheaper) or a **Strongly Consistent Read**.\\nIf you request `ConsistentRead=True`, DynamoDB routes the request to the Leader node to guarantee the latest data, but it consumes 2x capacity units (costs more).\\n:::\\n\\n### Scenario 3: Write Heavy Load\\nEventually, your Master DB won\'t handle the incoming write volume.\\n\\n**Step 1: Vertical Scaling.** Upgrade the Master to a bigger machine (more RAM, better CPU).\\n**Step 2: Sharding.** When vertical scaling hits the limit (physics or cost), we shard.\\n\\nMore details on sharding can be found in my previous [post on sharding](/scaling.md#3-sharding).\\n\\n## Final Architecture\\n\\nHere is the final system diagram, fully scaled vertically and horizontally, featuring sharded masters and read replicas.\\n\\n```mermaid\\nflowchart TD\\n    Client --\x3e LB[\\"Load Balancer\\"]\\n    \\n    subgraph API_Layer [\\"Horizontally Scaled API\\"]\\n        API1[\\"API Server 1\\"]\\n        API2[\\"API Server ..\\"]\\n        API3[\\"API Server N\\"]\\n    end\\n\\n    LB --\x3e API1 & API2 & API3\\n    \\n    subgraph Shard_1 [\\"Shard 1: Keys A-M\\"]\\n        M1[(\\"Master 1\\")] -- Async Repl --\x3e R1a[(\\"Replica 1A\\")]\\n        M1 -- Async Repl --\x3e R1b[(\\"Replica 1B\\")]\\n    end\\n    \\n    subgraph Shard_2 [\\"Shard 2: Keys N-Z\\"]\\n        M2[(\\"Master 2\\")] -- Async Repl --\x3e R2a[(\\"Replica 2A\\")]\\n        M2 -- Async Repl --\x3e R2b[(\\"Replica 2B\\")]\\n    end\\n    \\n    %% Routing connections\\n    API1 & API2 -.->|\\"Writes A-M\\"| M1\\n    API1 & API2 -.->|\\"Writes N-Z\\"| M2\\n    \\n    %% Read connections (Simplified)\\n    API2 -.->|\\"Reads A-M\\"| R1a & R1b\\n    API2 -.->|\\"Reads N-Z\\"| R2a & R2b\\n\\n    style M1 fill:#f96,stroke:#333\\n    style M2 fill:#f96,stroke:#333\\n```"},{"id":"db-pessimistic-optimistic-locking","metadata":{"permalink":"/docs/blog/db-pessimistic-optimistic-locking","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/pessimistic-lock.md","source":"@site/blog/pessimistic-lock.md","title":"Deep Dive into Database Pessimistic & Optimistic Locking","description":"In distributed systems, managing access to shared resources is one of the most critical challenges we face. When multiple processes or threads attempt to modify the same data simultaneously, we risk race conditions that can corrupt the system\'s state.","date":"2025-12-27T00:00:00.000Z","tags":[],"readingTime":13.55,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Deep Dive into Database Pessimistic & Optimistic Locking","slug":"db-pessimistic-optimistic-locking","authors":"ashish","date":"2025-12-27T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Distributed Key-Value Store","permalink":"/docs/blog/distributed-key-value-store"},"nextItem":{"title":"Scaling Databases with Proxy Servers","permalink":"/docs/blog/proxy-server"}},"content":"In distributed systems, managing access to shared resources is one of the most critical challenges we face. When multiple processes or threads attempt to modify the same data simultaneously, we risk race conditions that can corrupt the system\'s state.\\n\\n## Pessimistic Locking\\nIt is a mechanism where we assume a conflict will occur and lock the data before operating on it. We will dive into MySQL\'s locking mechanisms, deadlock detection, and analyze a Python simulation to see these locks in action.\\n\\n## 1. What are Locks and Why Do We Need Them?\\n\\nAt its core, a database is the \\"Source of Truth\\" for an application. If this source becomes corrupted, the entire integrity of the system collapses.\\n\\nIn a concurrent environment (like a web server handling thousands of requests), multiple transactions often try to access and modify the same database row at the exact same millisecond. Without protection, we encounter the **Lost Update Problem**:\\n\\n1.  Transaction A reads a value (e.g., `seats_available = 1`).\\n2.  Transaction B reads the same value (`seats_available = 1`).\\n3.  Transaction A updates the value to 0.\\n4.  Transaction B *also* updates the value to 0.\\n\\nBoth transactions believe they successfully reserved the last seat, but the database is now in an inconsistent state.\\n\\n**Pessimistic Locking** solves this by preventing other transactions from modifying (and often reading) the data until the lock holder is finished. It enforces a strict \\"I was here first\\" policy, ensuring that the shared resource is mutually exclusive during the critical section of the update.\\n\\n## 2. The Downside: Deadlocks\\n\\nWhile locks protect data, they introduce a new danger: **Deadlocks**.\\n\\nA deadlock occurs when two or more transactions are waiting for one another to give up locks, resulting in a cycle where no one can proceed.\\n\\n### Deadlock Detection in MySQL (InnoDB)\\n\\nModern storage engines have sophisticated deadlock detection mechanisms. They maintain a **Wait-For Graph** (a directed graph) where nodes are transactions and edges represent one transaction waiting for a lock held by another.\\n\\n* If the graph contains a **cycle**, a deadlock exists.\\n* DB immediately detects this cycle.\\n* It resolves the deadlock by kills one of the transactions (usually the one that has done the least amount of work) to allow the other to proceed.\\n\\n**The Error:**\\nWhen MySQL kills a transaction to break a deadlock, it throws the following error:\\n> `ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction`\\n\\n### Visualizing the Deadlock Graph\\n\\n```mermaid\\ngraph TD\\n    subgraph Deadlock_Cycle\\n        T1[Transaction A] -- Waits for Lock on Row 2 --\x3e T2[Transaction B]\\n        T2 -- Waits for Lock on Row 1 --\x3e T1\\n    end\\n    \\n    style T1 fill:#ff9999,stroke:#333,stroke-width:2px\\n    style T2 fill:#ff9999,stroke:#333,stroke-width:2px\\n```\\n\\n## 3. Acquiring Locks in MySQL\\n\\nMySQL provides specific syntax to interact with row-level locking during a `SELECT` statement.\\n\\n### `FOR UPDATE`\\nThis is the standard pessimistic lock. It tells the database: *\\"I intend to update these rows. Lock them exclusively. If anyone else has them locked, I will wait until they are released.\\"*\\n\\n* **Behavior:** Blocks if the row is already locked.\\n* **Use Case:** Strict consistency where processing order matters or where you must update specific rows.\\n\\n### `NOWAIT`\\nThis tells the database: *\\"I want to lock these rows. If they are already locked, do not make me wait. Fail immediately.\\"*\\n\\n* **Behavior:** Throws an error immediately if the row is locked.\\n* **Use Case:** Real-time systems where failing fast is better than queuing.\\n\\n### `SKIP LOCKED`\\nThis is a powerful feature for high-concurrency queues. It tells the database: *\\"I want to lock rows that match my criteria. If some are already locked by others, just skip them and give me the next available ones.\\"*\\n\\n* **Behavior:** Does not wait; simply ignores locked rows and returns the free ones.\\n* **Use Case:** Ticket booking, job queues, task distribution.\\n\\n#### Locking Visualization\\n\\n```mermaid\\nsequenceDiagram\\n    participant T1 as Thread 1\\n    participant DB as Database\\n    participant T2 as Thread 2\\n\\n    Note over T1, T2: SCENARIO: FOR UPDATE\\n    T1->>DB: SELECT * FROM seats WHERE id=1 FOR UPDATE\\n    DB--\x3e>T1: Grants Lock (id=1)\\n    T2->>DB: SELECT * FROM seats WHERE id=1 FOR UPDATE\\n    Note right of T2: BLOCKED (Waiting...)\\n    T1->>DB: COMMIT (Releases Lock)\\n    DB--\x3e>T2: Grants Lock (id=1)\\n\\n    Note over T1, T2: SCENARIO: SKIP LOCKED\\n    T1->>DB: SELECT * FROM seats LIMIT 1 FOR UPDATE SKIP LOCKED\\n    DB--\x3e>T1: Returns Row 1 (Locks it)\\n    T2->>DB: SELECT * FROM seats LIMIT 1 FOR UPDATE SKIP LOCKED\\n    Note right of T2: Sees Row 1 locked... Skips to Row 2\\n    DB--\x3e>T2: Returns Row 2 (Locks it)\\n```\\n\\n## 4. Code Walkthrough & Scenario Analysis\\n\\nI created a Python script to simulate a high-concurrency seat reservation system. The script spawns multiple threads, where each thread attempts to:\\n1.  Start a transaction.\\n2.  Find an available seat (`user_id IS NULL`).\\n3.  Book it (`UPDATE seats SET user_id...`).\\n4.  Commit.\\n\\nLet\'s analyze the behavior under three different locking strategies.\\n\\n### Scenario 1: WITHOUT \\"FOR UPDATE\\"\\n\\n**Observation:** The number of reserved seats is **highly non-deterministic** and almost always less than the number of threads.\\n\\n**Why? (The Race Condition)**\\nThis is a classic \\"Check-Then-Act\\" race condition.\\n1.  **Thread Scheduling:** The OS schedules Thread A and Thread B to run almost simultaneously on different CPU cores.\\n2.  **Snapshot Isolation:** Thread A runs `SELECT` and sees Seat #1 is empty. Before Thread A can run the `UPDATE` statement, the OS context switches or simply executes Thread B in parallel.\\n3.  **Duplicate Reads:** Thread B runs the same `SELECT` and *also* sees Seat #1 is empty (because A hasn\'t committed yet).\\n4.  **The Overwrite:** Thread A updates Seat #1 and commits. Thread B updates Seat #1 (overwriting A\'s work) and commits.\\n5.  **Result:** Two threads think they booked a seat, but only one record exists in the DB. One reservation is effectively \\"lost.\\"\\n\\n```mermaid\\nsequenceDiagram\\n    participant TA as Thread A\\n    participant DB as Database\\n    participant TB as Thread B\\n    \\n    TA->>DB: SELECT id FROM seats WHERE user_id IS NULL (Returns ID: 5)\\n    TB->>DB: SELECT id FROM seats WHERE user_id IS NULL (Returns ID: 5)\\n    Note over TA, TB: Both threads see Seat 5 as empty!\\n    TA->>DB: UPDATE seats SET user_id=A WHERE id=5\\n    TB->>DB: UPDATE seats SET user_id=B WHERE id=5\\n    TA->>DB: COMMIT\\n    TB->>DB: COMMIT\\n    Note over DB: Result: Seat 5 belongs to B. A\'s work is lost.\\n```\\n```info\\n2025-12-26 17:38:55,400 - MainThread - INFO - SEAT RESERVATION SYSTEM - COMPLETED\\n2025-12-26 17:38:55,400 - MainThread - INFO - ================================================================================\\n2025-12-26 17:38:55,400 - MainThread - INFO - Threads requested: 32\\n2025-12-26 17:38:55,400 - MainThread - INFO - Seats created: 32\\n2025-12-26 17:38:55,400 - MainThread - INFO - Seats reserved: 4\\n2025-12-26 17:38:55,400 - MainThread - INFO - Execution time: 7 ms\\n2025-12-26 17:38:55,400 - MainThread - INFO - ================================================================================\\n```\\n\\n### Scenario 2: WITH \\"FOR UPDATE\\"\\n\\n**Observation:** The number of reserved seats **always equals** the total number of seats (up to the thread limit). Consistency is maintained.\\n\\n**Why? (Blocking & Re-evaluation)**\\nWhen Thread A runs `SELECT ... FOR UPDATE`, the database locks that specific row (Seat #1).\\n\\n1.  **The Block:** If Thread B tries to select Seat #1, the DB forces it to **wait**.\\n2.  **The Re-evaluation:** Once Thread A commits, it releases the lock. Thread B wakes up.\\n3.  **The Loop/Retry:** When Thread B acquires the lock, it sees the *latest* committed data. It realizes Seat #1 is now taken (user_id is NOT NULL), so it must restart the search or move to the next available row.\\n\\nThis serialization ensures that no two threads can ever \\"win\\" the same seat.\\n\\n```mermaid\\nsequenceDiagram\\n    participant TA as Thread A\\n    participant DB as Database\\n    participant TB as Thread B\\n    \\n    TA->>DB: SELECT ... FOR UPDATE (Locks Seat 5)\\n    TB->>DB: SELECT ... FOR UPDATE (Tries to Lock Seat 5)\\n    Note right of TB: BLOCKED by DB\\n    TA->>DB: UPDATE ...\\n    TA->>DB: COMMIT (Releases Lock)\\n    Note right of TB: Unblocked!\\n    TB->>DB: re-evaluates row... sees it\'s taken\\n    TB->>DB: SELECT ... next row ... FOR UPDATE\\n```\\n```info\\n2025-12-26 17:43:25,625 - MainThread - INFO - SEAT RESERVATION SYSTEM - COMPLETED\\n2025-12-26 17:43:25,625 - MainThread - INFO - ================================================================================\\n2025-12-26 17:43:25,625 - MainThread - INFO - Threads requested: 32\\n2025-12-26 17:43:25,625 - MainThread - INFO - Seats created: 32\\n2025-12-26 17:43:25,625 - MainThread - INFO - Seats reserved: 32\\n2025-12-26 17:43:25,625 - MainThread - INFO - Execution time: 25 ms\\n2025-12-26 17:43:25,625 - MainThread - INFO - ================================================================================\\n```\\n\\n### Scenario 3: WITH \\"FOR UPDATE SKIP LOCKED\\"\\n\\n**Observation:** The number of seats reserved is correct (consistent), but the **total execution time is significantly lower** than Scenario 2.\\n\\n**Why? (Concurrency without Contention)**\\nThis is the gold standard for job queues or booking systems.\\n\\n1.  **No Waiting:** Thread A grabs Seat #1. Thread B comes in, sees Seat #1 is locked, and *instantly* skips it to grab Seat #2.\\n2.  **Parallelism:** Because threads don\'t wait for each other, they can utilize the full connection pool simultaneously.\\n3.  **Result:** If you have 10 threads, you reserve 10 seats in roughly the time it takes to reserve 1 seat, rather than the time it takes to reserve 10 seats sequentially.\\n\\n```info\\n2025-12-26 17:47:55,625 - MainThread - INFO - SEAT RESERVATION SYSTEM - COMPLETED\\n2025-12-26 17:47:55,625 - MainThread - INFO - ================================================================================\\n2025-12-26 17:47:55,625 - MainThread - INFO - Threads requested: 32\\n2025-12-26 17:47:55,625 - MainThread - INFO - Seats created: 32\\n2025-12-26 17:47:55,625 - MainThread - INFO - Seats reserved: 32\\n2025-12-26 17:47:55,625 - MainThread - INFO - Execution time: 9.71 ms\\n2025-12-26 17:47:55,625 - MainThread - INFO - ================================================================================\\n```\\n```mermaid\\nsequenceDiagram\\n    participant TA as Thread A\\n    participant DB as Database\\n    participant TB as Thread B\\n    \\n    TA->>DB: SELECT ... LIMIT 1 FOR UPDATE SKIP LOCKED\\n    Note right of TA: DB locks Row 1, returns it to TA\\n    \\n    TB->>DB: SELECT ... LIMIT 1 FOR UPDATE SKIP LOCKED\\n    Note right of TB: DB sees Row 1 locked...\\n    Note right of TB: DB skips Row 1...\\n    Note right of TB: DB locks Row 2, returns it to TB\\n    \\n    TA->>DB: UPDATE Row 1 ... COMMIT\\n    TB->>DB: UPDATE Row 2 ... COMMIT\\n    Note over TA, TB: Both executed in parallel!\\n```\\n\\n## Code Reference\\n\\nHere is the Python script used for this simulation:\\n\\n```python\\ndef reserve_seat(thread_id, user_id):\\n    conn = None\\n    try:\\n        # Get a connection from the pool\\n        conn = connection_pool.get_connection()\\n        cursor = conn.cursor()\\n\\n        logger.info(f\\"Thread {thread_id} (User {user_id}): Starting seat reservation\\")\\n\\n        # Start transaction\\n        conn.start_transaction()\\n        logger.debug(f\\"Thread {thread_id}: Transaction started\\")\\n\\n        # Select an unreserved seat (user_id is NULL) with FOR UPDATE to lock the row\\n        # NOTE: Change \'FOR UPDATE SKIP LOCKED\' to \'FOR UPDATE\' or remove it to test different scenarios\\n        select_query = \\"SELECT id, seat_number FROM seats WHERE user_id IS NULL AND trip = %s LIMIT 1 FOR UPDATE SKIP LOCKED\\"\\n        cursor.execute(select_query, (\'TRIP_001\',))\\n        seat = cursor.fetchone()\\n\\n        if seat:\\n            seat_id, seat_number = seat\\n            logger.info(f\\"Thread {thread_id} (User {user_id}): Found available seat #{seat_number}\\")\\n\\n            # Update the seat with user_id\\n            update_query = \\"UPDATE seats SET user_id = %s WHERE id = %s\\"\\n            cursor.execute(update_query, (user_id, seat_id))\\n\\n            # Commit the transaction\\n            conn.commit()\\n            logger.info(f\\"Thread {thread_id} (User {user_id}): Successfully reserved seat #{seat_number}\\")\\n        else:\\n            conn.commit()\\n            logger.warning(f\\"Thread {thread_id} (User {user_id}): No available seats found\\")\\n\\n        cursor.close()\\n    except Error as e:\\n        logger.error(f\\"Thread {thread_id} (User {user_id}): Error during seat reservation: {e}\\")\\n        if conn and conn.is_connected():\\n            try:\\n                conn.rollback()\\n                logger.info(f\\"Thread {thread_id}: Transaction rolled back\\")\\n            except Error as rollback_error:\\n                logger.error(f\\"Thread {thread_id}: Error during rollback: {rollback_error}\\")\\n    finally:\\n        if conn and conn.is_connected():\\n            conn.close()\\n\\n# ... (Main execution logic)\\n```\\n## Optmimistic Locking: The Non-Blocking Alternative\\n\\n**Optimistic Locking** offers an alternative strategy. Instead of locking the door before you enter, you assume the door is unlocked, do your work, and check if anyone else locked it only when you are leaving.\\n\\nOptimistic locking is a strategy where you read a record, take note of a version number, and check that version number again only when you write the changes back.\\n\\n### Optimistic vs. Pessimistic\\n* **Pessimistic Locking:** Assumes the worst. \\"Something bad will happen, so I will lock this row as soon as I select it (`SELECT ... FOR UPDATE`). No one else can touch it until I\'m done.\\"\\n* **Optimistic Locking:** Assumes the best. \\"It\'s unlikely anyone else will modify this row while I\'m working on it. I won\'t lock anything during the read. I will only check for conflicts when I attempt to update.\\"\\n\\n### The Implementation: The `version_number`\\nThe standard way to implement optimistic locking is by adding a column to your table, usually named `version` or `revision`.\\n\\n1.  **Read:** When you read a row, you also read the current `version` (e.g., 1).\\n2.  **Modify:** You perform your logic in the application layer.\\n3.  **Update:** When you send the update query, you add a `WHERE` clause checking if the `version` is still 1. You also increment the version.\\n\\n```sql\\n-- Step 1: Read the data\\nSELECT id, content, version FROM wiki_pages WHERE id = 101;\\n-- Result: version = 5\\n\\n-- ... Application logic happens here ...\\n\\n-- Step 2: Update with guard clause\\nUPDATE wiki_pages\\nSET \\n    content = \'New Content\', \\n    version = version + 1 \\nWHERE \\n    id = 101 \\n    AND version = 5; -- Crucial Check!\\n```\\n\\n**The Outcome:**\\n* If the database returns **1 row affected**, the update succeeded.\\n* If the database returns **0 rows affected**, it means someone else changed the `version` to 6 (or higher) while you were working. Your transaction fails, and you must handle the conflict (usually by retrying).\\n\\n## When to Use: The Case for Non-Contention\\n\\nOptimistic locking shines in scenarios of **Low Contention**.\\n\\n### High Contention vs. Low Contention\\n* **High Contention (Use Pessimistic):** Imagine a limited drop of 10 concert tickets with 10,000 users trying to buy them instantly. If you use optimistic locking here, 1 person succeeds, and 9,999 users fail and have to retry. Those retries hammer the database. Pessimistic locking is better here to queue the requests.\\n* **Low Contention (Use Optimistic):** Imagine a user updating their profile bio or editing a specific wiki page. The chances of two admins editing the exact same paragraph at the exact same second are very low.\\n\\n## The Benefits: Zero Locking Overhead\\n\\nThe primary benefit of optimistic locking is that it is **lock-free during the \\"think time\\"**.\\n\\nIn a Pessimistic setup, the database holds a lock from the moment you run `SELECT ... FOR UPDATE` until you `COMMIT`. If your application server is slow, or if the user is taking time to fill out a form, that database row is frozen. No one can read it.\\n\\nWith Optimistic locking:\\n1.  **High Concurrency:** Threads or users are never blocked during the read phase.\\n2.  **Scalability:** Because there are no long-held locks, the database doesn\'t have to maintain a heavy lock graph or check for deadlocks.\\n3.  **No Deadlocks:** Since you aren\'t holding resources while waiting for others, the classic deadlock scenarios (A waits for B, B waits for A) are virtually eliminated in the application logic.\\n\\n## The Downsides: Latency and Retries\\n\\nWhile \\"lock-free\\" sounds perfect, Optimistic locking introduces its own penalties, particularly when conflicts *do* occur.\\n\\n### A) The \\"Read-Modify-Write\\" Latency\\nOptimistic locking strictly requires a round-trip pattern:\\n1.  Network Call 1: Fetch data (Read version).\\n2.  App Logic: Compute.\\n3.  Network Call 2: Update data.\\n\\nYou cannot simply fire a \\"blind write\\" to the database. You must know the state before you change it. This adds network latency compared to simple \\"fire and forget\\" updates.\\n\\n### B) The Cost of Retries\\nThis is the most significant hidden cost. When an optimistic lock fails (returns 0 rows updated), the application cannot simply crash; it must **Retry**.\\n\\nA retry loop looks like this:\\n1.  Read Data (Latency).\\n2.  Compute (CPU).\\n3.  Update Fails.\\n4.  **Repeat Step 1.**\\n\\nIf you have a system with moderate contention, you might do this loop 3 or 4 times before succeeding. This wastes:\\n* **DB IO:** Reading the same row multiple times.\\n* **App CPU:** Recalculating the logic multiple times.\\n* **Network Bandwidth:** Sending the same data back and forth.\\n\\n## 5. Conclusion: Why Non-Contention is Key\\n\\nGiven the penalties of retrying, **Optimistic Locking should only be used when you expect the update to succeed 95% to 99% of the time.**\\n\\nIf your system expects frequent collisions, the cost of retrying (thrashing) will outweigh the cost of simply queuing users with a Pessimistic Lock. However, for the vast majority of web applications (CMS, User Profiles, E-commerce carts, Comments), actual data collisions are rare, making Optimistic Locking the most performant and scalable choice."},{"id":"/proxy-server","metadata":{"permalink":"/docs/blog/proxy-server","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/proxy-server.md","source":"@site/blog/proxy-server.md","title":"Scaling Databases with Proxy Servers","description":"In the complex landscape of distributed systems, direct communication between clients and backend resources is often inefficient or insecure. Enter the Proxy Server\u2014an intermediary that sits between the end-user clients and the resources they intend to browse or access.","date":"2025-12-25T00:00:00.000Z","tags":[],"readingTime":10.18,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"id":"proxysql-deep-dive","title":"Scaling Databases with Proxy Servers","sidebar_label":"Database Proxy Servers","authors":"ashish","date":"2025-12-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Deep Dive into Database Pessimistic & Optimistic Locking","permalink":"/docs/blog/db-pessimistic-optimistic-locking"},"nextItem":{"title":"Real-Time Communication with Server-Sent Events (SSE)","permalink":"/docs/blog/server-sent-events-explained"}},"content":"In the complex landscape of distributed systems, direct communication between clients and backend resources is often inefficient or insecure. Enter the **Proxy Server**\u2014an intermediary that sits between the end-user clients and the resources they intend to browse or access.\\n\\nIn the context of database architecture, a database proxy abstracts the underlying database topology from the application layer. Instead of applications connecting directly to a specific database instance (like a primary or a replica), they connect to the proxy, which then intelligently routes the query to the appropriate destination.\\n\\nPopular examples in the relational database world include **PgBouncer** for PostgreSQL, which excels at connection pooling, and **ProxySQL** or **MaxScale** for MySQL, which provide advanced features like query routing, caching, and failover management.\\n\\n## Benefits of Using a Proxy Server\\n\\nImplementing a proxy server in your database architecture offers several robust advantages:\\n\\n* **Connection Pooling and Multiplexing**\\n  Opening and closing database connections is an expensive operation. Proxies maintain a pool of open connections to the backend databases and reuse them for multiple client requests (multiplexing). This drastically reduces the connection overhead on the database server, allowing it to dedicate more resources to executing queries rather than managing network handshakes.\\n\\n* **Load Balancing and Read/Write Splitting**\\n  Proxies can distribute incoming traffic across multiple database nodes. By identifying the type of query (e.g., specific `SELECT` statements versus `INSERT`/`UPDATE`s), the proxy can automatically route write operations to the primary node and read operations to read replicas. This ensures optimal resource utilization and prevents a single node from becoming a bottleneck.\\n\\n* **High Availability and Failover**\\n  A proxy creates an abstraction layer that insulates the application from database failures. If a database node goes down, the proxy detects the failure and immediately stops sending traffic to that node, rerouting it to healthy instances. This failover happens transparently to the application, which continues to communicate with the proxy without realizing the backend topology has changed.\\n\\n* **Query Caching**\\n  Some advanced proxies can cache the results of frequently executed queries in their own memory. If a client requests data that is already cached, the proxy serves the result instantly without ever hitting the backend database. This significantly reduces latency for the application and lowers the load on the database server.\\n\\n## Deep Dive: ProxySQL for MySQL\\n\\nProxySQL is a high-performance, open-source proxy for MySQL (and forks like Percona Server and MariaDB). It is uniquely designed to solve the challenges of high-traffic, highly available database clusters. Unlike simple connection poolers, ProxySQL understands the SQL protocol, allowing it to inspect queries and make intelligent routing decisions based on the actual SQL content, user, or schema.\\n\\n### The Scenario: Elastic Scaling for Burst Traffic\\n\\nLet\'s weave our exploration around a specific, common business scenario:\\n\\n> **The Use Case:** Imagine an e-commerce platform that experiences a massive surge in traffic only during \\"Flash Sales\\" (e.g., 6 PM to 8 PM). During these hours, read traffic spikes by 10x.\\n>\\n> **The Strategy:** To handle this cost-effectively, we want an **elastic database topology**. We will spin up additional Read Replicas specifically for the flash sale window and tear them down afterward.\\n>\\n> **The Challenge:** We need to add and remove these database nodes dynamically *without* changing application code, *without* redeploying the app, and *without* any downtime.\\n\\n## Using MySQL to Distribute and Divert Traffic\\n\\nProxySQL manages backend servers using **Hostgroups**. A hostgroup is simply a logical grouping of database servers (e.g., Hostgroup 10 for Writers, Hostgroup 20 for Readers).\\n\\nLet\'s walk through the configuration steps to achieve our elastic routing.\\n\\n### a) Configuring Backend Servers\\n\\nFirst, we define our backend servers in the `mysql_servers` table.\\n\\n* **Hostgroup 10:** The Primary Writer (High consistency).\\n* **Hostgroup 20:** The Permanent Readers.\\n* **Hostgroup 30:** The \\"Flash Sale\\" Elastic Readers (only active during surges).\\n\\n```sql\\n-- Connect to ProxySQL Admin interface (usually port 6032)\\n-- Insert the Writer (Primary)\\nINSERT INTO mysql_servers (hostgroup_id, hostname, port) \\nVALUES (10, \'db-primary\', 3306);\\n\\n-- Insert the Permanent Reader\\nINSERT INTO mysql_servers (hostgroup_id, hostname, port) \\nVALUES (20, \'db-reader-permanent\', 3306);\\n\\n-- Insert the Elastic Readers (initially can be offline or added dynamically)\\nINSERT INTO mysql_servers (hostgroup_id, hostname, port) \\nVALUES (30, \'db-reader-elastic-1\', 3306);\\n```\\n\\n### b) Significance of LOAD MYSQL SERVERS\\n\\nWhen you run the `INSERT` commands above, you are modifying the configuration in **Memory** (specifically, the `main` database in memory). These changes are not yet effective for live traffic.\\n\\nTo apply these changes to the runtime engine (the part of ProxySQL actually handling traffic), you must run:\\n\\n```sql\\nLOAD MYSQL SERVERS TO RUNTIME;\\n```\\n\\nThis command acts as a \\"commit\\" to the running process. It tells ProxySQL to start using the new server list immediately without restarting the service.\\n\\n### c) Significance of SAVE MYSQL SERVERS\\n\\nWhile `LOAD` pushes changes to runtime, it does not persist them to disk. If ProxySQL restarts, all changes in memory are lost. To ensure your configuration survives a reboot, you must save them to the internal SQLite database file on disk:\\n\\n```sql\\nSAVE MYSQL SERVERS TO DISK;\\n```\\n\\n### d) Adding More Replicas (Scaling Out)\\n\\nWhen the \\"Flash Sale\\" begins, your orchestration tool (like Terraform or Kubernetes) provisions new DB replicas. To register them with ProxySQL:\\n\\n```sql\\n-- Adding new elastic nodes for the surge\\nINSERT INTO mysql_servers (hostgroup_id, hostname, port) \\nVALUES (30, \'db-reader-elastic-2\', 3306);\\n\\nINSERT INTO mysql_servers (hostgroup_id, hostname, port) \\nVALUES (30, \'db-reader-elastic-3\', 3306);\\n\\n-- Apply changes instantly\\nLOAD MYSQL SERVERS TO RUNTIME;\\nSAVE MYSQL SERVERS TO DISK;\\n```\\n\\n*The application knows nothing about these new IPs; it just continues talking to ProxySQL.*\\n\\n### e) Significance of mysql_query_rules\\n\\nThe real magic happens in the `mysql_query_rules` table. This acts as a firewall and router combined. You define rules that match specific criteria (regex on the query, username, schema name, etc.) and assign an action, such as assigning a `destination_hostgroup`.\\n\\nProxySQL evaluates rules in order of their `rule_id`.\\n\\n### f) Routing Based on Query Type (Read/Write Split)\\n\\nA classic rule is to send all `SELECT` queries to the reader groups (Hostgroup 20 and 30) and everything else to the writer (Hostgroup 10).\\n\\n```sql\\n-- Rule: Direct all SELECT statements to Hostgroup 20 (Readers)\\nINSERT INTO mysql_query_rules (rule_id, active, match_digest, destination_hostgroup, apply)\\nVALUES (\\n    100,                 -- rule_id\\n    1,                   -- active\\n    \'^SELECT.*\',         -- match_digest (Regex for queries starting with SELECT)\\n    20,                  -- destination_hostgroup (Permanent Readers)\\n    1                    -- apply (Stop processing further rules if this matches)\\n);\\n\\nLOAD MYSQL QUERY RULES TO RUNTIME;\\nSAVE MYSQL QUERY RULES TO DISK;\\n```\\n\\n*Note: In production, you might set up a \\"Replication Lag\\" check so ProxySQL doesn\'t send queries to readers that are too far behind the writer.*\\n\\n### g) Routing Based on Schema (Analytics vs. Transactions)\\n\\nSuppose you have a specific schema `analytics_db` that runs heavy aggregation queries. You don\'t want these blocking your transactional users. You can route traffic hitting this specific schema to the elastic hostgroup (Hostgroup 30).\\n\\n```sql\\n-- Rule: Direct traffic for schema \'analytics_db\' to Hostgroup 30\\nINSERT INTO mysql_query_rules (rule_id, active, schemaname, destination_hostgroup, apply)\\nVALUES (\\n    50,                  -- Lower ID = Higher Priority than the generic SELECT rule\\n    1, \\n    \'analytics_db\',      -- Exact match on schema name\\n    30,                  -- Elastic/Analytics Readers\\n    1\\n);\\n\\nLOAD MYSQL QUERY RULES TO RUNTIME;\\nSAVE MYSQL QUERY RULES TO DISK;\\n```\\n\\n### h) Comment/Hint Based Routing\\n\\nSometimes regex isn\'t enough. You might have a critical `SELECT` statement (e.g., checking inventory before checkout) that *must* go to the Writer (Hostgroup 10) to ensure strong consistency, bypassing the replica entirely.\\n\\nDevelopers can inject a SQL comment (hint) into the query, and ProxySQL can route based on that comment.\\n\\n**Example Query from Application:**\\n\\n```sql\\n/* forceful_write */ SELECT quantity FROM products WHERE id = 101;\\n```\\n\\n**ProxySQL Rule:**\\n\\n```sql\\n-- Rule: If query contains \\"forceful_write\\", send to Writer (HG 10)\\nINSERT INTO mysql_query_rules (rule_id, active, match_digest, destination_hostgroup, apply)\\nVALUES (\\n    10,                  -- Highest Priority\\n    1,\\n    \'forceful_write\',    -- Regex matching the comment\\n    10,                  -- Writer Hostgroup\\n    1\\n);\\n\\nLOAD MYSQL QUERY RULES TO RUNTIME;\\nSAVE MYSQL QUERY RULES TO DISK;\\n```\\n\\nThis gives developers granular control over routing without hardcoding IP addresses in the application.\\n\\n## Deployment of ProxySQL\\n\\n### 1. Fleet of ProxySQL Fronted by a Load Balancer\\n\\nIn this topology, you run multiple independent ProxySQL instances behind a classic Load Balancer (like AWS NLB or HAProxy).\\n\\n```mermaid\\nflowchart TD\\n    Client[Application] --\x3e LB(Load Balancer)\\n    LB --\x3e Proxy1[ProxySQL A]\\n    LB --\x3e Proxy2[ProxySQL B]\\n    LB --\x3e Proxy3[ProxySQL C]\\n    \\n    Proxy1 --\x3e DB[(Databases)]\\n    Proxy2 --\x3e DB\\n    Proxy3 --\x3e DB\\n```\\n\\n* **Advantages:**\\n    * **Simple to understand:** Each ProxySQL node is independent.\\n    * **High Availability:** If one ProxySQL node dies, the Load Balancer removes it.\\n* **Disadvantages:**\\n    * **Configuration Drift:** If you add a new query rule to `ProxySQL A`, you must manually apply the exact same SQL commands to `ProxySQL B` and `C`. Automation tools (Ansible/Chef) are required to keep them in sync.\\n    * **Split Brain Risks:** If configurations diverge, users might get different results depending on which proxy the Load Balancer hits.\\n\\n### 2. ProxySQL Cluster Mode (Native Clustering)\\n\\nProxySQL supports native clustering. You configure the instances to form a cluster, and they automatically synchronize their configuration.\\n\\n```mermaid\\nflowchart TD\\n    Client[Application] --\x3e LB(Load Balancer)\\n    \\n    subgraph ProxySQL_Cluster [ProxySQL Native Cluster]\\n        Proxy1[ProxySQL Leader]\\n        Proxy2[ProxySQL Follower]\\n        Proxy3[ProxySQL Follower]\\n        \\n        Proxy1 <--\x3e|Sync Config| Proxy2\\n        Proxy2 <--\x3e|Sync Config| Proxy3\\n        Proxy3 <--\x3e|Sync Config| Proxy1\\n    end\\n    \\n    ProxySQL_Cluster --\x3e DB[(Databases)]\\n```\\n\\n* **Advantages:**\\n    * **Auto-Synchronization:** You only need to run the `INSERT` and `LOAD` commands on *one* node. The changes are automatically propagated to all other nodes in the cluster.\\n    * **Management Ease:** Drastically reduces the operational overhead of managing a large fleet.\\n* **Disadvantages:**\\n    * **Complexity:** Setting up the internal cluster network requires careful configuration.\\n    * **Network Overhead:** There is a small amount of traffic between proxies for synchronization.\\n\\n## Elastic Topology Visualization\\n\\nHere is how our Elastic/Flash Sale setup looks. The application connects to ProxySQL, which handles the dynamic routing to the elastic fleet.\\n\\n```mermaid\\nflowchart TD\\n    subgraph APP_LAYER [Application Layer]\\n        App1[App Instance 1]\\n        App2[App Instance 2]\\n    end\\n\\n    subgraph PROXY_LAYER [ProxySQL Layer]\\n        P1[ProxySQL]\\n    end\\n\\n    subgraph DB_LAYER [Database Layer]\\n        Writer[(Primary DB \\\\n HG: 10)]\\n        ReaderPerm[(Reader Permanent \\\\n HG: 20)]\\n        ReaderElastic1[(Reader Elastic 1 \\\\n HG: 30 \\\\n *Flash Sale*)]\\n        ReaderElastic2[(Reader Elastic 2 \\\\n HG: 30 \\\\n *Flash Sale*)]\\n    end\\n\\n    App1 --\x3e P1\\n    App2 --\x3e P1\\n\\n    P1 -- \\"INSERT / UPDATE\\" --\x3e Writer\\n    P1 -- \\"SELECT (Standard)\\" --\x3e ReaderPerm\\n    P1 -- \\"SELECT (Analytics/High Load)\\" --\x3e ReaderElastic1\\n    P1 -- \\"SELECT (Analytics/High Load)\\" --\x3e ReaderElastic2\\n```\\n## AWS Aurora Alternative: Custom Endpoints\\n\\nIf you are hosted specifically on AWS Aurora, you might not always need an external layer like ProxySQL for basic traffic segregation. Aurora offers a native feature called **Custom Endpoints**.\\n\\nIn the AWS Aurora ecosystem, **Custom Endpoints** offer a native alternative to external proxies for routing traffic to specific groups of database instances. Unlike the standard Cluster Reader Endpoint which cycles through *all* available replicas, a Custom Endpoint allows you to arbitrarily group specific instances\u2014such as high-performance nodes\u2014into a distinct VIP. When an application connects to this custom DNS name, Aurora performs DNS-based load balancing to distribute connections across only the pre-selected instances in that group. This feature is ideal for isolating workloads; for example, you can direct heavy analytical reports to a specific set of larger instances while keeping fast transactional reads on smaller instances for the web application. Because it operates at the DNS level, it provides instance-level isolation but lacks the granular query-level routing rules (regex parsing) found in tools like ProxySQL. It simplifies infrastructure management by removing the need for an intermediate proxy layer when simple instance grouping satisfies the routing requirements.\\n\\n### Visualizing Aurora Custom Endpoints\\n\\nThe following diagram illustrates how a Custom Endpoint acts as a specific entry point, load balancing traffic across a dedicated subset of readers, separate from the general traffic.\\n\\n```mermaid\\nflowchart LR\\n    subgraph Clients\\n        App[\\"Web Application\\"]\\n        Analytics[\\"Analytics Service\\"]\\n    end\\n\\n    subgraph Aurora_Cluster [\\"AWS Aurora Cluster\\"]\\n        \\n        subgraph Endpoints\\n            DefaultEP(\\"Default Reader Endpoint\\")\\n            CustomEP(\\"Custom Endpoint *Analytics*\\")\\n        end\\n        \\n        subgraph Reader_Nodes [\\"Read Replicas\\"]\\n            R1[\\"Reader 1 (General)\\"]\\n            R2[\\"Reader 2 (General)\\"]\\n            R3[\\"Reader 3 (Analytics Specific)\\"]\\n            R4[\\"Reader 4 (Analytics Specific)\\"]\\n        end\\n\\n        %% Routing Logic\\n        DefaultEP -.->|Load Balance| R1\\n        DefaultEP -.->|Load Balance| R2\\n        \\n        CustomEP -.->|Load Balance| R3\\n        CustomEP -.->|Load Balance| R4\\n    end\\n\\n    App --\x3e|Connects to| DefaultEP\\n    Analytics --\x3e|Connects to| CustomEP\\n```\\n\\n### Conclusion\\n\\nBy leveraging ProxySQL, we effectively decoupled our application from the database infrastructure. We achieved the ability to \\"burst\\" our read capacity by adding elastic nodes (Hostgroup 30) and routing heavy traffic to them via `mysql_query_rules`. All of this happened transparently to the application, ensuring high availability and cost efficiency during peak business hours."},{"id":"server-sent-events-explained","metadata":{"permalink":"/docs/blog/server-sent-events-explained","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/server-sent-event.md","source":"@site/blog/server-sent-event.md","title":"Real-Time Communication with Server-Sent Events (SSE)","description":"A deep dive into Server-Sent Events, their use cases, protocol details, and a practical Python implementation.","date":"2025-12-25T00:00:00.000Z","tags":[],"readingTime":9.69,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Real-Time Communication with Server-Sent Events (SSE)","description":"A deep dive into Server-Sent Events, their use cases, protocol details, and a practical Python implementation.","slug":"server-sent-events-explained","authors":"ashish","date":"2025-12-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Scaling Databases with Proxy Servers","permalink":"/docs/blog/proxy-server"},"nextItem":{"title":"Implementing Shard Aware Application","permalink":"/docs/blog/application-level-sharding-design"}},"content":"In the world of web development, the classic request-response model serves us well for most interactions. The client asks for data, and the server provides it. But what happens when the server has new data *after* the initial request? How do we push updates to the client in real-time?\\n\\nWhile WebSockets are a popular choice for bi-directional communication, they can be overkill for many scenarios. Enter **Server-Sent Events (SSE)**\u2014a standard that allows servers to push data to web pages over a single HTTP connection.\\n\\nIn this post, we will explore what SSE is, typical use cases, the underlying protocol, and a practical implementation using Python and Flask.\\n\\n## 1. What is Server-Sent Events (SSE)?\\n\\nServer-Sent Events (SSE) is a technology that enables a client (usually a browser) to receive automatic updates from a server via an HTTP connection. It creates a unidirectional channel: the client opens the connection, and the server keeps it open, pushing text-based messages whenever new data is available.\\n\\nThink of it like subscribing to a newsletter. You sign up once (the HTTP request), and the publisher sends you updates continuously as they happen, without you needing to ask for them again.\\n\\n**Key Characteristics:**\\n* **Unidirectional:** Server -> Client only.\\n* **Standard HTTP:** Works over standard HTTP/HTTPS.\\n* **Text-Based:** Sends data as UTF-8 text.\\n* **Automatic Reconnection:** Browsers automatically attempt to reconnect if the connection drops.\\n\\n## 2. Typical Use Cases\\n\\nSince SSE is unidirectional, it is perfect for scenarios where the client needs to be updated, but doesn\'t need to send high-frequency data back to the server.\\n\\n* **Live Log Streaming:** Viewing server logs in real-time (as we will see in our example code).\\n* **Stock Tickers & Crypto Prices:** Updating dashboards with the latest financial data.\\n* **Progress Indicators:** Showing the progress of long-running tasks (e.g., file uploads, video processing).\\n* **Sports Scores:** Live updates during a match.\\n\\n## 3. The Protocol of Server-Sent Events\\n\\nSSE is deceptively simple. It sits on top of HTTP. Here is how the handshake and data transfer work:\\n\\n1.  **The Request:** The client sends a standard GET request.\\n2.  **The Headers:** The server responds with specific headers to keep the connection open:\\n    * `Content-Type: text/event-stream` (Crucial: tells the client to expect a stream).\\n    * `Cache-Control: no-cache` (Prevents the browser from caching the stream).\\n    * `Connection: keep-alive` (Keeps the TCP connection open).\\n3.  **The Payload:** The server sends data blocks separated by a pair of newlines (`\\\\n\\\\n`).\\n\\n### The Message Format\\nEach message in the stream is a block of text. The most common field is `data`.\\n\\n```text\\ndata: This is a message\\n\\ndata: This is the second message\\n\\n```\\n\\n*Note: In the raw protocol, every message must end with two newlines (`\\\\n\\\\n`) to signal the end of the event.*\\n\\nYou can also include event types and IDs:\\n\\n```text\\nid: 101\\nevent: update\\ndata: {\\"status\\": \\"processing\\", \\"progress\\": 50}\\n\\n```\\n\\n### Visualizing the Flow\\n\\nHere is a sequence diagram showing how the Client establishes the connection and receives a stream of log data.\\n\\n```mermaid\\nsequenceDiagram\\n    participant Client\\n    participant Server\\n\\n    Note over Client: Step 1: Client initiates connection\\n    Client->>Server: GET /api/logs/{id}/stream\\n    \\n    Note over Server: Step 2: Server accepts & keeps connection open\\n    Server--\x3e>Client: 200 OK\\n    Server--\x3e>Client: Content-Type: text/event-stream\\n    \\n    loop Stream Data\\n        Note over Server: Log file updated\\n        Server--\x3e>Client: data: [Log Line 1] (plus \\\\n\\\\n)\\n        \\n        Note over Server: Log file updated\\n        Server--\x3e>Client: data: [Log Line 2] (plus \\\\n\\\\n)\\n    end\\n    \\n    Note over Client, Server: Connection stays open indefinitely\\n```\\n\\n## 4. Implementation: Streaming Logs with Python\\n\\nLet\'s look at a practical implementation. Below is a Python script using `Flask`. It simulates a system where logs are being written to a file in the background, and a client listens to these logs in real-time via SSE.\\n\\n### Key Components Explained\\n\\nThere are two critical parts in the code that make SSE work:\\n\\n#### 1. The Generator Function (`read_log_stream`)\\nStandard HTTP requests return a block of data and close. To stream, Python uses **generators** (functions that `yield` instead of `return`).\\n\\n```python\\ndef read_log_stream(log_id: str):\\n    # ... setup code ...\\n    while True:\\n        # Check for new data\\n        if new_lines:\\n             for line in new_lines:\\n                 # Format specifically for SSE\\n                 yield f\\"data: {line.rstrip()}\\\\n\\\\n\\"\\n        else:\\n             # Wait briefly to prevent 100% CPU usage\\n             time.sleep(0.1)\\n```\\n\\nThis function keeps running. Every time it finds a new line in the log file, it pushes it out using `yield`.\\n\\n#### 2. The Flask Response (`stream_log`)\\nFlask needs to know this isn\'t a normal response. We pass the generator function to `Response` and set the headers.\\n\\n```python\\n@app.route(\\"/api/logs/<log_id>/stream\\", methods=[\\"GET\\"])\\ndef stream_log(log_id: str):\\n    return Response(\\n        read_log_stream(log_id),\\n        mimetype=\\"text/event-stream\\",  # Required for SSE\\n        headers={\\n            \\"Cache-Control\\": \\"no-cache\\", # Prevent buffering\\n            \\"Connection\\": \\"keep-alive\\"   # Keep connection open\\n        }\\n    )\\n```\\n\\n#### 3. The Client (`index` route)\\nOn the frontend (embedded in the `index` function), the `EventSource` API handles the heavy lifting.\\n\\n```javascript\\neventSource = new EventSource(`/api/logs/${logId}/stream`);\\n\\neventSource.onmessage = function(event) {\\n    // This runs every time the server sends a \\"data:\\" packet\\n    console.log(\\"New Log:\\", event.data);\\n};\\n```\\n![SSE Log Streaming](/img/sse-log-streaming.png)\\n\\nSSE is a powerful tool in your system design toolkit. While it doesn\'t replace WebSockets for complex bidirectional needs (like a chat app), it is often the simpler, more efficient choice for one-way updates.\\n\\n### The Code\\n\\n```python\\nimport time\\nimport threading\\nimport uuid\\nimport logging\\nfrom datetime import datetime\\nfrom pathlib import Path\\nfrom flask import Flask, request, Response\\nfrom faker import Faker\\n\\n# Configure logging\\nlogging.basicConfig(\\n    level=logging.DEBUG,\\n    format=\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'\\n)\\nlogger = logging.getLogger(__name__)\\n\\napp = Flask(__name__)\\nfake = Faker()\\n\\n# Directory to store log files\\nLOG_DIR = Path(\\"../logs\\")\\nLOG_DIR.mkdir(exist_ok=True)\\nlogger.info(f\\"Log directory set to: {LOG_DIR.absolute()}\\")\\n\\n# Track active log streams\\nactive_streams = {}\\n\\n\\ndef generate_random_text():\\n    \\"\\"\\"Generate random text using Faker\\"\\"\\"\\n    choices = [\\n        lambda: fake.sentence(),\\n        lambda: fake.paragraph(),\\n        lambda: fake.word(),\\n        lambda: f\\"[{datetime.now().isoformat()}] {fake.sentence()}\\",\\n    ]\\n    text = choices[id(time.time()) % len(choices)]()\\n    logger.debug(f\\"Generated random text: {text[:50]}...\\")\\n    return text\\n\\n\\ndef write_to_log(log_id: str, message: str):\\n    \\"\\"\\"Write a message to the log file\\"\\"\\"\\n    log_file = LOG_DIR / f\\"{log_id}.log\\"\\n    try:\\n        with open(log_file, \\"a\\") as f:\\n            f.write(message + \\"\\\\n\\")\\n        logger.debug(f\\"Wrote to log {log_id}: {message[:50]}...\\")\\n    except IOError as e:\\n        logger.error(f\\"Failed to write to log {log_id}: {e}\\")\\n\\n\\ndef simulate_log_writing(log_id: str, duration: int = 30):\\n    \\"\\"\\"Simulate writing to log file for a given duration\\"\\"\\"\\n    logger.info(f\\"Starting log writing thread for log_id={log_id}, duration={duration}s\\")\\n    end_time = time.time() + duration\\n    write_count = 0\\n\\n    while time.time() < end_time:\\n        message = generate_random_text()\\n        write_to_log(log_id, message)\\n        write_count += 1\\n        time.sleep(0.5)  # Write every 500ms\\n\\n    logger.info(f\\"Finished log writing for log_id={log_id}. Total lines written: {write_count}\\")\\n\\n\\ndef read_log_stream(log_id: str):\\n    \\"\\"\\"Generator that streams log content\\"\\"\\"\\n    log_file = LOG_DIR / f\\"{log_id}.log\\"\\n    logger.info(f\\"Starting log stream for log_id={log_id}\\")\\n\\n    if not log_file.exists():\\n        logger.warning(f\\"Log file not found for log_id={log_id}\\")\\n        yield f\\"data: Log file {log_id} not found\\\\n\\\\n\\"\\n        return\\n\\n    # Start reading from beginning\\n    position = 0\\n    line_count = 0\\n\\n    while True:\\n        try:\\n            with open(log_file, \\"r\\") as f:\\n                f.seek(position)\\n                lines = f.readlines()\\n\\n                if lines:\\n                    for line in lines:\\n                        # Protocol: Prefix with \\"data: \\" and end with double newline\\n                        yield f\\"data: {line.rstrip()}\\\\n\\\\n\\"\\n                        line_count += 1\\n                    position = f.tell()\\n                    logger.debug(f\\"Streamed {len(lines)} lines for log_id={log_id}, position={position}\\")\\n                else:\\n                    # No new data, check if log is still being written\\n                    # In a real app, you might have a mechanism to break if the process is dead\\n                    logger.debug(f\\"No new data for log_id={log_id}, waiting...\\")\\n                    time.sleep(0.1)\\n        except IOError as e:\\n            logger.error(f\\"Error reading log file for log_id={log_id}: {e}\\")\\n            yield f\\"data: Error reading log file\\\\n\\\\n\\"\\n            break\\n\\n    logger.info(f\\"Finished streaming log_id={log_id}. Total lines streamed: {line_count}\\")\\n\\n\\n@app.route(\\"/api/logs\\", methods=[\\"POST\\"])\\ndef create_log():\\n    \\"\\"\\"\\n    Create a new log file and start streaming random content to it.\\n    Returns the log ID for later reference.\\n    \\"\\"\\"\\n    log_id = str(uuid.uuid4())\\n    log_file = LOG_DIR / f\\"{log_id}.log\\"\\n\\n    logger.info(f\\"Creating new log with log_id={log_id}\\")\\n\\n    # Create empty log file\\n    log_file.touch()\\n    logger.debug(f\\"Log file created at: {log_file.absolute()}\\")\\n\\n    # Write initial message\\n    write_to_log(log_id, f\\"[LOG CREATED] {datetime.now().isoformat()}\\")\\n\\n    # Start background thread to write random content\\n    duration = request.args.get(\\"duration\\", 30, type=int)\\n    logger.info(f\\"Starting background thread for log_id={log_id} with duration={duration}s\\")\\n\\n    thread = threading.Thread(\\n        target=simulate_log_writing,\\n        args=(log_id, duration),\\n        daemon=True\\n    )\\n    thread.start()\\n    logger.debug(f\\"Thread started for log_id={log_id}\\")\\n\\n    return {\\n        \\"log_id\\": log_id,\\n        \\"message\\": f\\"Log file created with ID: {log_id}\\",\\n        \\"stream_url\\": f\\"/api/logs/{log_id}/stream\\"\\n    }, 201\\n\\n\\n@app.route(\\"/api/logs/<log_id>/stream\\", methods=[\\"GET\\"])\\ndef stream_log(log_id: str):\\n    \\"\\"\\"\\n    Stream the log file content using Server-Sent Events.\\n    Reads from the beginning and continuously streams new content.\\n    \\"\\"\\"\\n    logger.info(f\\"Stream request received for log_id={log_id}\\")\\n    log_file = LOG_DIR / f\\"{log_id}.log\\"\\n\\n    if not log_file.exists():\\n        logger.warning(f\\"Stream request for non-existent log_id={log_id}\\")\\n        def error_stream():\\n            yield f\\"data: Log file {log_id} not found\\\\n\\\\n\\"\\n        return Response(error_stream(), mimetype=\\"text/event-stream\\")\\n\\n    logger.debug(f\\"Starting SSE stream for log_id={log_id}\\")\\n    return Response(\\n        read_log_stream(log_id),\\n        mimetype=\\"text/event-stream\\",\\n        headers={\\n            \\"Cache-Control\\": \\"no-cache\\",\\n            \\"X-Accel-Buffering\\": \\"no\\",\\n            \\"Connection\\": \\"keep-alive\\"\\n        }\\n    )\\n\\n\\n@app.route(\\"/api/logs/<log_id>\\", methods=[\\"GET\\"])\\ndef get_log_info(log_id: str):\\n    \\"\\"\\"Get information about a specific log file\\"\\"\\"\\n    logger.info(f\\"Info request received for log_id={log_id}\\")\\n    log_file = LOG_DIR / f\\"{log_id}.log\\"\\n\\n    if not log_file.exists():\\n        logger.warning(f\\"Info request for non-existent log_id={log_id}\\")\\n        return {\\"error\\": f\\"Log file {log_id} not found\\"}, 404\\n\\n    file_size = log_file.stat().st_size\\n    created_at = datetime.fromtimestamp(log_file.stat().st_ctime).isoformat()\\n    logger.debug(f\\"Log info for log_id={log_id}: size={file_size}, created={created_at}\\")\\n\\n    return {\\n        \\"log_id\\": log_id,\\n        \\"file_size\\": file_size,\\n        \\"created_at\\": created_at,\\n        \\"stream_url\\": f\\"/api/logs/{log_id}/stream\\"\\n    }, 200\\n\\n\\n@app.route(\\"/\\", methods=[\\"GET\\"])\\ndef index():\\n    \\"\\"\\"Serve a simple HTML client for testing\\"\\"\\"\\n    return \\"\\"\\"\\n    <!DOCTYPE html>\\n    <html>\\n    <head>\\n        <title>Log Streaming PoC</title>\\n        <style>\\n            body { font-family: monospace; margin: 20px; }\\n            button { padding: 10px 20px; margin: 10px 0; cursor: pointer; }\\n            #log-container { \\n                border: 1px solid #ccc; \\n                padding: 10px; \\n                height: 400px; \\n                overflow-y: auto; \\n                background: #f5f5f5;\\n                margin: 10px 0;\\n            }\\n            .log-line { margin: 2px 0; }\\n            .status { padding: 10px; border-radius: 5px; margin: 10px 0; }\\n            .status.success { background: #d4edda; color: #155724; }\\n            .status.error { background: #f8d7da; color: #721c24; }\\n        </style>\\n    </head>\\n    <body>\\n        <h1>Log Streaming Proof of Concept</h1>\\n        \\n        <div>\\n            <button onclick=\\"createLog()\\">Create New Log</button>\\n            <input type=\\"number\\" id=\\"duration\\" placeholder=\\"Duration (seconds)\\" value=\\"30\\" min=\\"5\\" max=\\"300\\">\\n        </div>\\n        \\n        <div id=\\"status\\"></div>\\n        \\n        <div>\\n            Log ID: <input type=\\"text\\" id=\\"log-id\\" placeholder=\\"Enter log ID or create new\\">\\n            <button onclick=\\"startStream()\\">Start Streaming</button>\\n            <button onclick=\\"stopStream()\\">Stop Streaming</button>\\n        </div>\\n        \\n        <div id=\\"log-container\\"></div>\\n        \\n        <script>\\n            let eventSource = null;\\n            \\n            async function createLog() {\\n                const duration = document.getElementById(\'duration\').value;\\n                const statusDiv = document.getElementById(\'status\');\\n                \\n                try {\\n                    const response = await fetch(\'/api/logs?duration=\' + duration, { method: \'POST\' });\\n                    const data = await response.json();\\n                    \\n                    document.getElementById(\'log-id\').value = data.log_id;\\n                    statusDiv.innerHTML = `<div class=\\"status success\\">\u2713 Log created: ${data.log_id}</div>`;\\n                    \\n                    // Auto-start streaming\\n                    setTimeout(() => startStream(), 500);\\n                } catch (error) {\\n                    statusDiv.innerHTML = `<div class=\\"status error\\">\u2717 Error: ${error.message}</div>`;\\n                }\\n            }\\n            \\n            function startStream() {\\n                const logId = document.getElementById(\'log-id\').value;\\n                const statusDiv = document.getElementById(\'status\');\\n                \\n                if (!logId) {\\n                    statusDiv.innerHTML = \'<div class=\\"status error\\">\u2717 Please create or enter a log ID</div>\';\\n                    return;\\n                }\\n                \\n                if (eventSource) {\\n                    eventSource.close();\\n                }\\n                \\n                const logContainer = document.getElementById(\'log-container\');\\n                logContainer.innerHTML = \'\';\\n                \\n                eventSource = new EventSource(`/api/logs/${logId}/stream`);\\n                statusDiv.innerHTML = `<div class=\\"status success\\">\u2713 Streaming logs...</div>`;\\n                \\n                eventSource.onmessage = function(event) {\\n                    const line = document.createElement(\'div\');\\n                    line.className = \'log-line\';\\n                    line.textContent = event.data;\\n                    logContainer.appendChild(line);\\n                    logContainer.scrollTop = logContainer.scrollHeight;\\n                };\\n                \\n                eventSource.onerror = function(error) {\\n                    statusDiv.innerHTML = \'<div class=\\"status error\\">\u2717 Streaming stopped</div>\';\\n                    eventSource.close();\\n                };\\n            }\\n            \\n            function stopStream() {\\n                if (eventSource) {\\n                    eventSource.close();\\n                    eventSource = null;\\n                }\\n                document.getElementById(\'status\').innerHTML = \'<div class=\\"status\\">Streaming stopped</div>\';\\n            }\\n        <\/script>\\n    </body>\\n    </html>\\n    \\"\\"\\", 200\\n\\n\\nif __name__ == \\"__main__\\":\\n    logger.info(\\"=\\" * 60)\\n    logger.info(\\"Starting Log Streaming PoC Service...\\")\\n    logger.info(\\"=\\" * 60)\\n    print(\\"Starting Log Streaming PoC Service...\\")\\n    print(\\"Open http://localhost:5000 in your browser\\")\\n    logger.info(\\"Service listening on [http://127.0.0.1:5000](http://127.0.0.1:5000)\\")\\n    app.run(debug=True, port=5000)\\n```"},{"id":"application-level-sharding-design","metadata":{"permalink":"/docs/blog/application-level-sharding-design","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/shard-aware-app-design.md","source":"@site/blog/shard-aware-app-design.md","title":"Implementing Shard Aware Application","description":"In my previous post, we explored the high-level strategies for scaling databases, touching upon vertical scaling, read replicas, and finally, sharding. Today, I want to double-click on Sharding, specifically focusing on the Application-Level Sharding strategy.","date":"2025-12-14T00:00:00.000Z","tags":[],"readingTime":4.47,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Implementing Shard Aware Application","slug":"application-level-sharding-design","authors":"ashish","date":"2025-12-14T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Real-Time Communication with Server-Sent Events (SSE)","permalink":"/docs/blog/server-sent-events-explained"},"nextItem":{"title":"Scaling Distributed Systems (Focus on Databases)","permalink":"/docs/blog/scaling-distributed-systems"}},"content":"In my previous [post](/scaling.md), we explored the high-level strategies for scaling databases, touching upon vertical scaling, read replicas, and finally, sharding. Today, I want to double-click on **Sharding**, specifically focusing on the **Application-Level Sharding** strategy.\\n\\nWe often talk about \\"making the application shard-aware,\\" but what does that look like in code? How do we manage thousands of connections without overwhelming the application or the database? How do we handle re-sharding without downtime?\\n\\nIn this post, I will walk you through a battle-tested architecture we implemented at a previous organization, where we managed multi-tenant data at scale using a custom **Data Access Layer (DAL)**.\\n\\n## The Metadata Control Plane\\n\\nTo build a robust shard-aware application, you cannot hardcode connection strings or shard logic. You need a dynamic **Control Plane**. We solved this by externalizing our topology into two DynamoDB tables.\\n\\n### 1. Database Metadata\\nThis table acts as the source of truth for the physical infrastructure. It decouples the *logical* definition of a database from its *physical* connection details.\\n\\n* **Key:** `ShardName` + `DatabaseName`\\n* **Attributes:**\\n    * `ConnectionDetails` (JSON): Contains endpoints for different use cases (Write, Read, Analytics), credentials, and schema names.\\n    * `CapacityMetrics`: Tracks the current load (e.g., number of Small/Medium/Large tenants) to aid in placement decisions for new tenants.\\n\\n### 2. Tenant Metadata\\nThis table serves as the routing directory. It maps a specific tenant to a logical shard.\\n\\n* **Key:** `TenantID`\\n* **Attributes:**\\n    * `ShardName`: The logical shard where this tenant resides.\\n    * `DatabaseName`: The specific database instance within that shard.\\n\\n### The Logical Relationship\\n\\nBy splitting these concerns, we create a normalized view of our topology. The Tenant Metadata points to a logical location, and the Database Metadata resolves that location to physical credentials.\\n\\n```mermaid\\nerDiagram\\n    TENANT_METADATA {\\n        string TenantID PK\\n        string ShardName\\n        string DatabaseName\\n        string TenantTier\\n    }\\n    DATABASE_METADATA {\\n        string ShardName PK\\n        string DatabaseName PK\\n        json ConnectionDetails \\"Writes, Reads, Analytics\\"\\n        string CurrentCapacity\\n    }\\n    TENANT_METADATA }|..|| DATABASE_METADATA : \\"resolves to\\"\\n```\\n\\n---\\n\\n## The Bootup Sequence: `initializeDB()`\\n\\nEfficiency is paramount. We cannot fetch connection details from DynamoDB for every single query. Instead, we cache and initialize heavy resources during application startup.\\n\\nWhen the application boots, the **Data Access Layer (DAL)** performs the following `initializeDB()` routine:\\n\\n1.  **Fetch Topology:** It scans the `Database Metadata` table to understand the entire available fleet.\\n2.  **Pool Creation:** For every unique combination of `Shard` + `Database` + `UseCase` (Read/Write), it initializes a connection pool (e.g., HikariCP).\\n3.  **In-Memory Map:** These pools are stored in a concurrent map, keyed by the logical topology identifier.\\n\\nThis ensures that all expensive TCP handshakes are done eagerly, and the app is ready to serve traffic immediately upon becoming healthy.\\n\\n```mermaid\\nsequenceDiagram\\n    participant App as Application (DAL)\\n    participant DDB as DynamoDB (DB Metadata)\\n    participant DB as Physical Databases\\n    \\n    Note over App: Application Bootup\\n    App->>DDB: Scan Database Metadata Table\\n    DDB--\x3e>App: Return List<DbConfig>\\n    \\n    loop For each DbConfig\\n        App->>App: Extract Connection JSON\\n        App->>DB: Initialize Connection Pool (HikariCP)\\n        DB--\x3e>App: Connection Established\\n        App->>App: Map.put(Shard-DB-UseCase, Pool)\\n    end\\n    \\n    Note over App: Ready to serve traffic\\n```\\n\\n---\\n\\n## Runtime Request Handling: `getConnection(tenantId)`\\n\\nOnce the application is running, the focus shifts to low-latency routing. When a request comes in for `Tenant-123`, the DAL needs to find the correct connection seamlessly.\\n\\nThe API exposed to the upper layers is simple: `getConnection(Long tenantId)`.\\n\\n1.  **Lookup:** The DAL queries the `Tenant Metadata` (often cached locally with a TTL) to find the `ShardName` and `DatabaseName` for the tenant.\\n2.  **Resolution:** It constructs the lookup key for the in-memory map.\\n3.  **Acquisition:** It borrows a connection from the pre-warmed pool and hands it to the request thread.\\n\\n```mermaid\\nflowchart TD\\n    Request[\\"Incoming Request (Tenant 123)\\"] --\x3e DAL{Data Access Layer}\\n    \\n    subgraph Phase1 [Resolution Phase]\\n    DAL --\x3e|1. Lookup Tenant| Cache[(\\"Tenant Meta Cache\\")]\\n    Cache --\x3e|Returns| Info[\\"Shard: S1, DB: DB_A\\"]\\n    end\\n    \\n    subgraph Phase2 [Connection Acquisition]\\n    Info --\x3e|2. Construct Key| Key[\\"Key: S1-DB_A-Write\\"]\\n    Key --\x3e|3. Get from Map| Map[Connection Pool Map]\\n    Map --\x3e|4. Borrow Connection| Conn[Active Connection]\\n    end\\n    \\n    Conn --\x3e|Returns| DAL\\n    DAL --\x3e|Execute Query| DB[(\\"Physical DB Shard 1\\")]\\n```\\n\\n---\\n\\n## The Strategic Benefits\\n\\nImplementing this logic within a library provides two massive architectural advantages that go beyond simple connectivity.\\n\\n### 1. Logical Representation of Infrastructure\\nThe application code never touches a raw JDBC URL.\\n* **Scenario:** You need to rotate database credentials or migrate a database to a new host URL.\\n* **Action:** You simply update the JSON in the `Database Metadata` table and trigger a refresh event in the application.\\n* **Result:** The DAL re-initializes the specific pool. No code deployment is required. The infrastructure is now configuration-driven.\\n\\n### 2. Logical Mapping & Zero-Downtime Migration\\nThe mapping between a tenant and a database is fluid, not static.\\n* **Scenario:** `Tenant-ABC` has grown too large for `Shard-1` and needs to be moved to `Shard-2`.\\n* **Action:** After replicating the data to `Shard-2`, you simply update the `Tenant Metadata` table for `Tenant-ABC`.\\n* **Result:** The very next request for `Tenant-ABC` will resolve to `Shard-2`. The application layer requires zero changes to support tenant migration, enabling seamless re-balancing of the fleet.\\n\\n## Conclusion\\n\\nBuilding a shard-aware application is an exercise in **separation of concerns**. By isolating the *topology* (Tenant Metadata) from the *infrastructure* (Database Metadata) and wrapping it all in a smart Data Access Layer, you gain the flexibility to scale, migrate, and manage your data tier without constantly refactoring your application code."},{"id":"scaling-distributed-systems","metadata":{"permalink":"/docs/blog/scaling-distributed-systems","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/scaling.md","source":"@site/blog/scaling.md","title":"Scaling Distributed Systems (Focus on Databases)","description":"In distributed systems, the four possible chokepoints that a system can run into are CPU, Memory, Network, and Disk. Whenever the system hits the thresholds on any of these chokepoints, latency increases, throughput degrades, and we typically need to scale our systems.","date":"2025-12-13T00:00:00.000Z","tags":[],"readingTime":6.32,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Scaling Distributed Systems (Focus on Databases)","slug":"scaling-distributed-systems","authors":"ashish","date":"2025-12-13T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Implementing Shard Aware Application","permalink":"/docs/blog/application-level-sharding-design"},"nextItem":{"title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","permalink":"/docs/blog/avoid-hard-deletes"}},"content":"In distributed systems, the four possible chokepoints that a system can run into are **CPU, Memory, Network, and Disk**. Whenever the system hits the thresholds on any of these chokepoints, latency increases, throughput degrades, and we typically need to scale our systems.\\n\\nScaling isn\'t just about \\"adding more\\"; it\'s about *how* and *where* you add capacity. Broadly, we have two strategies:\\n\\n1.  **Vertical Scaling**\\n2.  **Horizontal Scaling**\\n\\n## The Scaling Strategies\\n\\n### Vertical Scaling (Scale Up)\\nIn Vertical scaling, we aim to solve for bottlenecks by throwing more CPU, memory, disk, and network bandwidth to the application. Basically, we make the infrastructure bulky enough to handle more load and try to remain under the threshold of our resources.\\n\\n**Pros:** Simplifies operations (no network overhead between nodes, no distributed consensus issues).\\n**Cons:** Has a hard ceiling (hardware limits) and can get exponentially expensive.\\n\\n```mermaid\\ngraph TD\\n    subgraph \\"Vertical Scaling\\"\\n    A[Small Server] --\x3e|Scale Up| B[Massive Server]\\n    B --\x3e|Contains| C[More CPU]\\n    B --\x3e|Contains| D[More RAM]\\n    B --\x3e|Contains| E[Higher I/O]\\n    end\\n    style A fill:#f9f,stroke:#333,stroke-width:2px\\n    style B fill:#bbf,stroke:#333,stroke-width:4px\\n```\\n\\n### Horizontal Scaling (Scale Out)\\nIn horizontal scaling, we run multiple instances of the application on multiple machines. Basically, we add more machines to the compute layer. This fleet of compute servers is typically fronted by a load balancer.\\n\\nAs the load increases, the compute layer is linearly scaled to cope with the load. Horizontal scaling also aids in fault tolerance; if one node dies, the others pick up the slack.\\n\\n```mermaid\\ngraph TD\\n    User((User)) --\x3e LB[Load Balancer]\\n    subgraph \\"Compute Fleet\\"\\n    LB --\x3e Server1[App Instance 1]\\n    LB --\x3e Server2[App Instance 2]\\n    LB --\x3e Server3[App Instance 3]\\n    end\\n    style LB fill:#f96,stroke:#333,stroke-width:2px\\n```\\n\\n:::info The \\"Unit Tech Economics\\"\\n\\nDouble-clicking a bit on the phrase *\\"As the load typically increases, the compute layer is linearly scaled\\"*: In order for this step to be efficient, it is very important to know the **\\"Unit Tech Economics\\"**.\\n\\nFor a given machine of a certain size (i.e., # of CPUs, RAM, Disk, and N/W bandwidth), what is the throughput that can be successfully and healthily served?\\n\\nThere isn\'t a magic formula for this. Each application needs to be load tested with **one machine** to arrive at the \\"Unit Tech Economics.\\" Once you have these numbers, all you need to do is extrapolate them to the actual load anticipated in production or during peak hours.\\n:::\\n\\n:::caution The \\"Infinite Scale\\" Fallacy\\nDoes this mean that with horizontal scaling of the API servers you can achieve infinite scale? **Not really.**\\n\\nWhat about your downstream systems, caches, and DBs? Will they be able to serve the spiked-up load? **NO.**\\n\\nWhen scaling, one should adopt the **bottoms-up approach**. Scaling API servers should ideally be the last component to be scaled up.\\n:::\\n---\\n\\n## Deep Dive: Scaling the Database\\n\\nWith this context set, let\'s go deep into the scaling aspects of DBs (stateful components).\\n\\n### 1. Vertical Scaling\\nAs explained above, with vertical scaling\u2014without splitting your DB instance\u2014you typically host your DB on a bigger server, i.e., a machine with more capacity in terms of CPU, Memory, Disk, and Network. Basically, you run your DB on a bulkier server.\\n\\n```mermaid\\ngraph TD\\n    App[Application Fleet] --\x3e|Writes & Reads| DB[Massive Primary DB Instance]\\n    subgraph \\"Vertical DB Scaling\\"\\n    DB\\n    end\\n    style DB fill:#bbf,stroke:#333,stroke-width:4px\\n```\\n\\n> **Real World Example: Zerodha**\\n> One of India\'s largest stock brokers, famously scaled their massive volume by avoiding premature distributed complexity. They run their core systems on a **single, massive Postgres Server**.\\n>\\n> **Summary:** Instead of rushing to sharding, Zerodha focused on optimizing queries and utilizing high-end hardware.\\n> * **How:** They use high-performance hardware (huge RAM to fit working sets in memory) and optimized Postgres configurations.\\n> * **Pros:** Simplifies architecture (no distributed transactions), ensures strict ACID compliance, and offers low latency.\\n> * **Cons:** It is a Single Point of Failure (SPOF) if not HA, and restart times can be long due to the sheer size of memory buffers.\\n>\\n> *Read more: [Zerodha Tech Blog: Working with PostgreSQL](https://zerodha.tech/blog/working-with-postgresql/)*\\n\\n### 2. Adding Read Replicas\\nIn this approach, you provision for serving read traffic. Basically, you add one or more DB servers that specifically serve read traffic. This approach helps in reserving the primary node of the DB for writes, effectively isolating read and write traffic.\\n\\n```mermaid\\ngraph TD\\n    App[Application] --\x3e|Writes| Master[Primary Node]\\n    App --\x3e|Reads| Replica1[Read Replica 1]\\n    App --\x3e|Reads| Replica2[Read Replica 2]\\n    Master -.->|Async Replication| Replica1\\n    Master -.->|Async Replication| Replica2\\n```\\n\\nIn terms of reading from the application standpoint, the application need not know about multiple servers. It can leverage a couple of solutions to simplify reader connection management:\\n\\n#### a) Reader Endpoint (AWS Aurora / RDS)\\nIf you are using AWS Aurora or RDS, you can leverage the **Reader Endpoint**. A reader endpoint is a DNS record exposed by AWS. The application simply connects to this endpoint.\\n\\nAt a high level, one can visualize the reader endpoint as a load balancer sitting in front of the DB readers. It distributes these read requests fairly amongst the available readers.\\n\\n```mermaid\\ngraph TD\\n    App[Application] --\x3e|Connects to| RE[AWS Reader Endpoint]\\n    RE --\x3e|Distributes| R1[Replica 1]\\n    RE --\x3e|Distributes| R2[Replica 2]\\n    RE --\x3e|Distributes| R3[Replica 3]\\n```\\n\\n#### b) RDS Proxy\\n**What is RDS Proxy?**\\nAmazon RDS Proxy is a fully managed, highly available database proxy that sits between your application and your relational database.\\n\\n**Use Cases:**\\n1.  **Connection Pooling:** It maintains a pool of established connections to your database, sharing them among application requests. This is crucial for serverless applications (like Lambda) that might open thousands of connections quickly.\\n2.  **Resiliency:** It reduces failover times by bypassing DNS cache propagation delays.\\n\\n```mermaid\\ngraph TD\\n    Lambda[Lambda/App] --\x3e|Open Connections| Proxy[RDS Proxy]\\n    Proxy --\x3e|Pooled Connections| DB[Database Cluster]\\n    style Proxy fill:#ff9,stroke:#333,stroke-width:2px\\n```\\n\\n### 3. Sharding\\nSharding is **not** a Day 0 approach. You typically do this when you know the user and data volume will exceed the capacity of a vertically scaled single node.\\n\\nIn Sharding, you split the data across multiple primary shards.\\n\\n```mermaid\\ngraph TD\\n    App[Application]\\n    App --\x3e|Subset A-J| Shard1[Shard 1: Users A-J]\\n    App --\x3e|Subset K-T| Shard2[Shard 2: Users K-T]\\n    App --\x3e|Subset U-Z| Shard3[Shard 3: Users U-Z]\\n```\\n\\n**How do you decide where data resides?**\\n\\n1.  **Range-Based Mapping:**\\n    * Assume you are a platform serving millions of tenants. You might say: Tenants starting with `a-j` reside in Shard 1, `k-t` in Shard 2, and `u-z` in Shard 3.\\n2.  **Static Mapping:**\\n    * In this approach, you maintain a static mapping (lookup table) between tenants and DB shards. E.g., Tenant 1 maps to Shard 1, Tenant 2 maps to Shard 1, Tenant 3 maps to Shard 2, etc.\\n\\n**Where do you host this mapping logic?**\\n\\n1.  **Application Layer**\\n    * We make the application \\"shard aware.\\" The application **KNOWS** the DB topology and determines which shard a given tenant resides in.\\n    * The application then connects to the respective DB shard to perform reads and writes.\\n\\n2.  **External Layer (Smart Proxy)**\\n    * Instead of the application knowing the topology, a proxy layer **KNOWS** the DB topology.\\n    * You configure server rules and query rules in the proxy layer. Based on the incoming query, the proxy routes the request to the correct DB shard.\\n\\n*I have written separate blogs on [Shard Aware Applications](/shard-aware-app-design.md) and [Exteral DB Proxy](/proxy-server.md) to go really deep into BOTH the above techniques (Application vs. Proxy sharding).*"},{"id":"avoid-hard-deletes","metadata":{"permalink":"/docs/blog/avoid-hard-deletes","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/avoid-hard-deletes.md","source":"@site/blog/avoid-hard-deletes.md","title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","description":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","date":"2025-12-11T12:26:27.000Z","tags":[],"readingTime":18.05,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"slug":"avoid-hard-deletes","title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","sidebar_label":"The Pains of Hard Delete","description":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","authors":"ashish"},"unlisted":false,"prevItem":{"title":"Scaling Distributed Systems (Focus on Databases)","permalink":"/docs/blog/scaling-distributed-systems"},"nextItem":{"title":"The Thundering Herd - Understanding and Solving the Cache Stampede","permalink":"/docs/blog/cache-stampede-thundering-herd"}},"content":"## Summary\\n\\nIn the discipline of database architecture, the mechanism of deletion is frequently underestimated. While the creation and retrieval of data receive significant attention regarding optimization and structure, the removal of data\u2014specifically the \\"Hard Delete\\"\u2014is often treated as a trivial housekeeping operation. However, an exhaustive analysis of storage engine internals reveals that hard deletion is one of the most resource-intensive and structurally disruptive operations in a relational database management system (RDBMS).\\n\\nThis report provides a comprehensive technical analysis of the storage engines underpinning major databases, specifically MySQL\'s InnoDB and PostgreSQL\'s Heap architecture. It articulates the high costs associated with physical record removal, including B+ Tree rebalancing, locking contention, input/output (I/O) thrashing, and vacuum-induced bloat. Consequently, this document advocates for a \\"Soft Delete by Design\\" methodology, treating deletion as a logical state transition rather than a physical storage event. It concludes with a robust lifecycle management strategy that couples logical soft deletion with asynchronous, batched hard deletion to maintain long-term storage hygiene without compromising system stability.\\n\\n## 1\\\\. Storage Engine Internals\\n\\nTo comprehend the catastrophic potential of a hard delete, one must first possess a nuanced understanding of how data resides on the disk. The abstraction of a \\"table\\" composed of \\"rows\\" and \\"columns\\" is a logical convenience; the physical reality is a complex arrangement of binary pages, pointers, and trees.\\n\\n### **1.1 MySQL InnoDB: The Index-Organized Table**\\n\\nIn the MySQL ecosystem, the default storage engine is InnoDB. Its defining characteristic is that it does not merely use an index to find data; the table *is* the index. This architecture is known as a Clustered Index, implemented via a B+ Tree data structure.\\n\\n#### **1.1.1 The B+ Tree Hierarchy**The Lifecycle Strategy\\n\\nThe B+ Tree is a self-balancing tree structure designed to maintain sorted data and allow searches, sequential access, insertions, and deletions in logarithmic time. It is distinct from a binary tree in its high fan-out; a single node can have hundreds of children, allowing the tree to remain incredibly shallow (usually 3 to 4 levels deep) even for tables containing billions of rows.\\n\\n* **The Root Page:** The entry point of the tree. It resides at a fixed location and contains pointers to internal nodes.\\n* **Internal Nodes (Non-Leaf):** These nodes act as the navigational roadmap. They contain only the Primary Key values and pointers to child pages. Because they do not store the full row data, they are lightweight. A single 16KB page can store hundreds of keys, maximizing the \\"fan-out\\" capability and ensuring that traversing the tree requires minimal disk I/O.\\n* **Leaf Nodes (The Data):** This is where the physical reality of the Clustered Index becomes apparent. The leaf nodes contain the actual row data\u2014every column of every record.\\n\\n#### **1.1.2 Primary Key Ordering in Leaf Nodes**\\n\\nA critical architectural feature of InnoDB is that the leaf nodes are strictly ordered by the Primary Key. This is not a suggestion; it is a physical enforcement. If a table has a Primary Key of ID, the row with ID=1 is physically stored adjacent to ID=2 within the 16KB page.\\n\\nFurthermore, these leaf nodes are strictly linked in a doubly linked list. Each leaf page contains a pointer to the previous page and the next page. This structure enables the database to perform extremely fast range scans (e.g., SELECT \\\\* FROM orders WHERE id BETWEEN 100 AND 200\\\\) by traversing the linked list rather than returning to the root node.\\n\\n#### **1.1.3 Page Filling and Fragmentation**\\n\\nInnoDB manages storage in units called Pages, typically 16KB in size. When records are inserted sequentially (e.g., using an auto-incrementing integer), InnoDB fills these pages efficiently, leaving only a small fraction (typically 1/16th) free for future modifications. This results in a \\"Fill Factor\\" of nearly 100%, ensuring optimal disk usage and cache efficiency.\\n\\nHowever, when data is modified or deleted, this order is disturbed. A hard delete creates a physical \\"hole\\" in the page. If enough holes are created, the page density drops, leading to fragmentation\u2014a state where the database engine is caching and reading pages that are largely empty air.\\n\\n**Table 1: B+ Tree Node Characteristics**\\n\\n| Node Type | Content | Purpose | Density |\\n| :---- | :---- | :---- | :---- |\\n| **Internal Node** | Primary Keys \\\\+ Child Pointers | Navigation | High (Keys only) |\\n| **Leaf Node** | Full Row Data | Storage | Variable (Depends on Row Size) |\\n| **Leaf Linkage** | Double Pointers (Prev/Next) | Range Scans | N/A |\\n\\n```mermaid\\ngraph TD\\n    subgraph \\"Internal Nodes (Navigation Layer)\\"\\n        Root --\x3e NodeA[\\"<b>Page 10</b><br/>Keys: 10, 50\\"]\\n        Root --\x3e NodeB[\\"<b>Page 20</b><br/>Keys: 500, 700\\"]\\n    end\\n\\n    subgraph \\"Leaf Nodes (Data Layer - Ordered by PK)\\"\\n        NodeA --\x3e Leaf1\\n        NodeA --\x3e Leaf2\\n        NodeB --\x3e Leaf3\\n        NodeB --\x3e Leaf4\\n    end\\n\\n    Leaf1 <==>|Doubly Linked List| Leaf2\\n    Leaf2 <==>|Doubly Linked List| Leaf3\\n    Leaf3 <==>|Doubly Linked List| Leaf4\\n\\n    style Leaf1 fill:#d4f1f4,stroke:#00acc1,stroke-width:2px\\n    style Leaf2 fill:#d4f1f4,stroke:#00acc1,stroke-width:2px\\n    style Leaf3 fill:#d4f1f4,stroke:#00acc1,stroke-width:2px\\n    style Leaf4 fill:#d4f1f4,stroke:#00acc1,stroke-width:2px\\n```\\n\\n\\n### **1.2 PostgreSQL: The Heap and Tuple Architecture**\\n\\nPostgreSQL employs a fundamentally different storage paradigm known as the Heap. Unlike InnoDB, where the table is the index, PostgreSQL separates the table storage (the Heap) from the index storage.\\n\\n#### **1.2.1 Heap Pages and Item Pointers**\\n\\nIn PostgreSQL, data is stored in a file known as the \\"Heap.\\" This file is divided into fixed-length blocks, typically 8KB. Within these blocks, records (referred to as \\"tuples\\") are stored in an unordered fashion. A tuple is inserted into the first page that has sufficient free space, regardless of its Primary Key value.\\n\\nThis lack of inherent order necessitates a mechanism to locate data. PostgreSQL uses the Tuple Identifier (TID), often referred to as ctid. The ctid is a coordinate pair: (Block Number, Offset Number). For example, (42, 7\\\\) means the 7th item on the 42nd page.\\n\\n#### **1.2.2 The Array of Line Pointers**\\n\\nTo manage the internal organization of an 8KB page, PostgreSQL uses an array of \\"Line Pointers\\" (ItemIds) at the beginning of the page header.\\n\\n* **Indirection:** The ctid actually points to a Line Pointer, not the tuple itself. The Line Pointer then points to the byte offset where the tuple begins within the page.\\n* **Why Indirection Matters:** This allows the database to defragment the page internally (move tuples around to close gaps) without changing the ctid or updating external indexes. As long as the Line Pointer at index 7 remains, the tuple can be anywhere in the page.\\n\\n#### **1.2.3 Ordered Indexes vs. Unordered Heap**\\n\\nIt is a common misconception that PostgreSQL tables are ordered by Primary Key. They are not. The Heap is unordered.8 However, the **Primary Key Index** is a B-Tree structure that *is* strictly ordered. This index stores the Primary Key value and the corresponding ctid of the heap tuple. When a query requests a row by ID, the engine searches the ordered B-Tree Index, finds the ctid, and then retrieves the unordered tuple from the Heap.\\n\\n**Table 2: Comparison of Storage Architectures**\\n\\n| Feature | MySQL InnoDB (B+ Tree) | PostgreSQL (Heap) |\\n| :---- | :---- | :---- |\\n| **Organization** | Clustered (Data is in the Index) | Heap (Data is separate from Index) |\\n| **Ordering** | Physically ordered by Primary Key | Unordered (Insert order / Random) |\\n| **Row ID** | Primary Key | CTID (Block \\\\+ Offset) |\\n| **Secondary Index** | Points to Primary Key Value | Points to CTID |\\n| **Page Size** | 16KB (Default) | 8KB (Default) |\\n\\n```mermaid\\ngraph TB\\n    subgraph \\"Postgres Page Layout (8KB)\\"\\n        Header\\n\\n        subgraph \\"Line Pointer Array\\"\\n            LP1[ItemId 1]\\n            LP2[ItemId 2]\\n            LP3[ItemId 3]\\n            LP4\\n        end\\n\\n        subgraph \\"Tuple Storage (Unordered)\\"\\n            Tuple1\\n            Tuple3\\n            Tuple2\\n            FreeSpace\\n        end\\n    end\\n\\n    Header --\x3e LP1\\n    LP1 -.-> Tuple1\\n    LP2 -.-> Tuple2\\n    LP3 -.-> Tuple3\\n    LP4 -.->|Deleted| FreeSpace\\n\\n    style Tuple1 fill:#fff9c4,stroke:#fbc02d\\n    style Tuple2 fill:#fff9c4,stroke:#fbc02d\\n    style Tuple3 fill:#fff9c4,stroke:#fbc02d\\n    style FreeSpace fill:#eceff1,stroke:#cfd8dc\\n```\\n\\n## 2\\\\. The pain caused by Hard Deletion\\n\\nWith the storage context established, we can analyze the mechanics of the DELETE command. Far from a simple erasure, a hard delete triggers a complex sequence of internal operations that can destabilize the database.\\n\\n### **2.1 MySQL InnoDB: The Rebalancing Storm**\\n\\nIn a B+ Tree, structural integrity is paramount. The tree must remain balanced to ensure predictable performance. Deleting a row threatens this balance.\\n\\n#### **2.1.1 The Search and Destroy Mission**\\n\\nWhen a DELETE is issued, InnoDB must first traverse the tree to locate the leaf node containing the record. Once found, the record is not immediately wiped; it is \\"delete-marked.\\" This is a logical flag in the record header indicating the space is technically free but occupied by a \\"ghost\\".\\n\\n#### **2.1.2 The Merge Threshold**\\n\\nThe true cost arises when deletions accumulate. Each page has a MERGE\\\\_THRESHOLD, typically defaulting to 50%.\\n\\n1. **Underflow:** If a deletion causes the page\'s data volume to drop below this threshold, InnoDB determines that the page is inefficient.\\n2. **Locking:** The engine places locks on the page and its neighbors (sibling nodes).\\n3. **Merge Operation:** It attempts to merge the remaining records into a sibling page (left or right).\\n4. **Cascading Reparenting:** If a page is emptied and removed, the pointer in the *parent* node must be deleted. If this deletion causes the *parent* node to drop below its own threshold, the merge operation propagates upward. This \\"rebalancing storm\\" can ripple up to the root, causing massive I/O and locking overhead.\\n\\n#### **2.1.3 Secondary Index Maintenance**\\n\\nEvery secondary index in InnoDB stores the Primary Key as its pointer. If you delete a user with ID=100, Email=bob@test.com, and Status=Active, InnoDB must:\\n\\n1. Delete 100 from the Clustered Index (B+ Tree).\\n2. Delete bob@test.com from the Email Index (B+ Tree).\\n3. Delete Active from the Status Index (B+ Tree).  \\n   Each of these requires random I/O operations. While the Change Buffer helps mitigate this for non-unique indexes by caching changes, unique indexes require immediate, synchronous disk operations to enforce constraints, amplifying the I/O cost significantly.\\n\\n### **2.2 PostgreSQL**\\n\\nPostgreSQL handles deletion via Multi-Version Concurrency Control (MVCC). It does not remove data immediately; it versions it.\\n\\n#### **2.2.1 The Invisible Update**\\n\\nIn PostgreSQL, an UPDATE is effectively a DELETE followed by an INSERT. A DELETE is simply an UPDATE that puts nothing back.\\n\\n* **xmin and xmax:** Every tuple has an xmin (the transaction ID that created it) and an xmax (the transaction ID that deleted it).\\n* **Marking as Dead:** When DELETE is run, Postgres finds the tuple and sets its xmax to the current transaction ID. The data remains physically on the disk.\\n* **Visibility Rules:** Future transactions check the xmax. If xmax is set and committed, the tuple is invisible. To the storage engine, the page looks exactly the same as before, but the tuple is logically dead.\\n\\n#### **2.2.2 The Vacuum Necessity**\\n\\nSince DELETE does not free space, the table size does not decrease. It creates \\"Dead Tuples.\\" If these are not cleaned up, the table becomes \\"bloated\\"\u2014a mixture of live data and digital corpses.\\n\\n* **Autovacuum:** The autovacuum daemon runs in the background. It scans tables, looking for dead tuples that are older than any active transaction.\\n* **Freeing Space:** It marks the line pointers as \\"unused,\\" allowing new inserts to overwrite the dead space. However, this process consumes CPU and I/O bandwidth and can block schema changes.\\n\\n```mermaid\\ngraph LR\\n    subgraph \\"Postgres MVCC Delete Process\\"\\n        T1\\n        Action\\n        T2\\n        Vacuum[Vacuum Process]\\n        Free\\n\\n        T1 --\x3e|Update xmax| T2\\n        T2 --\x3e|Scanned by| Vacuum\\n        Vacuum --\x3e|Reclaim| Free\\n    end\\n```\\n\\n## 3\\\\. The Performance Impact of Hard Deletion\\n\\nThe internal mechanics described above manifest as tangible performance degradation in production environments. The impact of hard deletes is rarely linear; it is exponential relative to the volume of data and concurrency of the system.\\n\\n### **3.1 The Locking Bottleneck**\\n\\nHard deletes are blocking operations.\\n\\n* **Gap Locks (MySQL):** To preserve transaction isolation (specifically to prevent \\"Phantom Reads\\"), InnoDB places \\"Gap Locks\\" on the space *between* records. If you delete ID=10, InnoDB might lock the gap from ID=5 to ID=15. Any other transaction trying to insert ID=12 will be blocked until the delete commits. In high-concurrency systems, this leads to lock contention and \\"Lock Wait Timeout Exceeded\\" errors.\\n* **Exclusive Locks:** Both engines take exclusive locks on the specific rows being deleted. If a reporting query is reading those rows, the delete will block (or vice versa), causing system stutter.\\n\\n### **3.2 I/O Thrashing and Buffer Pool Pollution**\\n\\nDatabase performance relies heavily on caching \\"hot\\" pages in RAM (Buffer Pool).\\n\\n* **Random Access:** Hard deletes are often random (e.g., deleting users who cancelled today). This forces the database to load widely scattered pages from the disk into memory just to mark a single bit.\\n* **Dirty Pages:** Modifying a page marks it as \\"dirty.\\" Dirty pages must be flushed back to disk. A massive delete operation creates a flood of dirty pages, saturating the I/O subsystem and slowing down critical read operations.\\n\\n### **3.3 Index Bloat and Degradation**\\n\\n* **Postgres Index Bloat:** In PostgreSQL, indexes contain pointers to heap tuples. When a tuple is updated or deleted, the index entry remains until Vacuum runs. If a table is heavily churned (high delete/update rate), the indexes can grow larger than the table itself. Larger indexes equate to slower searches, as they are less likely to fit in RAM.\\n* **MySQL Fragmentation:** As described in Section 2.1.2, B+ Tree pages that are 50-60% full are inefficient. This fragmentation means that to read 1GB of actual data, the engine might need to read 2GB of disk pages, effectively halving the I/O throughput.\\n\\n## 4\\\\. The Architectural Solution: Soft Deletion\\n\\nThe \\"Soft Delete\\" pattern solves the physical storage problems by decoupling the **business intent** of deletion from the **database mechanism** of deletion. Instead of instructing the storage engine to perform a destructive structural change, the application performs a non-structural state change.\\n\\n### **4.1 Implementation Patterns**\\n\\n#### **4.1.1 The Boolean Flag**\\n\\nThe simplest implementation is a boolean column.\\n\\n* **Schema:** is\\\\_deleted BOOLEAN DEFAULT FALSE\\n* **Logic:** UPDATE table SET is\\\\_deleted \\\\= TRUE WHERE id \\\\= 1\\n* **Critique:** While lightweight, this pattern lacks context. It tells you *that* a record was deleted, but not *when*.\\n\\n#### **4.1.2 The Timestamp (Recommended)**\\n\\nThe industry standard pattern involves a nullable timestamp.\\n\\n* **Schema:** deleted\\\\_at TIMESTAMP NULL\\n* **Logic:** UPDATE table SET deleted\\\\_at \\\\= NOW() WHERE id \\\\= 1\\n* **Query:** SELECT \\\\* FROM table WHERE deleted\\\\_at IS NULL\\n* **Benefit:** This acts as a boolean flag (Null/NotNull) while simultaneously providing an audit trail of the deletion event.\\n\\n#### **4.1.3 The Status Enum**\\n\\nFor complex state machines, deletion is just one of many states.\\n\\n* **Schema:** status VARCHAR(20) CHECK (status IN (\'active\', \'pending\', \'archived\', \'deleted\'))\\n* **Benefit:** Useful when \\"deletion\\" is part of a workflow, such as a \\"Recycle Bin\\" that transitions to \\"Permanently Deleted.\\"\\n\\n### **4.2 Why Soft Deletes Resolve Storage Issues**\\n\\n1. **Zero Rebalancing:** Updating a deleted\\\\_at timestamp is an in-place update. In MySQL, since the Primary Key is not changing, the row does not move. The B+ Tree structure remains perfectly balanced. No page merges occur.\\n2. **Preserved Sequentiality:** The data remains physically adjacent. Sequential read performance is preserved.\\n3. **Reduced Locking:** An update to a non-indexed column (like deleted\\\\_at) generally requires only a row lock, avoiding the aggressive Gap Locks associated with structural removal.\\n\\n```mermaid\\ngraph TD\\nUser\\n\\nsubgraph \\"Hard Delete Path (Destructive)\\"\\nUser --\x3e|DELETE SQL| HD_Lock\\nHD_Lock --\x3e|B-Tree Rebalance| HD_Struct\\nHD_Struct --\x3e|Page Merge| HD_IO[High I/O Cost]\\nHD_IO --\x3e|Data Gone| HD_End[Irrecoverable]\\nend\\n\\nsubgraph \\"Soft Delete Path (Non-Destructive)\\"\\nUser --\x3e|UPDATE SQL| SD_Lock\\nSD_Lock --\x3e|In-Place Update| SD_Struct\\nSD_Struct --\x3e|Log Write| SD_IO[Low I/O Cost]\\nSD_IO --\x3e|Data Hidden| SD_End\\nend\\n\\nstyle HD_End fill:#ffcdd2,stroke:#c62828\\nstyle SD_End fill:#c8e6c9,stroke:#2e7d32\\n```\\n\\n## 5\\\\. Business Use Cases for Soft Deletion\\n\\nBeyond performance optimization, Soft Deletion enables critical business capabilities that Hard Deletion inherently destroys.\\n\\n### **5.1 Recoverability: The \\"Undo\\" Button**\\n\\nHuman error is an inevitability in software systems.\\n\\n* **Unintended Deletes:** Users frequently delete content accidentally on mobile devices. Soft deletes allow for an immediate \\"Undo\\" feature without complex backup restoration.\\n* **Production Safety:** Engineering history is replete with stories of developers accidentally running DELETE without a WHERE clause. With soft deletes, this catastrophe is reversible via a simple SQL UPDATE statement (UPDATE table SET deleted\\\\_at \\\\= NULL). With hard deletes, this becomes a disaster recovery scenario requiring Point-in-Time Recovery (PITR), potentially costing hours of downtime and data loss.\\n\\n### **5.2 Audit and Compliance**\\n\\nIn regulated industries, data deletion is often a legal construct rather than a physical one.\\n\\n* **Traceability:** A deleted\\\\_at column, often paired with deleted\\\\_by\\\\_user\\\\_id, provides an immutable audit trail. Organizations can answer inquiries such as \\"Who cancelled this order?\\" or \\"When was this account terminated?\\".\\n* **Legal Obligations:** Depending upon the business, some regulations might mandate data retention for 7-10 years. A user\'s request to \\"delete my account\\" must be balanced against the legal requirement to \\"retain transaction history.\\" Soft deletion satisfies the user\'s visibility requirement while satisfying the regulator\'s retention requirement.\\n\\n### **5.3 Referential Integrity and Cascading**\\n\\nRelational databases enforce integrity via Foreign Keys. You cannot delete a Parent if it has Children.\\n\\n* **Hard Delete Complexity:** To hard delete a User, you must first delete their Orders, Invoices, Logs, and Comments. This triggers a massive \\"Cascading Delete\\" transaction that can touch dozens of tables and lock millions of rows.\\n* **Soft Delete Simplicity:** You simply mark the User as deleted. The child records remain untouched (and referentially valid). The application layer is responsible for filtering out \\"Orders belonging to deleted Users\\" during display. This avoids the massive transactional overhead of cascading deletes.\\n\\n## 6\\\\. Disadvantages and Engineering Trade-offs\\n\\nSoft deletion is an architectural compromise. It solves storage and recovery issues but introduces application-layer complexity.\\n\\n### **6.1 The \\"Forgotten WHERE Clause\\"**\\n\\nThe most pervasive risk is query correctness.\\n\\n* **The Leak:** Every query in the application must now include AND deleted\\\\_at IS NULL. If a developer forgets this clause in a \\"Total Revenue\\" report, the report will incorrectly include refunded (deleted) orders.\\n* **Mitigation:**\\n    * **ORM Scopes:** Use framework features (e.g., Hibernate @Where, Eloquent SoftDeletes) to automatically inject this clause.\\n    * **Views:** Create a database view active\\\\_users that filters the data, and restrict application access to the view rather than the raw table.\\n\\n### **6.2 Index Bloat**\\n\\nSoft-deleted rows are still physically present. If a table contains 90% deleted data (e.g., a queue table), the indexes will be 90% \\"junk.\\" This reduces the effectiveness of the RAM cache, as valuable memory is wasted storing pointers to deleted data.\\n\\n## 7\\\\. The Lifecycle Strategy: Batch Hard Deletion\\n\\nWe have established that Hard Deletes are destructive, but Soft Deletes cause unlimited growth (bloat). The optimal architecture is a hybrid lifecycle: **Soft Delete for Operations, Batch Hard Delete for Maintenance.**\\n\\n### **7.1 The \\"Nightly\\" Batch Deletion Job**\\n\\nThe goal is to decouple the high-frequency user action (clicking delete) from the high-cost database operation (physical removal).\\n\\n1. **User Action:** Soft Delete. Instant, safe, recoverable.\\n2. **Retention Policy:** \\"We keep deleted data for 30 days.\\"\\n3. **Background Process:** A nightly job runs during off-peak hours (e.g., 3:00 AM) to physically purge records older than 30 days.\\n\\n### **7.2 Why Batching is Superior**\\n\\n* **Amortized I/O:** Loading a page to delete 100 records is 100x more efficient than loading that page 100 separate times to delete 1 record at a time.\\n* **Sequential Access:** Batch jobs can process deletions in Primary Key order, ensuring the disk head moves linearly.\\n* **Reduced Locking:** The batch job can lock a small range, perform the delete, and release the lock, minimizing impact on active users.\\n\\n### **7.3 Implementation: Chunked Deletion**\\n\\nRunning DELETE FROM logs WHERE created\\\\_at \\\\< \'2023-01-01\' is dangerous. It will attempt to lock millions of rows, potentially crashing the database. The correct approach is **Chunking**.\\n\\n#### **Algorithm**\\n\\n1. Identify the target rows.\\n2. Delete in small batches (e.g., 1000 rows).\\n3. Sleep/Throttle between batches to allow replication to catch up and CPU to cool down.\\n\\n#### **MySQL Example (Stored Procedure)**\\n\\nMySQL allows LIMIT in DELETE statements, enabling simple chunking.\\n\\n```sql\\nCREATE PROCEDURE PurgeOldData()  \\nBEGIN  \\nREPEAT  \\n-- Delete 1000 rows, ordered by ID to maintain B-Tree locality  \\nDELETE FROM users   \\nWHERE deleted\\\\_at \\\\< DATE\\\\_SUB(NOW(), INTERVAL 30 DAY)   \\nORDER BY id   \\nLIMIT 1000;\\n\\n    -- Sleep to prevent lock contention  \\n    DO SLEEP(0.5);  \\nUNTIL ROW\\\\_COUNT() \\\\= 0 END REPEAT;  \\nEND  \\n```\\n\\n\\n\\nNote: The ORDER BY id clause is critical. It ensures the deletion walks the B+ Tree leaves sequentially, preventing random I/O thrashing.\\n\\n#### **PostgreSQL Example (CTE)**\\n\\nPostgreSQL requires a Common Table Expression (CTE) to achieve similar chunking with lock skipping.\\n\\n```sql\\nWITH rows_to_delete AS (  \\n  SELECT ctid  \\n  FROM users  \\n  WHERE deleted_at < NOW() - INTERVAL \'30 days\'  \\n  LIMIT 1000  \\n  FOR UPDATE SKIP LOCKED -- Critical: Skip rows currently in use  \\n)  \\nDELETE FROM users u  \\nUSING rows_to_delete d  \\nWHERE u.ctid = d.ctid;\\n```\\n\\nNote: FOR UPDATE SKIP LOCKED allows the maintenance job to run concurrently with user activity without blocking.\\n\\n### **7.4 Partition Pruning**\\n\\nFor massive datasets (e.g., Audit Logs, Event Streams), even batch deletion is too slow. The architectural solution is **Table Partitioning**.\\n\\n* **Strategy:** Partition the table by date (e.g., audit\\\\_2023\\\\_01, audit\\\\_2023\\\\_02).\\n* **Deletion:** When January\'s data expires, you do not run DELETE. You run DROP TABLE audit\\\\_2023\\\\_01.\\n\\n## 8\\\\. Conclusion\\n\\nThe decision to adopt Soft Deletes is not merely a preference for data retention; it is a fundamental alignment with the physics of database storage engines.\\n\\n* **Hard Deletes** operate *against* the grain of the storage engine, forcing expensive rebalancing, causing fragmentation, and introducing dangerous locking contention in OLTP systems.\\n* **Soft Deletes** operate *with* the grain, converting destructive structural changes into efficient state updates. They provide the safety net required for modern applications, enabling undo capabilities, audit trails, and simplified synchronization.\\n\\nBy implementing Soft Deletes by design, combined with a disciplined Batch Hard Delete lifecycle, architects can build systems that remain performant, recoverable, and stable under scale."},{"id":"cache-stampede-thundering-herd","metadata":{"permalink":"/docs/blog/cache-stampede-thundering-herd","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/thundering-herd-problem.md","source":"@site/blog/thundering-herd-problem.md","title":"The Thundering Herd - Understanding and Solving the Cache Stampede","description":"In the world of high-scale distributed systems, caching is our best friend. It\u2019s the primary defense layer that protects our fragile databases from the onslaught of user traffic, ensuring low latency and high throughput.","date":"2025-12-11T00:00:00.000Z","tags":[],"readingTime":7.37,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"The Thundering Herd - Understanding and Solving the Cache Stampede","slug":"cache-stampede-thundering-herd","authors":"ashish","date":"2025-12-11T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","permalink":"/docs/blog/avoid-hard-deletes"},"nextItem":{"title":"The Transformer Architecture","permalink":"/docs/blog/transformer-architecture"}},"content":"In the world of high-scale distributed systems, caching is our best friend. It\u2019s the primary defense layer that protects our fragile databases from the onslaught of user traffic, ensuring low latency and high throughput.\\n\\nBut what happens when that defense layer momentarily fails exactly when you need it most?\\n\\nWelcome to the **Cache Stampede** problem\u2014also known as the \\"Thundering Herd.\\" It\'s a scenario where the very mechanism designed to speed up your system ends up bringing it to its knees.\\n\\nLet\'s dive into what it is, why it happens, and the practical techniques used by major systems (like CDNs) to solve it.\\n\\n## **The Typical Architecture**\\n\\nBefore things go wrong, let\'s look at how things go right.\\n\\nConsider a standard three-tier web architecture. We have API servers fronted by a load balancer. Sitting between these API servers and the \\"source of truth\\" (the Database) is a caching layer (like Redis or Memcached).\\n\\nThe goal of the cache is simple: serve data fast and reduce load on the database.\\n\\nHere is a high-level view of this architecture:\\n\\n```mermaid\\ngraph TD\\n    Client1[Client] --\x3e LB[Load Balancer]\\n    Client2[Client] --\x3e LB\\n    Client3[Client] --\x3e LB\\n    LB --\x3e API1[API Server 1]\\n    LB --\x3e API2[API Server 2]\\n    API1 <--\x3e Cache[(Distributed Cache)]\\n    API2 <--\x3e Cache\\n    API1 --\x3e DB[(Database)]\\n    API2 --\x3e DB\\n\\n    style Cache fill:#f9f,stroke:#333,stroke-width:2px\\n    style DB fill:#ccf,stroke:#333,stroke-width:2px\\n```\\n\\n\\n### **The \\"Happy Path\\" Flow**\\n\\nUnder normal operation, the flow for retrieving data is a standard \\"Read-Through\\" cache pattern:\\n\\n1. The API Server receives a request for a specific key.\\n2. It first checks the cache.\\n3. **Cache Hit:** If the data is there, return it immediately. (Fast\\\\!)\\n4. **Cache Miss:** If the data is *not* there:\\n    * Query the Database (Slow).\\n    * Populate the cache with the result for future requests.\\n    * Return the result to the client.\\n\\nHere is what this typically looks like in simple Java pseudo-code:\\n\\n```java\\npublic class SimpleDataService {\\n\\n    private Cache cache;  \\n    private Database db;\\n\\n    public String getData(String key) {  \\n        // 1. Try fetching from cache  \\n        String cachedValue = cache.get(key);  \\n        if (cachedValue != null) {  \\n            return cachedValue;  \\n        }\\n\\n        // 2. Cache miss - fetch from source of truth  \\n        String dbValue = db.query(key);\\n\\n        // 3. Backfill cache for next time  \\n        // Note: Often done with a Time-To-Live (TTL) to ensure freshness  \\n        cache.put(key, dbValue, Duration.ofMinutes(5)); \\n\\n        return dbValue;  \\n    }  \\n}\\n```\\n\\n\\n\\nThis works perfectly fine for normal traffic loads. The database only sees an occasional read when cache items expire.\\n\\n---\\n\\n## **The Problem: The Stampede Begins**\\n\\nNow, imagine a scenario where a piece of content goes viral.\\n\\nSuddenly, you have thousands of concurrent requests hitting your load balancer for the exact same resource key (e.g., product\\\\_details\\\\_123).\\n\\nIf the cache entry for product\\\\_details\\\\_123 has just expired, or perhaps was evicted due to memory pressure, you have a problem.\\n\\nIn that brief window of time\u2014perhaps just a few hundred milliseconds before the first request can refill the cache\u2014**every single concurrent request will result in a Cache Miss.**\\n\\nIf 5,000 requests arrive simultaneously for that missing key, all 5,000 requests will bypass the cache and bombard your database at the exact same moment.\\n\\nYour database, which was happily serving a few dozen requests per second, suddenly receives thousands. CPU spikes, connection pools become exhausted, queries time out, and the database might even crash. This is the Cache Stampede. It defeats the entire purpose of having a cache.\\n\\n## **The Solution: Request Hedging (Debouncing)**\\n\\nTo handle this, we need to ensure that when a \\"hot\\" cache key is missing, we don\'t let the entire herd stampede to the database. We need to designate a leader.\\n\\nWe leverage a technique known as **Request Hedging** or **Debouncing**.\\n\\nThe concept is straightforward: Out of the thousands of concurrent requests for the missing key, we allow **only one** to proceed to the database. All other requests for that same key must **wait** until that first request completes the job and refills the cache. Once the cache is refilled, the waiting requests can read the data from the cache and proceed.\\n\\nThe idea is simple, but as always, the devil is in the implementation details. How do we make requests wait efficiently and cleanly in a highly concurrent environment? Let\'s look at two approaches in Java.\\n\\n### **Approach 1: The Busy Wait (Spinlock)**\\n\\nIn this technique, if a thread finds a cache miss, it enters a loop where it continuously checks the cache again after sleeping for a tiny duration. It \\"spins\\" until data appears.\\n\\n#### **Pseudo-code (Busy Wait)**\\n\\n```java\\npublic class BusyWaitDataService {\\n\\n    private Cache cache;  \\n    private Database db;\\n\\n    public String getData(String key) {  \\n        String value = cache.get(key);  \\n          \\n        // If cache hit, return immediately  \\n        if (value != null) return value;\\n\\n        // Determine if I am the \\"leader\\" responsible for fetching.  \\n        // Use an atomic operation like \'setIfAbsent\' (NX) in Redis.  \\n        // Set a short TTL on this lock to prevent deadlocks if the service crashes.  \\n        boolean acquiredLock = cache.setIfAbsent(\\"lock::\\" + key, \\"locked\\", Duration.ofSeconds(5));\\n\\n        if (acquiredLock) {  \\n            try {  \\n                // I am the leader. Fetch from DB.  \\n                value = db.query(key);  \\n                cache.put(key, value, Duration.ofMinutes(5));  \\n            } finally {  \\n                // Release lock so others know fetching is done  \\n                cache.delete(\\"lock::\\" + key);  \\n            }  \\n        } else {  \\n            // I am a follower. Spin and wait.  \\n            while (value == null) {  \\n                try {  \\n                    // Sleep briefly to avoid hammering CPU  \\n                    Thread.sleep(50);   \\n                } catch (InterruptedException e) { Thread.currentThread().interrupt(); }  \\n                  \\n                // Check cache again  \\n                value = cache.get(key);  \\n                  \\n                // Optional: Check if lock still exists, if not, break and retry fetch  \\n            }  \\n        }  \\n        return value;  \\n    }  \\n}\\n```\\n\\n\\n#### **The Downside**\\n\\nAs the name suggests, this approach makes the CPU \\"busy.\\" Even though the threads are sleeping, the constant context switching and polling consumes precious CPU cycles. In high-load scenarios, this wasted CPU can become substantial. It works, but it\'s not elegant.\\n\\n### **Approach 2: The Wait/Notify Mechanism (JVM Locks/Futures)**\\n\\nA far more efficient approach is to use native concurrency constructs. Instead of polling, threads should block and go to sleep until they are explicitly notified that the data is ready.\\n\\nIn modern Java, a CompletableFuture combined with a ConcurrentHashMap is a robust way to implement this \\"promise\\" pattern without getting tangled in low-level monitor locks (synchronized/wait/notify).\\n\\n#### **Pseudo-code (Wait/Notify with Futures)**\\n\\nWe maintain a local map of \\"pending database operations.\\" If a request comes in and an operation is already pending for that key, we hook into that existing operation\'s future result.\\n\\n```java\\nimport java.util.concurrent.*;\\n\\npublic class BlogService {\\n   // \\"use threadsafe implementation here\\"\\n   // sem_map: Tracks which keys are currently being fetched\\n   private final ConcurrentHashMap<String, CountDownLatch> semMap = new ConcurrentHashMap<>();\\n\\n   // res_map: Temporary storage for followers to grab the result immediately\\n   // Note: In a real impl, this might need a TTL (e.g., \\"1 min\\" per your note) or explicit cleanup\\n   private final ConcurrentHashMap<String, String> resMap = new ConcurrentHashMap<>();\\n\\n   public String getBlog(String k) {\\n      // 1. Check main cache first\\n      String v = cache.get(k);\\n      if (v != null) {\\n         return v;\\n      }\\n\\n      // 2. Check Semaphore Map (Thread-safe check)\\n      // We attempt to create a \\"lock\\" (Latch) for this key.\\n      CountDownLatch myLatch = new CountDownLatch(1); // Starts \\"blocked\\"\\n\\n      // \\"s = sem_map.get(k)\\" equivalent using atomic putIfAbsent\\n      // If returns value: someone else is already fetching (we are follower)\\n      // If returns null: we successfully inserted (we are leader)\\n      CountDownLatch existingLatch = semMap.putIfAbsent(k, myLatch);\\n\\n      if (existingLatch != null) {\\n         // --- FOLLOWER PATH (\\"if s: s.wait()\\") ---\\n         try {\\n            // Wait for the leader to signal\\n            existingLatch.await();\\n            return resMap.get(k);\\n         } catch (InterruptedException e) {\\n            Thread.currentThread().interrupt();\\n            return null;\\n         }\\n      } else {\\n         // --- LEADER PATH (\\"else\\") ---\\n         try {\\n            v = db.query(k);\\n\\n            // \\"cache.put(k, v)\\"\\n            cache.put(k, v);\\n\\n            // \\"res_map[k] = v\\" (Temporary map)\\n            resMap.put(k, v);\\n\\n            return v;\\n         } finally {\\n            // Open the gate for followers\\n            myLatch.countDown();\\n\\n            //Cleanup lock\\n            semMap.remove(k);\\n\\n            // Optional: You might schedule resMap cleanup here or rely on TTL\\n         }\\n      }\\n   }\\n}\\n```\\n\\n\\n\\n#### **The Benefit**\\n\\nThis approach is highly efficient. Waiting threads are parked by the OS and consume virtually no CPU until the leader thread completes the future. It handles concurrency cleanly within a single JVM.\\n\\n### **The Pragmatic Scope: JVM vs. Distributed Locks**\\n\\nA sharp observer might notice a slight flaw in Approach 2. The guardrails of approach 2 exist within the memory of a *single* API server JVM.\\n\\nIf you have a fleet of 20 API servers behind your load balancer, and a stampede occurs, *one* request on *each* of the 20 servers will proceed to the database.\\n\\nInstead of 5,000 database hits, you will have 20 hits.\\n\\nIs this perfect? No. To reduce it to exactly one global hit, you would need a **Distributed Lock** system (using Redis, Zookeeper, or etcd) to coordinate locking across all 20 servers.\\n\\nHowever, distributed locks introduce significant complexity, latency, and a new point of failure.\\n\\n**In practice, the JVM-level solution is often the pragmatic choice.** Reducing 5,000 simultaneous requests down to 20 is usually sufficient to save the database. It\'s a massive improvement for relatively low implementation complexity.\\n\\nIt\'s worth noting that large-scale Content Delivery Networks (CDNs) like Cloudflare, Akamai, and Fastly use exactly this hedging technique at their edge locations to protect customer origin servers from getting overwhelmed when content goes viral."},{"id":"transformer-architecture","metadata":{"permalink":"/docs/blog/transformer-architecture","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/transformer-architecture.md","source":"@site/blog/transformer-architecture.md","title":"The Transformer Architecture","description":"The release of the \\"Attention is All You Need\\" paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the Transformer architecture, which powers the Generative AI explosion we see today.","date":"2025-12-11T00:00:00.000Z","tags":[],"readingTime":7.48,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"The Transformer Architecture","slug":"transformer-architecture","authors":"ashish","date":"2025-12-11T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The Thundering Herd - Understanding and Solving the Cache Stampede","permalink":"/docs/blog/cache-stampede-thundering-herd"},"nextItem":{"title":"Design an Online/Offline Indicator (Presence Service)","permalink":"/docs/blog/offline-online-indicator"}},"content":"The release of the \\"Attention is All You Need\\" paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the **Transformer** architecture, which powers the Generative AI explosion we see today.\\n\\nBut how does a Transformer actually \\"read\\" and \\"understand\\" a sentence?\\n\\nIn this post, we will walk through the architecture step-by-step, transforming raw text into understanding using the logic from the original paper.\\n\\n## The High-Level View: Encoder and Decoder\\n\\nAt its highest level, the Transformer is a statistical calculator. It doesn\'t \\"know\\" English or Java; it knows math. The architecture is split into two distinct parts:\\n\\n1.  **The Encoder:** Processes the input data.\\n2.  **The Decoder:** Generates the output.\\n\\nThey work in conjunction, though in modern models (like GPT), we often see decoder-only architectures. For this guide, we will follow the flow from Input (bottom) to Output (top).\\n\\n```mermaid\\ngraph TD\\n    Input[Input Text] --\x3e Tokenizer\\n    Tokenizer --\x3e Encoder\\n    Encoder --\x3e Decoder\\n    Decoder --\x3e Output[Output Probability]\\n    style Input fill:#f9f,stroke:#333,stroke-width:2px\\n    style Output fill:#9f9,stroke:#333,stroke-width:2px\\n```\\n\\n---\\n\\n## Step 1: Tokenization (Words to Numbers)\\n\\nMachine learning models cannot process raw text. They need numbers. Before passing a sentence like *\\"The teacher has the book\\"* into the model, we must **tokenize** it.\\n\\n### What is Tokenization?\\nSimply put, this converts words into numbers. Imagine a giant dictionary where every word has a unique ID.\\n* \\"The\\" -> 101\\n* \\"Teacher\\" -> 2045\\n* \\"Book\\" -> 3011\\n\\n:::tip AI Terminology\\n**Tokenizer:** A tool that breaks text into smaller chunks (tokens). These can be whole words or parts of words. Ideally, if you train a model with a specific tokenizer, you must use the exact same one when generating text.\\n:::\\n\\n```mermaid\\nflowchart LR\\n    A[Input: \'The teacher\'] --\x3e B(Tokenizer)\\n    B --\x3e C[Token IDs: 101, 2045]\\n```\\n\\n---\\n\\n## Step 2: Embeddings (Numbers to Meaning)\\n\\nNow we have a list of numbers (IDs), but numbers don\'t have \\"meaning.\\" To a computer, the number 100 and 101 are just close in value, but the words they represent might be totally unrelated.\\n\\nWe solve this with the **Embedding Layer**.\\n\\n### The Concept: High-Dimensional Space\\nEach Token ID is matched to a **Vector** (a list of numbers). In the original paper, this vector size was 512.\\n* Imagine a 3D graph (X, Y, Z).\\n* Words with similar meanings are plotted physically close to each other in this space.\\n* \\"King\\" and \\"Queen\\" would be close together. \\"Apple\\" and \\"Car\\" would be far apart.\\n\\nThe model calculates the distance (often as an angle) between these words to understand their relationship mathematically.\\n\\n```mermaid\\ngraph TD\\n    ID[Token ID: 2045] --\x3e Embedding[Embedding Layer]\\n    Embedding --\x3e Vector[Vector: 0.1, -0.5, 0.8, ...]\\n```\\n\\n---\\n\\n## Step 3: Positional Encoding (Adding Order)\\n\\nTransformers process input tokens in **parallel** (all at once), unlike RNNs which read word-by-word. This is great for speed, but it creates a problem: the model loses the concept of word order.\\n\\nTo the model, *\\"The teacher has the book\\"* and *\\"The book has the teacher\\"* look the same because the ingredients (words) are the same.\\n\\n### The Fix: Positional Encoding\\nWe add a \\"timestamp\\" or \\"position signature\\" to the word vectors.\\n* **Analogy:** Imagine throwing a stack of unnumbered pages into the air. If you don\'t write page numbers (positional encoding) on them first, you can\'t put the book back together.\\n\\n```mermaid\\nflowchart LR\\n    A[Word Vector] --\x3e C{Sum}\\n    B[Positional Vector] --\x3e C\\n    C --\x3e D[Final Input to Encoder]\\n```\\n\\n---\\n\\n## Step 4: Self-Attention (The Core Magic)\\n\\nThis is the \\"Attention\\" in \\"Attention is All You Need.\\"\\n\\nOnce the vectors enter the model, the **Self-Attention Layer** analyzes the relationships between tokens. It allows the model to look at a specific word and understand its context based on *every other word* in the sentence.\\n\\n### Example: \\"Who has the book?\\"\\nIn the sentence: *\\"The teacher gave the book to the student.\\"*\\n* The word **\\"Book\\"** needs to understand who has it (Teacher) and who receives it (Student).\\n* The attention mechanism creates strong connections (weights) between \\"Book,\\" \\"Teacher,\\" and \\"Student,\\" while ignoring less relevant words like \\"the.\\"\\n\\n```mermaid\\ngraph TD\\n    Word[Word: Book] -- Strong Attention --\x3e Teacher\\n    Word -- Strong Attention --\x3e Student\\n    Word -- Weak Attention --\x3e The\\n    style Word fill:#ff9,stroke:#333\\n```\\n\\n---\\n\\n## Step 5: Multi-Head Attention (Many Perspectives)\\n\\nThe model doesn\'t just do self-attention once. It uses **Multi-Head Attention**.\\n\\n### What is a \\"Head\\"?\\nThink of a \\"Head\\" as a different lens or filter. The model runs 12 to 100 of these heads in parallel. Each head learns a different aspect of language randomly during training.\\n\\n* **Head 1 (The Grammar Lens):** Might focus on Subject-Verb agreement.\\n* **Head 2 (The Rhyme Lens):** Might focus on phonetics or poetry.\\n* **Head 3 (The Context Lens):** Might focus on relationships between people (Teacher/Student).\\n\\nThe results of all these heads are combined to give the model a complete understanding of the text.\\n\\n---\\n\\n## Step 6: The Output (Logits and Probabilities)\\n\\nAfter passing through the Feed Forward Network, the model produces an output. But it doesn\'t output a word immediately; it outputs a **Vector of Logits**.\\n\\n### Breaking Down the Jargon\\n\\n1.  **Logits:** These are raw, unnormalized scores. The model scores every single word in its dictionary on how likely it is to be the next word.\\n    * *Example:* Apple: 5.0, Ball: 1.2, Cat: -3.0\\n2.  **Softmax Layer:** This turns those raw scores (Logits) into probabilities (Percentages).\\n    * *Example:* Apple: 98%, Ball: 1.9%, Cat: 0.1%\\n\\nThe word with the highest probability is selected as the predicted token.\\n\\n```mermaid\\ngraph TD\\n    FFN[Feed Forward Network] --\x3e Logits[Vector of Logits]\\n    Logits --\x3e Softmax[Softmax Layer]\\n    Softmax --\x3e Probability[Probabilities: Apple 98%]\\n    Probability --\x3e Final[Final Word: Apple]\\n```\\n\\n---\\n\\n## Putting It All Together: An End-to-End Example\\n\\nWe have looked at the components individually. Now, let\'s watch them work together in a real scenario.\\n\\n**The Task:** Translate the French phrase *\\"J\'aime l\'apprentissage automatique\\"* into English.\\n\\n### Phase 1: The Encoder (Reading & Understanding)\\n\\nFirst, the model needs to understand the input. This happens entirely in the **Encoder**.\\n\\n1.  **Tokenization:** The raw text is broken down into numbers.\\n2.  **Embedding & Position:** These numbers are turned into vectors with position information.\\n3.  **Multi-Head Attention:** The model analyzes the relationships. For example, it understands that *\\"apprentissage\\"* (learning) and *\\"automatique\\"* (machine) are strongly related in this context.\\n4.  **Deep Representation:** The output of the Encoder is not text, but a rich matrix of vectors that represents the *meaning* of the French sentence.\\n\\n```mermaid\\ngraph TD\\n    Input[Input: Jaime lapprentissage automatique] --\x3e Tokenizer\\n    Tokenizer --\x3e Tokens[Tokens: 101, 405, 302, ...]\\n    Tokens --\x3e Encoder[ENCODER BLOCK]\\n    Encoder --\x3e Context[DEEP CONTEXT REPRESENTATION]\\n    style Context fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\\n    style Input fill:#bbf,stroke:#333\\n```\\n\\n---\\n\\n### Phase 2: The Decoder (Generating the Translation)\\n\\nThis is where the magic happens. The **Decoder** takes the \\"Deep Context\\" from the Encoder and generates the English translation one word at a time. This is a **loop**.\\n\\n#### Step A: The Trigger\\nThe process starts by feeding a special **Start-of-Sequence (SOS)** token into the Decoder.\\n\\n#### Step B: The Prediction Loop\\n1.  The Decoder looks at the **Context** (from the Encoder) and the **Inputs so far** (initially just `<SOS>`).\\n2.  It runs Self-Attention to see what English words match the French meaning.\\n3.  It outputs probabilities via Softmax.\\n4.  The word with the highest score is selected: **\\"I\\"**.\\n\\n#### Step C: The Feedback Loop\\nThe model doesn\'t stop. It takes the new word (**\\"I\\"**) and feeds it back into the input. Now the input is `<SOS> + \\"I\\"`.\\n* *Cycle 2 Output:* **\\"love\\"**\\n* *Cycle 3 Output:* **\\"machine\\"**\\n* *Cycle 4 Output:* **\\"learning\\"**\\n\\n#### Step D: The Stop Condition\\nFinally, the model predicts a special **End-of-Sequence (EOS)** token. This tells the system to stop generating.\\n\\n```mermaid\\nsequenceDiagram\\n    participant E as Encoder Context\\n    participant D as Decoder\\n    participant O as Output\\n\\n    Note over D: Loop Starts\\n    D->>D: Input: [Start Token]\\n    E->>D: Influences Attention\\n    D->>O: Predicts: \\"I\\"\\n    \\n    D->>D: Input: [Start Token] + \\"I\\"\\n    E->>D: Influences Attention\\n    D->>O: Predicts: \\"love\\"\\n    \\n    D->>D: Input: [Start Token] + \\"I\\" + \\"love\\"\\n    E->>D: Influences Attention\\n    D->>O: Predicts: \\"machine learning\\"\\n    \\n    D->>D: Input: ... + \\"machine learning\\"\\n    D->>O: Predicts: [End Token]\\n    Note over O: Generation Complete\\n```\\n\\n---\\n\\n## Transformer Variations: The Family Tree\\n\\nWhile the example above used both the Encoder and Decoder (Sequence-to-Sequence), modern AI has branched into three distinct families based on which parts they keep.\\n\\n| Type | Architecture | Best For | Popular Models |\\n| :--- | :--- | :--- | :--- |\\n| **Encoder-Only** | Inputs & Outputs are same length. | Classification, Sentiment Analysis, Entity Recognition. | **BERT** |\\n| **Encoder-Decoder** | Input & Output lengths vary. | Translation, Text Summarization. | **BART**, **T5** |\\n| **Decoder-Only** | Generates new tokens from a prompt. | General Text Generation (Chatbots, Code, Creative Writing). | **GPT-4**, **LLaMA**, **Bloom** |\\n\\n---\\n\\n## Summary\\n\\nThe Transformer architecture changed AI by allowing models to:\\n1.  Process data in parallel (Speed).\\n2.  Understand the context of every word relative to every other word (Attention).\\n3.  Learn multiple nuances of language simultaneously (Multi-Head).\\n\\nThis architecture is the foundation upon which tools like ChatGPT and Claude are built.\\n\\n## References: \\n1. https://arxiv.org/abs/1706.03762 \\n2. https://learn.deeplearning.ai"},{"id":"offline-online-indicator","metadata":{"permalink":"/docs/blog/offline-online-indicator","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/presence-design.md","source":"@site/blog/presence-design.md","title":"Design an Online/Offline Indicator (Presence Service)","description":"Designing a scalable presence system for 1 billion users.","date":"2025-12-10T10:55:15.000Z","tags":[],"readingTime":6.08,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"slug":"offline-online-indicator","title":"Design an Online/Offline Indicator (Presence Service)","sidebar_label":"Presence Service","description":"Designing a scalable presence system for 1 billion users.","authors":"ashish"},"unlisted":false,"prevItem":{"title":"The Transformer Architecture","permalink":"/docs/blog/transformer-architecture"}},"content":"Designing a system to indicate if a user is **Online** or **Offline** (and their last seen timestamp) sounds simple on the surface, but becomes a massive engineering challenge when scaling to **1 Billion users**.\\n\\nIn this post, we will breakdown the design of a Presence Service suitable for a massive social network or chat application like WhatsApp or Facebook Messenger.\\n\\n## Problem Statement\\n\\nWe need to build a service that indicates the availability status of a user\'s connections.\\n\\n**The Functional Requirements:**\\n1.  Indicate if a friend/connection is currently **Online**.\\n2.  If offline, display the **Last Seen** timestamp.\\n\\n**The Scale:**\\n* **Total Users:** ~1 Billion.\\n* **Concurrent Online Users:** ~500 Million.\\n* **Latency:** The status needs to be near real-time.\\n\\n## High-Level Strategy: Push vs. Pull\\n\\nHow do we keep the server updated about the client\'s status?\\n\\n### Option 1: Pull (Polling)\\nThe server periodically connects to the client to ask, \\"Are you there?\\"\\n* **Verdict:** \u274c **Impossible.**\\n* **Reasoning:** In a mobile/NAT environment, servers cannot initiate connections to clients easily. Furthermore, polling 1B users is computationally wasteful.\\n\\n### Option 2: Push (Heartbeat)\\nThe client sends a signal to the server periodically saying, \\"I am alive.\\"\\n* **Verdict:** \u2705 **Selected.**\\n* **Reasoning:** This is the standard pattern for presence. If the server stops receiving heartbeats (after a timeout threshold), the user is marked offline.\\n\\n---\\n\\n## The Protocol: HTTP vs. WebSockets\\n\\nWe established a \\"Push\\" model, but how should the client push this data?\\n\\n### 1. REST/HTTP Based\\nThe client sends a `POST /health` request every *N* seconds.\\n\\n**Why this fails at scale:**\\n* **Overhead:** HTTP is stateless. Every heartbeat requires a full 3-way TCP handshake (if not using keep-alive efficiently), SSL handshake overhead, and heavy HTTP headers.\\n* **Traffic:** With 500M concurrent users sending a heartbeat every 10 seconds, that is **50 Million requests per second**. Most of the data transferred would be HTTP headers, not the actual status payload.\\n\\n### 2. Persistent WebSockets\\nThe client opens a long-lived, bi-directional connection with the server.\\n\\n**Why this is the winner:**\\n* **Reduced Overhead:** Once the connection is established, data frames have minimal overhead (just a few bytes). There are no repeated headers or handshakes.\\n* **Real-time:** The server knows *immediately* if a connection is severed (TCP FIN or RST).\\n* **Bi-directional:** It allows the server to push status updates of *friends* back to the user over the same channel.\\n\\n:::caution The \\"Disconnect\\" Fallacy\\nA common misconception is that WebSockets natively handle all disconnects via an `onDisconnect` event.\\n* **Clean Disconnect:** If a user clicks \\"Logout\\", the client sends a TCP FIN. The server knows immediately.\\n* **Dirty Disconnect:** If the user loses internet connectivity or the connection is broken for any reason, **the server receives nothing.**\\n* **The Fix:** We must implement an **Application-Level Heartbeat**. If the server doesn\'t receive a \\"Ping\\" frame or message within $N$ seconds, it forcibly closes the socket and marks the user offline.\\n  :::\\n\\n:::info Scaling WebSockets\\nScaling persistent connections is harder than scaling stateless HTTP.\\n1.  **OS Limits:** You must tune the kernel to allow >65k open file descriptors (ephemeral ports) per server.\\n2.  **Load Balancing:** You need a Layer 7 Load Balancer that supports \\"Sticky Sessions\\" effectively, though for a pure presence service, state can be externalized.\\n3.  **Memory:** Holding 500M open connections requires massive RAM across your fleet of connection handlers (Gateway Service).\\n    :::\\n\\n---\\n\\n## Database Design & Estimation\\n\\nThe compute layer is pretty. All it has to do is upon receiving the request, perform a key value based insert or lookup and simply return back to the client. The complexity lies in the **storage layer**. \\n\\n### Query Patterns\\n1.  **Write (Heavy):** Update User *A*\'s timestamp (Heartbeat).\\n2.  **Read (Heavy):** Get status for User *A*\'s friends/connections when User *A* opens the app.\\n\\n### Data Schema\\nWe need a simple Key-Value pair.\\n\\n| Field  | Type    | Size |\\n|:-------|:--------|-----|\\n| UserID | Integer |  4 Bytes |\\n| LastSeen | Epoch (Int) | 4 Bytes |\\n| **Total** | | **8 Bytes** |\\n\\n### Capacity Planning\\nWith 1 Billion users, do we need massive storage?\\n\\n1,000,000,000 users * 8 bytes = 8 GB\\n\\n:::tip Insight\\nWe only need **~8 GB** of storage to hold the state of every user on the planet. This entire dataset can fit into the RAM of a single modern server instance.\\n:::\\n\\n---\\n\\n## Managing \\"Online\\" State Lifecycle\\n\\nHow do we decide when to switch a user from Online to Offline? We have three strategies.\\n\\n### 1. The Cron Job Reaper\\nA background process scans the database every few minutes and deletes entries older than $N$ minutes.\\n* **Pros:** Keeps DB clean eventually.\\n* **Cons:** **Terrible at scale.** Scanning a table of 500M rows every minute creates massive read pressure and locking issues. The \\"Offline\\" status will always be laggy.\\n\\n### 2. Connection Events (Explicit Disconnect)\\nLeverage WebSocket callbacks (`onConnect`, `onDisconnect`) to update the DB.\\n* **Pros:** extremely efficient. Writes only happen on state changes.\\n* **Cons:** Unreliable. If a user loses network (enters a tunnel) or the app crashes, the `onDisconnect` event might never fire sent to the server. The user will appear \\"Online\\" forever (a Zombie session).\\n\\n### 3. Database TTL (Time-To-Live)\\nUse the database\'s native feature to auto-expire keys. The Heartbeat simply resets the TTL.\\n* **Pros:** Handles \\"unclean\\" disconnects gracefully. If the heartbeat stops, the key vanishes automatically. No manual cleanup required.\\n* **Cons:** Moderate write load (every heartbeat is a write to reset the TTL).\\n\\n**Verdict:** We will use **Option 3 (TTL)** as the primary mechanism, potentially optimized by Option 2 (explicitly deleting the key on a clean logout to avoid the TTL wait).\\n\\n## Database Selection: Redis vs. DynamoDB\\n\\nWe need a Key-Value store that handles massive write throughput.\\n\\n**The Math:**\\n* 500 Million concurrent users.\\n* Heartbeat interval: 30 seconds.\\n* Throughput = $500,000,000 / 30 \\\\approx$ **16.6 Million Writes/Second**.\\n\\n### Candidate 1: Amazon DynamoDB\\n* **Pros:** Serverless, high durability, multi-region replication (Global Tables).\\n* **Cons:** **Cost and Hot Partitions.**\\n    * Cost: DynamoDB charges by **Write Capacity Units (WCUs)**.\\n        * 16.6 Million writes/sec = **16.6 Million WCUs**.\\n        * Cost per WCU (Provisioned) $\\\\approx \\\\$0.00065$ / hour.\\n        * **Hourly Cost:** $\\\\$10,833$.\\n        * **Monthly Cost:** **~$7.9 Million / Month**.\\n    * Hot Partition: In DynamoDB, a single partition is strictly limited to **1,000 WCUs**. If 2,000 users map to the same partition key, requests get throttled.\\n\\n### Candidate 2: Redis (The Winner)\\nRedis is an in-memory store. We are limited by CPU/Network throughput per node.\\n* **Pros:**\\n    * **In-Memory Speed:** Sub-millisecond reads/writes.\\n    * **Native TTL:** Redis handles key expiration natively and efficiently.\\n    * **Cost Effective:**\\n        * Redis is an in-memory store. We are limited by CPU/Network throughput per node.\\n        * A single robust Redis node (e.g., AWS `r7g.xlarge`) can handle **~600,000 writes/sec**. (**Benchmark**: https://aws.plainenglish.io/aws-elasticache-a-performance-and-cost-analysis-of-redis-7-1-vs-valkey-7-2-bfac4fb5c22a)\\n        * Nodes required: $16,600,000 / 600,000 \\\\approx$ **28 Shards**.\\n        * Cost per node $\\\\approx \\\\$0.30$ / hour.\\n        * **Monthly Cost:** $28 * $0.30 * 730 hours = **~$6132 / Month**.\\n* **Cons:**\\n    * **Persistence:** If Redis crashes, we lose \\"Last Seen\\" data (unless AOF mode is enabled, which slows performance).\\n* **Mitigation:** For a Presence system, *ephemeral* data loss is acceptable. If Redis crashes, users briefly appear offline until their next heartbeat (seconds later).\\n\\n:::tip Cost Epiphany\\nBy choosing Redis over DynamoDB for this high-throughput/ephemeral workload, we save the company roughly **$7.89 Million per month**.\\n:::\\n\\n## Final Architecture\\n\\n```mermaid\\nflowchart TD\\n    Client[User Client] --\x3e|WebSocket Connection| LB[Load Balancer]\\n    LB --\x3e|Sticky Session| WS[WebSocket Server]\\n    \\n    subgraph Data Layer\\n    WS --\x3e|HEARTBEAT every 10s| Redis[(Redis Cluster)]\\n    end\\n    \\n    note[Redis Key: UserID <br/> Value: Timestamp <br/> TTL: 30s] -.-> Redis\\n```"}]}}')}}]);