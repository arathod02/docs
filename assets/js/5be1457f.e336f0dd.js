"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[9258],{2253:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"offline-online-indicator","metadata":{"permalink":"/docs/blog/offline-online-indicator","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/presence-design.md","source":"@site/blog/presence-design.md","title":"Design an Online/Offline Indicator (Presence Service)","description":"Designing a scalable presence system for 1 billion users.","date":"2025-12-10T10:55:15.000Z","tags":[],"readingTime":6.08,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"slug":"offline-online-indicator","title":"Design an Online/Offline Indicator (Presence Service)","sidebar_label":"Presence Service","description":"Designing a scalable presence system for 1 billion users.","authors":"ashish"},"unlisted":false},"content":"Designing a system to indicate if a user is **Online** or **Offline** (and their last seen timestamp) sounds simple on the surface, but becomes a massive engineering challenge when scaling to **1 Billion users**.\\n\\nIn this post, we will breakdown the design of a Presence Service suitable for a massive social network or chat application like WhatsApp or Facebook Messenger.\\n\\n## Problem Statement\\n\\nWe need to build a service that indicates the availability status of a user\'s connections.\\n\\n**The Functional Requirements:**\\n1.  Indicate if a friend/connection is currently **Online**.\\n2.  If offline, display the **Last Seen** timestamp.\\n\\n**The Scale:**\\n* **Total Users:** ~1 Billion.\\n* **Concurrent Online Users:** ~500 Million.\\n* **Latency:** The status needs to be near real-time.\\n\\n## High-Level Strategy: Push vs. Pull\\n\\nHow do we keep the server updated about the client\'s status?\\n\\n### Option 1: Pull (Polling)\\nThe server periodically connects to the client to ask, \\"Are you there?\\"\\n* **Verdict:** \u274c **Impossible.**\\n* **Reasoning:** In a mobile/NAT environment, servers cannot initiate connections to clients easily. Furthermore, polling 1B users is computationally wasteful.\\n\\n### Option 2: Push (Heartbeat)\\nThe client sends a signal to the server periodically saying, \\"I am alive.\\"\\n* **Verdict:** \u2705 **Selected.**\\n* **Reasoning:** This is the standard pattern for presence. If the server stops receiving heartbeats (after a timeout threshold), the user is marked offline.\\n\\n---\\n\\n## The Protocol: HTTP vs. WebSockets\\n\\nWe established a \\"Push\\" model, but how should the client push this data?\\n\\n### 1. REST/HTTP Based\\nThe client sends a `POST /health` request every *N* seconds.\\n\\n**Why this fails at scale:**\\n* **Overhead:** HTTP is stateless. Every heartbeat requires a full 3-way TCP handshake (if not using keep-alive efficiently), SSL handshake overhead, and heavy HTTP headers.\\n* **Traffic:** With 500M concurrent users sending a heartbeat every 10 seconds, that is **50 Million requests per second**. Most of the data transferred would be HTTP headers, not the actual status payload.\\n\\n### 2. Persistent WebSockets\\nThe client opens a long-lived, bi-directional connection with the server.\\n\\n**Why this is the winner:**\\n* **Reduced Overhead:** Once the connection is established, data frames have minimal overhead (just a few bytes). There are no repeated headers or handshakes.\\n* **Real-time:** The server knows *immediately* if a connection is severed (TCP FIN or RST).\\n* **Bi-directional:** It allows the server to push status updates of *friends* back to the user over the same channel.\\n\\n:::caution The \\"Disconnect\\" Fallacy\\nA common misconception is that WebSockets natively handle all disconnects via an `onDisconnect` event.\\n* **Clean Disconnect:** If a user clicks \\"Logout\\", the client sends a TCP FIN. The server knows immediately.\\n* **Dirty Disconnect:** If the user loses internet connectivity or the connection is broken for any reason, **the server receives nothing.**\\n* **The Fix:** We must implement an **Application-Level Heartbeat**. If the server doesn\'t receive a \\"Ping\\" frame or message within $N$ seconds, it forcibly closes the socket and marks the user offline.\\n  :::\\n\\n:::info Scaling WebSockets\\nScaling persistent connections is harder than scaling stateless HTTP.\\n1.  **OS Limits:** You must tune the kernel to allow >65k open file descriptors (ephemeral ports) per server.\\n2.  **Load Balancing:** You need a Layer 7 Load Balancer that supports \\"Sticky Sessions\\" effectively, though for a pure presence service, state can be externalized.\\n3.  **Memory:** Holding 500M open connections requires massive RAM across your fleet of connection handlers (Gateway Service).\\n    :::\\n\\n---\\n\\n## Database Design & Estimation\\n\\nThe compute layer is pretty. All it has to do is upon receiving the request, perform a key value based insert or lookup and simply return back to the client. The complexity lies in the **storage layer**. \\n\\n### Query Patterns\\n1.  **Write (Heavy):** Update User *A*\'s timestamp (Heartbeat).\\n2.  **Read (Heavy):** Get status for User *A*\'s friends/connections when User *A* opens the app.\\n\\n### Data Schema\\nWe need a simple Key-Value pair.\\n\\n| Field  | Type    | Size |\\n|:-------|:--------|-----|\\n| UserID | Integer |  4 Bytes |\\n| LastSeen | Epoch (Int) | 4 Bytes |\\n| **Total** | | **8 Bytes** |\\n\\n### Capacity Planning\\nWith 1 Billion users, do we need massive storage?\\n\\n1,000,000,000 users * 8 bytes = 8 GB\\n\\n:::tip Insight\\nWe only need **~8 GB** of storage to hold the state of every user on the planet. This entire dataset can fit into the RAM of a single modern server instance.\\n:::\\n\\n---\\n\\n## Managing \\"Online\\" State Lifecycle\\n\\nHow do we decide when to switch a user from Online to Offline? We have three strategies.\\n\\n### 1. The Cron Job Reaper\\nA background process scans the database every few minutes and deletes entries older than $N$ minutes.\\n* **Pros:** Keeps DB clean eventually.\\n* **Cons:** **Terrible at scale.** Scanning a table of 500M rows every minute creates massive read pressure and locking issues. The \\"Offline\\" status will always be laggy.\\n\\n### 2. Connection Events (Explicit Disconnect)\\nLeverage WebSocket callbacks (`onConnect`, `onDisconnect`) to update the DB.\\n* **Pros:** extremely efficient. Writes only happen on state changes.\\n* **Cons:** Unreliable. If a user loses network (enters a tunnel) or the app crashes, the `onDisconnect` event might never fire sent to the server. The user will appear \\"Online\\" forever (a Zombie session).\\n\\n### 3. Database TTL (Time-To-Live)\\nUse the database\'s native feature to auto-expire keys. The Heartbeat simply resets the TTL.\\n* **Pros:** Handles \\"unclean\\" disconnects gracefully. If the heartbeat stops, the key vanishes automatically. No manual cleanup required.\\n* **Cons:** Moderate write load (every heartbeat is a write to reset the TTL).\\n\\n**Verdict:** We will use **Option 3 (TTL)** as the primary mechanism, potentially optimized by Option 2 (explicitly deleting the key on a clean logout to avoid the TTL wait).\\n\\n## Database Selection: Redis vs. DynamoDB\\n\\nWe need a Key-Value store that handles massive write throughput.\\n\\n**The Math:**\\n* 500 Million concurrent users.\\n* Heartbeat interval: 30 seconds.\\n* Throughput = $500,000,000 / 30 \\\\approx$ **16.6 Million Writes/Second**.\\n\\n### Candidate 1: Amazon DynamoDB\\n* **Pros:** Serverless, high durability, multi-region replication (Global Tables).\\n* **Cons:** **Cost and Hot Partitions.**\\n    * Cost: DynamoDB charges by **Write Capacity Units (WCUs)**.\\n        * 16.6 Million writes/sec = **16.6 Million WCUs**.\\n        * Cost per WCU (Provisioned) $\\\\approx \\\\$0.00065$ / hour.\\n        * **Hourly Cost:** $\\\\$10,833$.\\n        * **Monthly Cost:** **~$7.9 Million / Month**.\\n    * Hot Partition: In DynamoDB, a single partition is strictly limited to **1,000 WCUs**. If 2,000 users map to the same partition key, requests get throttled.\\n\\n### Candidate 2: Redis (The Winner)\\nRedis is an in-memory store. We are limited by CPU/Network throughput per node.\\n* **Pros:**\\n    * **In-Memory Speed:** Sub-millisecond reads/writes.\\n    * **Native TTL:** Redis handles key expiration natively and efficiently.\\n    * **Cost Effective:**\\n        * Redis is an in-memory store. We are limited by CPU/Network throughput per node.\\n        * A single robust Redis node (e.g., AWS `r7g.xlarge`) can handle **~600,000 writes/sec**. (**Benchmark**: https://aws.plainenglish.io/aws-elasticache-a-performance-and-cost-analysis-of-redis-7-1-vs-valkey-7-2-bfac4fb5c22a)\\n        * Nodes required: $16,600,000 / 600,000 \\\\approx$ **28 Shards**.\\n        * Cost per node $\\\\approx \\\\$0.30$ / hour.\\n        * **Monthly Cost:** $28 * $0.30 * 730 hours = **~$6132 / Month**.\\n* **Cons:**\\n    * **Persistence:** If Redis crashes, we lose \\"Last Seen\\" data (unless AOF mode is enabled, which slows performance).\\n* **Mitigation:** For a Presence system, *ephemeral* data loss is acceptable. If Redis crashes, users briefly appear offline until their next heartbeat (seconds later).\\n\\n:::tip Cost Epiphany\\nBy choosing Redis over DynamoDB for this high-throughput/ephemeral workload, we save the company roughly **$7.89 Million per month**.\\n:::\\n\\n## Final Architecture\\n\\n```mermaid\\nflowchart TD\\n    Client[User Client] --\x3e|WebSocket Connection| LB[Load Balancer]\\n    LB --\x3e|Sticky Session| WS[WebSocket Server]\\n    \\n    subgraph Data Layer\\n    WS --\x3e|HEARTBEAT every 10s| Redis[(Redis Cluster)]\\n    end\\n    \\n    note[Redis Key: UserID <br/> Value: Timestamp <br/> TTL: 30s] -.-> Redis"}]}}')}}]);