"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[1235],{3869:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>c,frontMatter:()=>o,metadata:()=>i,toc:()=>h});var i=t(4604),s=t(4848),r=t(8453);const o={title:"The Transformer Architecture",slug:"transformer-architecture",authors:"ashish",date:new Date("2025-12-11T00:00:00.000Z")},l=void 0,a={authorsImageUrls:[void 0]},h=[{value:"The High-Level View: Encoder and Decoder",id:"the-high-level-view-encoder-and-decoder",level:2},{value:"Step 1: Tokenization (Words to Numbers)",id:"step-1-tokenization-words-to-numbers",level:2},{value:"What is Tokenization?",id:"what-is-tokenization",level:3},{value:"Step 2: Embeddings (Numbers to Meaning)",id:"step-2-embeddings-numbers-to-meaning",level:2},{value:"The Concept: High-Dimensional Space",id:"the-concept-high-dimensional-space",level:3},{value:"Step 3: Positional Encoding (Adding Order)",id:"step-3-positional-encoding-adding-order",level:2},{value:"The Fix: Positional Encoding",id:"the-fix-positional-encoding",level:3},{value:"Step 4: Self-Attention (The Core Magic)",id:"step-4-self-attention-the-core-magic",level:2},{value:"Example: &quot;Who has the book?&quot;",id:"example-who-has-the-book",level:3},{value:"Step 5: Multi-Head Attention (Many Perspectives)",id:"step-5-multi-head-attention-many-perspectives",level:2},{value:"What is a &quot;Head&quot;?",id:"what-is-a-head",level:3},{value:"Step 6: The Output (Logits and Probabilities)",id:"step-6-the-output-logits-and-probabilities",level:2},{value:"Breaking Down the Jargon",id:"breaking-down-the-jargon",level:3},{value:"Putting It All Together: An End-to-End Example",id:"putting-it-all-together-an-end-to-end-example",level:2},{value:"Phase 1: The Encoder (Reading &amp; Understanding)",id:"phase-1-the-encoder-reading--understanding",level:3},{value:"Phase 2: The Decoder (Generating the Translation)",id:"phase-2-the-decoder-generating-the-translation",level:3},{value:"Step A: The Trigger",id:"step-a-the-trigger",level:4},{value:"Step B: The Prediction Loop",id:"step-b-the-prediction-loop",level:4},{value:"Step C: The Feedback Loop",id:"step-c-the-feedback-loop",level:4},{value:"Step D: The Stop Condition",id:"step-d-the-stop-condition",level:4},{value:"Transformer Variations: The Family Tree",id:"transformer-variations-the-family-tree",level:2},{value:"Summary",id:"summary",level:2},{value:"References:",id:"references",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:['The release of the "Attention is All You Need" paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the ',(0,s.jsx)(n.strong,{children:"Transformer"})," architecture, which powers the Generative AI explosion we see today."]}),"\n",(0,s.jsx)(n.p,{children:'But how does a Transformer actually "read" and "understand" a sentence?'}),"\n",(0,s.jsx)(n.p,{children:"In this post, we will walk through the architecture step-by-step, transforming raw text into understanding using the logic from the original paper."}),"\n",(0,s.jsx)(n.h2,{id:"the-high-level-view-encoder-and-decoder",children:"The High-Level View: Encoder and Decoder"}),"\n",(0,s.jsx)(n.p,{children:'At its highest level, the Transformer is a statistical calculator. It doesn\'t "know" English or Java; it knows math. The architecture is split into two distinct parts:'}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The Encoder:"})," Processes the input data."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"The Decoder:"})," Generates the output."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"They work in conjunction, though in modern models (like GPT), we often see decoder-only architectures. For this guide, we will follow the flow from Input (bottom) to Output (top)."}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    Input[Input Text] --\x3e Tokenizer\n    Tokenizer --\x3e Encoder\n    Encoder --\x3e Decoder\n    Decoder --\x3e Output[Output Probability]\n    style Input fill:#f9f,stroke:#333,stroke-width:2px\n    style Output fill:#9f9,stroke:#333,stroke-width:2px"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-1-tokenization-words-to-numbers",children:"Step 1: Tokenization (Words to Numbers)"}),"\n",(0,s.jsxs)(n.p,{children:["Machine learning models cannot process raw text. They need numbers. Before passing a sentence like ",(0,s.jsx)(n.em,{children:'"The teacher has the book"'})," into the model, we must ",(0,s.jsx)(n.strong,{children:"tokenize"})," it."]}),"\n",(0,s.jsx)(n.h3,{id:"what-is-tokenization",children:"What is Tokenization?"}),"\n",(0,s.jsx)(n.p,{children:"Simply put, this converts words into numbers. Imagine a giant dictionary where every word has a unique ID."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'"The" -> 101'}),"\n",(0,s.jsx)(n.li,{children:'"Teacher" -> 2045'}),"\n",(0,s.jsx)(n.li,{children:'"Book" -> 3011'}),"\n"]}),"\n",(0,s.jsx)(n.admonition,{title:"AI Terminology",type:"tip",children:(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tokenizer:"})," A tool that breaks text into smaller chunks (tokens). These can be whole words or parts of words. Ideally, if you train a model with a specific tokenizer, you must use the exact same one when generating text."]})}),"\n",(0,s.jsx)(n.mermaid,{value:"flowchart LR\n    A[Input: 'The teacher'] --\x3e B(Tokenizer)\n    B --\x3e C[Token IDs: 101, 2045]"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-2-embeddings-numbers-to-meaning",children:"Step 2: Embeddings (Numbers to Meaning)"}),"\n",(0,s.jsx)(n.p,{children:'Now we have a list of numbers (IDs), but numbers don\'t have "meaning." To a computer, the number 100 and 101 are just close in value, but the words they represent might be totally unrelated.'}),"\n",(0,s.jsxs)(n.p,{children:["We solve this with the ",(0,s.jsx)(n.strong,{children:"Embedding Layer"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"the-concept-high-dimensional-space",children:"The Concept: High-Dimensional Space"}),"\n",(0,s.jsxs)(n.p,{children:["Each Token ID is matched to a ",(0,s.jsx)(n.strong,{children:"Vector"})," (a list of numbers). In the original paper, this vector size was 512."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Imagine a 3D graph (X, Y, Z)."}),"\n",(0,s.jsx)(n.li,{children:"Words with similar meanings are plotted physically close to each other in this space."}),"\n",(0,s.jsx)(n.li,{children:'"King" and "Queen" would be close together. "Apple" and "Car" would be far apart.'}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The model calculates the distance (often as an angle) between these words to understand their relationship mathematically."}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    ID[Token ID: 2045] --\x3e Embedding[Embedding Layer]\n    Embedding --\x3e Vector[Vector: 0.1, -0.5, 0.8, ...]"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-3-positional-encoding-adding-order",children:"Step 3: Positional Encoding (Adding Order)"}),"\n",(0,s.jsxs)(n.p,{children:["Transformers process input tokens in ",(0,s.jsx)(n.strong,{children:"parallel"})," (all at once), unlike RNNs which read word-by-word. This is great for speed, but it creates a problem: the model loses the concept of word order."]}),"\n",(0,s.jsxs)(n.p,{children:["To the model, ",(0,s.jsx)(n.em,{children:'"The teacher has the book"'})," and ",(0,s.jsx)(n.em,{children:'"The book has the teacher"'})," look the same because the ingredients (words) are the same."]}),"\n",(0,s.jsx)(n.h3,{id:"the-fix-positional-encoding",children:"The Fix: Positional Encoding"}),"\n",(0,s.jsx)(n.p,{children:'We add a "timestamp" or "position signature" to the word vectors.'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Analogy:"})," Imagine throwing a stack of unnumbered pages into the air. If you don't write page numbers (positional encoding) on them first, you can't put the book back together."]}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:"flowchart LR\n    A[Word Vector] --\x3e C{Sum}\n    B[Positional Vector] --\x3e C\n    C --\x3e D[Final Input to Encoder]"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-4-self-attention-the-core-magic",children:"Step 4: Self-Attention (The Core Magic)"}),"\n",(0,s.jsx)(n.p,{children:'This is the "Attention" in "Attention is All You Need."'}),"\n",(0,s.jsxs)(n.p,{children:["Once the vectors enter the model, the ",(0,s.jsx)(n.strong,{children:"Self-Attention Layer"})," analyzes the relationships between tokens. It allows the model to look at a specific word and understand its context based on ",(0,s.jsx)(n.em,{children:"every other word"})," in the sentence."]}),"\n",(0,s.jsx)(n.h3,{id:"example-who-has-the-book",children:'Example: "Who has the book?"'}),"\n",(0,s.jsxs)(n.p,{children:["In the sentence: ",(0,s.jsx)(n.em,{children:'"The teacher gave the book to the student."'})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The word ",(0,s.jsx)(n.strong,{children:'"Book"'})," needs to understand who has it (Teacher) and who receives it (Student)."]}),"\n",(0,s.jsx)(n.li,{children:'The attention mechanism creates strong connections (weights) between "Book," "Teacher," and "Student," while ignoring less relevant words like "the."'}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    Word[Word: Book] -- Strong Attention --\x3e Teacher\n    Word -- Strong Attention --\x3e Student\n    Word -- Weak Attention --\x3e The\n    style Word fill:#ff9,stroke:#333"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-5-multi-head-attention-many-perspectives",children:"Step 5: Multi-Head Attention (Many Perspectives)"}),"\n",(0,s.jsxs)(n.p,{children:["The model doesn't just do self-attention once. It uses ",(0,s.jsx)(n.strong,{children:"Multi-Head Attention"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"what-is-a-head",children:'What is a "Head"?'}),"\n",(0,s.jsx)(n.p,{children:'Think of a "Head" as a different lens or filter. The model runs 12 to 100 of these heads in parallel. Each head learns a different aspect of language randomly during training.'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Head 1 (The Grammar Lens):"})," Might focus on Subject-Verb agreement."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Head 2 (The Rhyme Lens):"})," Might focus on phonetics or poetry."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Head 3 (The Context Lens):"})," Might focus on relationships between people (Teacher/Student)."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The results of all these heads are combined to give the model a complete understanding of the text."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"step-6-the-output-logits-and-probabilities",children:"Step 6: The Output (Logits and Probabilities)"}),"\n",(0,s.jsxs)(n.p,{children:["After passing through the Feed Forward Network, the model produces an output. But it doesn't output a word immediately; it outputs a ",(0,s.jsx)(n.strong,{children:"Vector of Logits"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"breaking-down-the-jargon",children:"Breaking Down the Jargon"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Logits:"})," These are raw, unnormalized scores. The model scores every single word in its dictionary on how likely it is to be the next word.","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Example:"})," Apple: 5.0, Ball: 1.2, Cat: -3.0"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Softmax Layer:"})," This turns those raw scores (Logits) into probabilities (Percentages).","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Example:"})," Apple: 98%, Ball: 1.9%, Cat: 0.1%"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The word with the highest probability is selected as the predicted token."}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    FFN[Feed Forward Network] --\x3e Logits[Vector of Logits]\n    Logits --\x3e Softmax[Softmax Layer]\n    Softmax --\x3e Probability[Probabilities: Apple 98%]\n    Probability --\x3e Final[Final Word: Apple]"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"putting-it-all-together-an-end-to-end-example",children:"Putting It All Together: An End-to-End Example"}),"\n",(0,s.jsx)(n.p,{children:"We have looked at the components individually. Now, let's watch them work together in a real scenario."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Task:"})," Translate the French phrase ",(0,s.jsx)(n.em,{children:"\"J'aime l'apprentissage automatique\""})," into English."]}),"\n",(0,s.jsx)(n.h3,{id:"phase-1-the-encoder-reading--understanding",children:"Phase 1: The Encoder (Reading & Understanding)"}),"\n",(0,s.jsxs)(n.p,{children:["First, the model needs to understand the input. This happens entirely in the ",(0,s.jsx)(n.strong,{children:"Encoder"}),"."]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tokenization:"})," The raw text is broken down into numbers."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Embedding & Position:"})," These numbers are turned into vectors with position information."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-Head Attention:"})," The model analyzes the relationships. For example, it understands that ",(0,s.jsx)(n.em,{children:'"apprentissage"'})," (learning) and ",(0,s.jsx)(n.em,{children:'"automatique"'})," (machine) are strongly related in this context."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deep Representation:"})," The output of the Encoder is not text, but a rich matrix of vectors that represents the ",(0,s.jsx)(n.em,{children:"meaning"})," of the French sentence."]}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:"graph TD\n    Input[Input: Jaime lapprentissage automatique] --\x3e Tokenizer\n    Tokenizer --\x3e Tokens[Tokens: 101, 405, 302, ...]\n    Tokens --\x3e Encoder[ENCODER BLOCK]\n    Encoder --\x3e Context[DEEP CONTEXT REPRESENTATION]\n    style Context fill:#f9f,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5\n    style Input fill:#bbf,stroke:#333"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"phase-2-the-decoder-generating-the-translation",children:"Phase 2: The Decoder (Generating the Translation)"}),"\n",(0,s.jsxs)(n.p,{children:["This is where the magic happens. The ",(0,s.jsx)(n.strong,{children:"Decoder"}),' takes the "Deep Context" from the Encoder and generates the English translation one word at a time. This is a ',(0,s.jsx)(n.strong,{children:"loop"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"step-a-the-trigger",children:"Step A: The Trigger"}),"\n",(0,s.jsxs)(n.p,{children:["The process starts by feeding a special ",(0,s.jsx)(n.strong,{children:"Start-of-Sequence (SOS)"})," token into the Decoder."]}),"\n",(0,s.jsx)(n.h4,{id:"step-b-the-prediction-loop",children:"Step B: The Prediction Loop"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["The Decoder looks at the ",(0,s.jsx)(n.strong,{children:"Context"})," (from the Encoder) and the ",(0,s.jsx)(n.strong,{children:"Inputs so far"})," (initially just ",(0,s.jsx)(n.code,{children:"<SOS>"}),")."]}),"\n",(0,s.jsx)(n.li,{children:"It runs Self-Attention to see what English words match the French meaning."}),"\n",(0,s.jsx)(n.li,{children:"It outputs probabilities via Softmax."}),"\n",(0,s.jsxs)(n.li,{children:["The word with the highest score is selected: ",(0,s.jsx)(n.strong,{children:'"I"'}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"step-c-the-feedback-loop",children:"Step C: The Feedback Loop"}),"\n",(0,s.jsxs)(n.p,{children:["The model doesn't stop. It takes the new word (",(0,s.jsx)(n.strong,{children:'"I"'}),") and feeds it back into the input. Now the input is ",(0,s.jsx)(n.code,{children:'<SOS> + "I"'}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Cycle 2 Output:"})," ",(0,s.jsx)(n.strong,{children:'"love"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Cycle 3 Output:"})," ",(0,s.jsx)(n.strong,{children:'"machine"'})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Cycle 4 Output:"})," ",(0,s.jsx)(n.strong,{children:'"learning"'})]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"step-d-the-stop-condition",children:"Step D: The Stop Condition"}),"\n",(0,s.jsxs)(n.p,{children:["Finally, the model predicts a special ",(0,s.jsx)(n.strong,{children:"End-of-Sequence (EOS)"})," token. This tells the system to stop generating."]}),"\n",(0,s.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant E as Encoder Context\n    participant D as Decoder\n    participant O as Output\n\n    Note over D: Loop Starts\n    D->>D: Input: [Start Token]\n    E->>D: Influences Attention\n    D->>O: Predicts: "I"\n    \n    D->>D: Input: [Start Token] + "I"\n    E->>D: Influences Attention\n    D->>O: Predicts: "love"\n    \n    D->>D: Input: [Start Token] + "I" + "love"\n    E->>D: Influences Attention\n    D->>O: Predicts: "machine learning"\n    \n    D->>D: Input: ... + "machine learning"\n    D->>O: Predicts: [End Token]\n    Note over O: Generation Complete'}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"transformer-variations-the-family-tree",children:"Transformer Variations: The Family Tree"}),"\n",(0,s.jsx)(n.p,{children:"While the example above used both the Encoder and Decoder (Sequence-to-Sequence), modern AI has branched into three distinct families based on which parts they keep."}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Type"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Architecture"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Best For"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Popular Models"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.strong,{children:"Encoder-Only"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Inputs & Outputs are same length."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Classification, Sentiment Analysis, Entity Recognition."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.strong,{children:"BERT"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.strong,{children:"Encoder-Decoder"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Input & Output lengths vary."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Translation, Text Summarization."}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,s.jsx)(n.strong,{children:"BART"}),", ",(0,s.jsx)(n.strong,{children:"T5"})]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.strong,{children:"Decoder-Only"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"Generates new tokens from a prompt."}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:"General Text Generation (Chatbots, Code, Creative Writing)."}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,s.jsx)(n.strong,{children:"GPT-4"}),", ",(0,s.jsx)(n.strong,{children:"LLaMA"}),", ",(0,s.jsx)(n.strong,{children:"Bloom"})]})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"The Transformer architecture changed AI by allowing models to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Process data in parallel (Speed)."}),"\n",(0,s.jsx)(n.li,{children:"Understand the context of every word relative to every other word (Attention)."}),"\n",(0,s.jsx)(n.li,{children:"Learn multiple nuances of language simultaneously (Multi-Head)."}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This architecture is the foundation upon which tools like ChatGPT and Claude are built."}),"\n",(0,s.jsx)(n.h2,{id:"references",children:"References:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/1706.03762",children:"https://arxiv.org/abs/1706.03762"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://learn.deeplearning.ai",children:"https://learn.deeplearning.ai"})}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},4604:e=>{e.exports=JSON.parse('{"permalink":"/docs/blog/transformer-architecture","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/transformer-architecture.md","source":"@site/blog/transformer-architecture.md","title":"The Transformer Architecture","description":"The release of the \\"Attention is All You Need\\" paper changed the landscape of Natural Language Processing (NLP) forever. It moved us away from Recurrent Neural Networks (RNNs) to the Transformer architecture, which powers the Generative AI explosion we see today.","date":"2025-12-11T00:00:00.000Z","tags":[],"readingTime":7.48,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"The Transformer Architecture","slug":"transformer-architecture","authors":"ashish","date":"2025-12-11T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The Thundering Herd - Understanding and Solving the Cache Stampede","permalink":"/docs/blog/cache-stampede-thundering-herd"},"nextItem":{"title":"Design an Online/Offline Indicator (Presence Service)","permalink":"/docs/blog/offline-online-indicator"}}')},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var i=t(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);