"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[647],{512:e=>{e.exports=JSON.parse('{"permalink":"/docs/blog/scaling-distributed-systems","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/scaling.md","source":"@site/blog/scaling.md","title":"Scaling Distributed Systems (Focus on Databases)","description":"In distributed systems, the four possible chokepoints that a system can run into are CPU, Memory, Network, and Disk. Whenever the system hits the thresholds on any of these chokepoints, latency increases, throughput degrades, and we typically need to scale our systems.","date":"2025-12-13T00:00:00.000Z","tags":[],"readingTime":6.29,"hasTruncateMarker":false,"authors":[{"name":"Ashish Rathod","title":"Ex-Intuit Staff Engineer","url":"https://www.linkedin.com/in/ashish-rathod02/","page":{"permalink":"/docs/blog/authors/ashish"},"socials":{"linkedin":"https://www.linkedin.com/in/ashish-rathod02/","github":"https://github.com/arathod02"},"imageURL":"https://github.com/arathod02.png","key":"ashish"}],"frontMatter":{"title":"Scaling Distributed Systems (Focus on Databases)","slug":"scaling-distributed-systems","authors":"ashish","date":"2025-12-13T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Implementing Shard Aware Application","permalink":"/docs/blog/application-level-sharding-design"},"nextItem":{"title":"A Deep Dive into Database Storage Engines and Soft Deletion Strategies.","permalink":"/docs/blog/avoid-hard-deletes"}}')},1057:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});var i=s(512),a=s(4848),t=s(8453);const r={title:"Scaling Distributed Systems (Focus on Databases)",slug:"scaling-distributed-systems",authors:"ashish",date:new Date("2025-12-13T00:00:00.000Z")},o=void 0,l={authorsImageUrls:[void 0]},d=[{value:"The Scaling Strategies",id:"the-scaling-strategies",level:2},{value:"Vertical Scaling (Scale Up)",id:"vertical-scaling-scale-up",level:3},{value:"Horizontal Scaling (Scale Out)",id:"horizontal-scaling-scale-out",level:3},{value:"Deep Dive: Scaling the Database",id:"deep-dive-scaling-the-database",level:2},{value:"1. Vertical Scaling",id:"1-vertical-scaling",level:3},{value:"2. Adding Read Replicas",id:"2-adding-read-replicas",level:3},{value:"a) Reader Endpoint (AWS Aurora / RDS)",id:"a-reader-endpoint-aws-aurora--rds",level:4},{value:"b) RDS Proxy",id:"b-rds-proxy",level:4},{value:"3. Sharding",id:"3-sharding",level:3}];function c(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["In distributed systems, the four possible chokepoints that a system can run into are ",(0,a.jsx)(n.strong,{children:"CPU, Memory, Network, and Disk"}),". Whenever the system hits the thresholds on any of these chokepoints, latency increases, throughput degrades, and we typically need to scale our systems."]}),"\n",(0,a.jsxs)(n.p,{children:["Scaling isn't just about \"adding more\"; it's about ",(0,a.jsx)(n.em,{children:"how"})," and ",(0,a.jsx)(n.em,{children:"where"})," you add capacity. Broadly, we have two strategies:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Vertical Scaling"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.strong,{children:"Horizontal Scaling"})}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"the-scaling-strategies",children:"The Scaling Strategies"}),"\n",(0,a.jsx)(n.h3,{id:"vertical-scaling-scale-up",children:"Vertical Scaling (Scale Up)"}),"\n",(0,a.jsx)(n.p,{children:"In Vertical scaling, we aim to solve for bottlenecks by throwing more CPU, memory, disk, and network bandwidth to the application. Basically, we make the infrastructure bulky enough to handle more load and try to remain under the threshold of our resources."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Pros:"})," Simplifies operations (no network overhead between nodes, no distributed consensus issues).\n",(0,a.jsx)(n.strong,{children:"Cons:"})," Has a hard ceiling (hardware limits) and can get exponentially expensive."]}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TD\n    subgraph "Vertical Scaling"\n    A[Small Server] --\x3e|Scale Up| B[Massive Server]\n    B --\x3e|Contains| C[More CPU]\n    B --\x3e|Contains| D[More RAM]\n    B --\x3e|Contains| E[Higher I/O]\n    end\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#bbf,stroke:#333,stroke-width:4px'}),"\n",(0,a.jsx)(n.h3,{id:"horizontal-scaling-scale-out",children:"Horizontal Scaling (Scale Out)"}),"\n",(0,a.jsx)(n.p,{children:"In horizontal scaling, we run multiple instances of the application on multiple machines. Basically, we add more machines to the compute layer. This fleet of compute servers is typically fronted by a load balancer."}),"\n",(0,a.jsx)(n.p,{children:"As the load increases, the compute layer is linearly scaled to cope with the load. Horizontal scaling also aids in fault tolerance; if one node dies, the others pick up the slack."}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TD\n    User((User)) --\x3e LB[Load Balancer]\n    subgraph "Compute Fleet"\n    LB --\x3e Server1[App Instance 1]\n    LB --\x3e Server2[App Instance 2]\n    LB --\x3e Server3[App Instance 3]\n    end\n    style LB fill:#f96,stroke:#333,stroke-width:2px'}),"\n",(0,a.jsxs)(n.admonition,{title:'The "Unit Tech Economics"',type:"info",children:[(0,a.jsxs)(n.p,{children:["Double-clicking a bit on the phrase ",(0,a.jsx)(n.em,{children:'"As the load typically increases, the compute layer is linearly scaled"'}),": In order for this step to be efficient, it is very important to know the ",(0,a.jsx)(n.strong,{children:'"Unit Tech Economics"'}),"."]}),(0,a.jsx)(n.p,{children:"For a given machine of a certain size (i.e., # of CPUs, RAM, Disk, and N/W bandwidth), what is the throughput that can be successfully and healthily served?"}),(0,a.jsxs)(n.p,{children:["There isn't a magic formula for this. Each application needs to be load tested with ",(0,a.jsx)(n.strong,{children:"one machine"}),' to arrive at the "Unit Tech Economics." Once you have these numbers, all you need to do is extrapolate them to the actual load anticipated in production or during peak hours.']})]}),"\n",(0,a.jsxs)(n.admonition,{title:'The "Infinite Scale" Fallacy',type:"caution",children:[(0,a.jsxs)(n.p,{children:["Does this mean that with horizontal scaling of the API servers you can achieve infinite scale? ",(0,a.jsx)(n.strong,{children:"Not really."})]}),(0,a.jsxs)(n.p,{children:["What about your downstream systems, caches, and DBs? Will they be able to serve the spiked-up load? ",(0,a.jsx)(n.strong,{children:"NO."})]}),(0,a.jsxs)(n.p,{children:["When scaling, one should adopt the ",(0,a.jsx)(n.strong,{children:"bottoms-up approach"}),". Scaling API servers should ideally be the last component to be scaled up."]})]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"deep-dive-scaling-the-database",children:"Deep Dive: Scaling the Database"}),"\n",(0,a.jsx)(n.p,{children:"With this context set, let's go deep into the scaling aspects of DBs (stateful components)."}),"\n",(0,a.jsx)(n.h3,{id:"1-vertical-scaling",children:"1. Vertical Scaling"}),"\n",(0,a.jsx)(n.p,{children:"As explained above, with vertical scaling\u2014without splitting your DB instance\u2014you typically host your DB on a bigger server, i.e., a machine with more capacity in terms of CPU, Memory, Disk, and Network. Basically, you run your DB on a bulkier server."}),"\n",(0,a.jsx)(n.mermaid,{value:'graph TD\n    App[Application Fleet] --\x3e|Writes & Reads| DB[Massive Primary DB Instance]\n    subgraph "Vertical DB Scaling"\n    DB\n    end\n    style DB fill:#bbf,stroke:#333,stroke-width:4px'}),"\n",(0,a.jsxs)(n.blockquote,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Real World Example: Zerodha"}),"\nOne of India's largest stock brokers, famously scaled their massive volume by avoiding premature distributed complexity. They run their core systems on a ",(0,a.jsx)(n.strong,{children:"single, massive Postgres Server"}),"."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Summary:"})," Instead of rushing to sharding, Zerodha focused on optimizing queries and utilizing high-end hardware."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"How:"})," They use high-performance hardware (huge RAM to fit working sets in memory) and optimized Postgres configurations."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pros:"})," Simplifies architecture (no distributed transactions), ensures strict ACID compliance, and offers low latency."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cons:"})," It is a Single Point of Failure (SPOF) if not HA, and restart times can be long due to the sheer size of memory buffers."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsxs)(n.em,{children:["Read more: ",(0,a.jsx)(n.a,{href:"https://zerodha.tech/blog/working-with-postgresql/",children:"Zerodha Tech Blog: Working with PostgreSQL"})]})}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"2-adding-read-replicas",children:"2. Adding Read Replicas"}),"\n",(0,a.jsx)(n.p,{children:"In this approach, you provision for serving read traffic. Basically, you add one or more DB servers that specifically serve read traffic. This approach helps in reserving the primary node of the DB for writes, effectively isolating read and write traffic."}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TD\n    App[Application] --\x3e|Writes| Master[Primary Node]\n    App --\x3e|Reads| Replica1[Read Replica 1]\n    App --\x3e|Reads| Replica2[Read Replica 2]\n    Master -.->|Async Replication| Replica1\n    Master -.->|Async Replication| Replica2"}),"\n",(0,a.jsx)(n.p,{children:"In terms of reading from the application standpoint, the application need not know about multiple servers. It can leverage a couple of solutions to simplify reader connection management:"}),"\n",(0,a.jsx)(n.h4,{id:"a-reader-endpoint-aws-aurora--rds",children:"a) Reader Endpoint (AWS Aurora / RDS)"}),"\n",(0,a.jsxs)(n.p,{children:["If you are using AWS Aurora or RDS, you can leverage the ",(0,a.jsx)(n.strong,{children:"Reader Endpoint"}),". A reader endpoint is a DNS record exposed by AWS. The application simply connects to this endpoint."]}),"\n",(0,a.jsx)(n.p,{children:"At a high level, one can visualize the reader endpoint as a load balancer sitting in front of the DB readers. It distributes these read requests fairly amongst the available readers."}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TD\n    App[Application] --\x3e|Connects to| RE[AWS Reader Endpoint]\n    RE --\x3e|Distributes| R1[Replica 1]\n    RE --\x3e|Distributes| R2[Replica 2]\n    RE --\x3e|Distributes| R3[Replica 3]"}),"\n",(0,a.jsx)(n.h4,{id:"b-rds-proxy",children:"b) RDS Proxy"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"What is RDS Proxy?"}),"\nAmazon RDS Proxy is a fully managed, highly available database proxy that sits between your application and your relational database."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Use Cases:"})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Connection Pooling:"})," It maintains a pool of established connections to your database, sharing them among application requests. This is crucial for serverless applications (like Lambda) that might open thousands of connections quickly."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resiliency:"})," It reduces failover times by bypassing DNS cache propagation delays."]}),"\n"]}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TD\n    Lambda[Lambda/App] --\x3e|Open Connections| Proxy[RDS Proxy]\n    Proxy --\x3e|Pooled Connections| DB[Database Cluster]\n    style Proxy fill:#ff9,stroke:#333,stroke-width:2px"}),"\n",(0,a.jsx)(n.h3,{id:"3-sharding",children:"3. Sharding"}),"\n",(0,a.jsxs)(n.p,{children:["Sharding is ",(0,a.jsx)(n.strong,{children:"not"})," a Day 0 approach. You typically do this when you know the user and data volume will exceed the capacity of a vertically scaled single node."]}),"\n",(0,a.jsx)(n.p,{children:"In Sharding, you split the data across multiple primary shards."}),"\n",(0,a.jsx)(n.mermaid,{value:"graph TD\n    App[Application]\n    App --\x3e|Subset A-J| Shard1[Shard 1: Users A-J]\n    App --\x3e|Subset K-T| Shard2[Shard 2: Users K-T]\n    App --\x3e|Subset U-Z| Shard3[Shard 3: Users U-Z]"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"How do you decide where data resides?"})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Range-Based Mapping:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Assume you are a platform serving millions of tenants. You might say: Tenants starting with ",(0,a.jsx)(n.code,{children:"a-j"})," reside in Shard 1, ",(0,a.jsx)(n.code,{children:"k-t"})," in Shard 2, and ",(0,a.jsx)(n.code,{children:"u-z"})," in Shard 3."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Static Mapping:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"In this approach, you maintain a static mapping (lookup table) between tenants and DB shards. E.g., Tenant 1 maps to Shard 1, Tenant 2 maps to Shard 1, Tenant 3 maps to Shard 2, etc."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Where do you host this mapping logic?"})}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Application Layer"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:['We make the application "shard aware." The application ',(0,a.jsx)(n.strong,{children:"KNOWS"})," the DB topology and determines which shard a given tenant resides in."]}),"\n",(0,a.jsx)(n.li,{children:"The application then connects to the respective DB shard to perform reads and writes."}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"External Layer (Smart Proxy)"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Instead of the application knowing the topology, a proxy layer ",(0,a.jsx)(n.strong,{children:"KNOWS"})," the DB topology."]}),"\n",(0,a.jsx)(n.li,{children:"You configure server rules and query rules in the proxy layer. Based on the incoming query, the proxy routes the request to the correct DB shard."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.em,{children:"I will write another blog to go really deep into BOTH the above techniques (Application vs. Proxy sharding). I will link those two blogs here soon."})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var i=s(6540);const a={},t=i.createContext(a);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);